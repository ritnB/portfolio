âœ¨ Antmeter (Redacted) â€” Productionâ€‘Style ML Timeseries Pipeline

A portfolioâ€‘friendly, securityâ€‘sanitized showcase of an endâ€‘toâ€‘end ML pipeline for timeseries classification. It highlights engineering practices (pipelines, model training, inference, verification, incremental learning) while redacting sensitive details.

ğŸ§­ Table of Contents
- ğŸŒŸ Highlights
- ğŸ§± Architecture at a Glance
- ğŸ“‚ Project Structure
- âš™ï¸ Quickstart
- ğŸ³ Docker
- ğŸŒ API Endpoints
- ğŸ” Security & Redaction
- ğŸ§ª Config Philosophy
- ğŸ”§ Environment Variables

ğŸŒŸ Highlights
- ğŸ§  Generic patchâ€‘based sequence classifier (CLS/mean pooling) with optional FocalLoss
- ğŸ” Timeâ€‘aware CV (leakageâ€‘aware rolling splits) and consistent preprocessing
- â™»ï¸ Incremental learning with validation gating and safety checks
- ğŸ“ˆ Batched postâ€‘inference enrichment (e.g., price updates) via external API
- â˜ï¸ Artifact management through cloud storage (model/scaler discovery)
- ğŸ§° Clean module boundaries, cache hygiene, and memoryâ€‘safe utilities

ğŸ§± Architecture at a Glance
- Data I/O: Load technical indicators from a backend (e.g., Supabase)
- Training: Optuna search â†’ final training â†’ artifacts (model/scaler)
- Inference: Download latest artifacts â†’ window generation â†’ prediction â†’ persistence
- Verification: Timeâ€‘aligned validation of predictions against observed outcomes
- Incremental: Lightweight fineâ€‘tuning with performance guards â†’ reâ€‘upload artifacts

ğŸ“‚ Project Structure
```
project_root/
â”œâ”€â”€ app.py                       # Flask entrypoint (API)
â”œâ”€â”€ config.py                    # Centralized config (envâ€‘driven, redacted)
â”œâ”€â”€ Dockerfile                   # Container build
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preprocess.py            # Sliding windows, datasets, CV helpers
â”‚   â”œâ”€â”€ supabase_io.py           # Backend I/O utils
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ timeseries_inference.py  # Inference + persistence + enrichment
â”œâ”€â”€ models/
â”‚   â””â”€â”€ timeseries_model.py      # PatchSequenceModel + checkpoint loader
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ pipeline_timeseries.py   # Orchestrates inference pipeline
â”‚   â”œâ”€â”€ pipeline_verify.py       # Validates past predictions
â”‚   â”œâ”€â”€ pipeline_retrain.py      # Thresholdâ€‘based retraining trigger
â”‚   â”œâ”€â”€ pipeline_incremental.py  # Incremental learning cycle
â”‚   â””â”€â”€ pipeline_labeling.py     # Label generation with future lookup
â”œâ”€â”€ trainers/
â”‚   â””â”€â”€ train_patchtst.py        # Training with Optuna
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ gcs_utils.py             # Cloud storage helpers
â”‚   â”œâ”€â”€ memory_utils.py          # Memory hygiene helpers
â”‚   â”œâ”€â”€ price_utils.py           # External price API helpers
â”‚   â”œâ”€â”€ timestamp_utils.py       # Timestamp parsing utilities
â”‚   â””â”€â”€ training_utils.py        # Training callbacks/utilities
â””â”€â”€ debug_tools/
    â””â”€â”€ debug_tools.py           # Simple diagnostics printers
```

âš™ï¸ Quickstart
1) Prerequisites
   - Python 3.10+
   - `pip install -r requirements.txt`

2) Environment
   - Create a `.env` with the required variables (see section below)

3) Run the API
   - Local (gunicorn):
     ```bash
     gunicorn --timeout 3600 --bind 0.0.0.0:8080 app:app
     ```
   - Health check:
     ```bash
     curl http://localhost:8080/
     ```

ğŸ³ Docker
- Build
  ```bash
  docker build -t antmeter .
  ```
- Run (with env file)
  ```bash
  docker run -p 8080:8080 --env-file .env antmeter
  ```

ğŸŒ API Endpoints
- GET `/`            â†’ Health
- POST `/timeseries` â†’ Run timeseries inference (downloads latest artifacts)
- POST `/verify`     â†’ Verify historical predictions
- POST `/retrain`    â†’ Trigger retraining if accuracy falls below threshold
- POST `/incremental`â†’ Run incremental update with validation gating
- POST `/labeling`   â†’ Generate labels with futureâ€‘aware lookup

ğŸ” Security & Redaction
- No secrets or identifiers are committed. All sensitive values are envâ€‘driven.
- Cloud buckets/keys/URLs are redacted and must be provided via environment.
- Coin mappings are minimized; provide your own via `COIN_INFO_JSON` when needed.
- `.gitignore` includes `.env`, `secrets/`, and common sensitive patterns.

ğŸ§ª Config Philosophy
- `config.py` centralizes all configuration with strong sanitization:
  - Generic placeholders (e.g., `GENERIC_*`) replace tuned numeric values.
  - Ranges and thresholds are expressed as neutral defaults.
  - Optuna search space scales from a generic base (`OPTUNA_BASE`).
  - Everything is overrideable via environment variables for real deployments.

ğŸ”§ Environment Variables (subset)
- Required (examples)
  - `SUPABASE_URL`, `SUPABASE_KEY`                      # backend access
  - `GCS_BUCKET_NAME`, `GCS_MODEL_DIR`                  # artifact storage
- Optional (generalized defaults applied if absent)
  - `MODEL_PATH`, `SCALER_PATH`
  - `COIN_INFO_JSON`                                    # custom coin mapping JSON
  - `PREDICTION_EVAL_DAYS`, `THRESHOLD_ACCURACY`
  - `INCREMENTAL_*` (e.g., `INCREMENTAL_EPOCHS`, `INCREMENTAL_LR`, etc.)
  - `ROLLING_CV_*`, `N_TRIALS`, `MAX_TIME_GAP_HOURS`
  - External API knobs (e.g., `COINGECKO_*`)

ğŸ’¬ Notes
- This repository is intentionally sanitized for public presentation. Replace placeholders with real values in your private environment.
- The pipeline modules are designed to be composable; feel free to swap the data source, model head, or storage backend.