# models/timeseries_model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

# ðŸ§¨ FocalLoss ì •ì˜ (CrossEntropy ëŒ€ì²´ìš©)
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, input, target):
        ce_loss = F.cross_entropy(input, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

# ðŸ§  PatchTST with CLS or Mean Pooling
class PatchTST(nn.Module):
    def __init__(self, input_size, d_model, num_layers, num_heads,
                 patch_size, window_size, num_classes,
                 dropout=0.0, pooling_type='cls', mlp_hidden_mult=2,
                 activation='relu', stride=None):

        super().__init__()

        self.patch_size = patch_size
        self.stride = stride if stride is not None else patch_size  # ì‚¬ìš©ìžê°€ ì„¤ì • ì•ˆ í•˜ë©´ ê¸°ë³¸ì€ patch_size
        self.pooling_type = pooling_type.lower()

        assert window_size >= patch_size, "window_size must be >= patch_size"
        assert (window_size - patch_size) % self.stride == 0, "patches must align evenly"

        self.num_patches = 1 + (window_size - patch_size) // self.stride


        # Patch embedding
        self.input_proj = nn.Linear(input_size * patch_size, d_model)

        # CLS token
        if self.pooling_type == 'cls':
            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))

        # Position embedding
        self.pos_embedding = nn.Parameter(torch.randn(
            1, self.num_patches + (1 if self.pooling_type == 'cls' else 0), d_model))

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Activation
        self.act = {
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'silu': nn.SiLU()
        }[activation]

        # MLP Head
        hidden_dim = d_model * mlp_hidden_mult
        self.mlp = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            self.act,
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )

        self.ce_loss = nn.CrossEntropyLoss()
        self.focal_loss = None  # ì™¸ë¶€ì—ì„œ ì„¤ì • ê°€ëŠ¥

    def forward(self, x, labels=None):
        B, W, D = x.shape
        P = self.patch_size
        S = self.stride

        # â‘  ìŠ¬ë¼ì´ë”© íŒ¨ì¹˜ ìƒì„±: (B, num_patches, patch_size * D)
        x = x.unfold(dimension=1, size=P, step=S)
        x = x.contiguous().view(B, -1, P * D)

        # â‘¡ Linear projection
        x = self.input_proj(x)

        # â‘¢ CLS token ì‚½ìž… (ì„ íƒ)
        if self.pooling_type == 'cls':
            cls_token = self.cls_token.expand(B, -1, -1)
            x = torch.cat([cls_token, x], dim=1)

        # â‘£ í¬ì§€ì…˜ ìž„ë² ë”© ì¶”ê°€
        x = x + self.pos_embedding[:, :x.size(1), :]

        # â‘¤ Transformer ì¸ì½”ë” í†µê³¼
        x = self.transformer(x)

        # â‘¥ Pooling
        if self.pooling_type == 'cls':
            x = x[:, 0, :]
        else:
            x = x.mean(dim=1)

        # â‘¦ Classification
        logits = self.mlp(x)

        # â‘§ Loss (optional)
        if labels is not None:
            if self.focal_loss is not None:
                loss = self.focal_loss(logits, labels)
            else:
                loss = self.ce_loss(logits, labels)
            return {"loss": loss, "logits": logits}

        return {"logits": logits}

def load_model_from_checkpoint(path: str, return_args: bool = False):
    """
    ì €ìž¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ PatchTST ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        path (str): .pt íŒŒì¼ ê²½ë¡œ
        return_args (bool): Trueì‹œ (model, model_args) íŠœí”Œ ë°˜í™˜

    Returns:
        PatchTST ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” (model, model_args) íŠœí”Œ
    """
    checkpoint = torch.load(path, map_location=torch.device("cpu"))
    model_args = checkpoint["model_args"]
    
    # PatchTST ìƒì„±ìžì—ì„œ í—ˆìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•„í„°ë§
    valid_params = {
        'input_size', 'd_model', 'num_layers', 'num_heads', 
        'patch_size', 'window_size', 'num_classes', 'dropout', 
        'pooling_type', 'mlp_hidden_mult', 'activation', 'stride'
    }
    filtered_args = {k: v for k, v in model_args.items() if k in valid_params}
    
    model = PatchTST(**filtered_args)
    model.load_state_dict(checkpoint["state_dict"])
    
    if return_args:
        return model, model_args  # ì›ë³¸ model_args ë°˜í™˜ (v11_2 ì„¤ì •ê°’ í¬í•¨)
    return model

def load_model(**model_args) -> PatchTST:
    return PatchTST(**model_args)
