{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgYWBBQtugtz"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E45Uq1PuVRN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/to/project_directory')\n",
    "\n",
    "from config import *\n",
    "from util import get_center_points, is_same_length, is_list_strings, move_to_zero, move_to_zero_all, get_parsed_timestamp, process_dataframe\n",
    "# from Collected_dataset.dataloader import\n",
    "# from Collected_dataset.model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35AtLwAYuW23"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDNN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvtjLvjculnd"
   },
   "source": [
    "# File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pR3ErlDuYkq"
   },
   "outputs": [],
   "source": [
    "dict_path_tr_clean = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/clean/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/clean/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/clean/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/clean/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/clean/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/clean/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/clean/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/clean/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/clean/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_tr_attack = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/attack/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/attack/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/attack/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/attack/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/attack/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/attack/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/attack/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/attack/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/attack/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_clean = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/clean/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/clean/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/clean/35m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_attack = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/attack/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/attack/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/attack/35m_dyn2.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Cj6d5BuuapT"
   },
   "outputs": [],
   "source": [
    "#placeholder dictionary for collected dataframes\n",
    "dict_df_tr_clean={}\n",
    "dict_df_tr_attack={}\n",
    "dict_df_te_clean={}\n",
    "dict_df_te_attack={}\n",
    "\n",
    "#control variables\n",
    "dict_path=dict_path_tr_clean #path variable\n",
    "dict_df=dict_df_tr_clean #dict variable to collect dataframes read from paths\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_clean.items()})\n",
    "\n",
    "dict_path=dict_path_tr_attack\n",
    "dict_df=dict_df_tr_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_attack.items()})\n",
    "\n",
    "dict_path=dict_path_te_clean\n",
    "dict_df=dict_df_te_clean\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_clean.items()})\n",
    "\n",
    "dict_path=dict_path_te_attack\n",
    "dict_df=dict_df_te_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_attack.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_kP_HuVutdU"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YteIXxFuvhl"
   },
   "outputs": [],
   "source": [
    "# Make a new dictionary to store preprocessed df\n",
    "preprocessed_dict_tr_clean={}\n",
    "preprocessed_dict_tr_attack={}\n",
    "preprocessed_dict_te_clean={}\n",
    "preprocessed_dict_te_attack={}\n",
    "\n",
    "# Call def process_dataframe and store df\n",
    "point_numbers=0\n",
    "for key, df in dict_df_tr_clean.items():\n",
    "    if key not in preprocessed_dict_tr_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "        point_numbers+=len(df)\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_clean.items()})\n",
    "\n",
    "for key, df in dict_df_tr_attack.items():\n",
    "    if key not in preprocessed_dict_tr_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_attack.items()})\n",
    "\n",
    "for key, df in dict_df_te_clean.items():\n",
    "    if key not in preprocessed_dict_te_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_clean.items()})\n",
    "\n",
    "for key, df in dict_df_te_attack.items():\n",
    "    if key not in preprocessed_dict_te_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_attack.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx0H6Bu_uxXQ"
   },
   "outputs": [],
   "source": [
    "# labeling 0 for clean, 1 for attack\n",
    "for key, df in preprocessed_dict_tr_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_tr_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1\n",
    "for key, df in preprocessed_dict_te_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_te_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udl-IOtQu7d7"
   },
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C22sDvUuu6MW"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dict_tr_clean.keys())\n",
    "print(preprocessed_dict_tr_attack.keys())\n",
    "print(preprocessed_dict_te_clean.keys())\n",
    "print(preprocessed_dict_te_attack.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHEwzP3PvA3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import split_dataframe\n",
    "\n",
    "# split function... 왜인지 자꾸 import 에러 나서 여기 둠\n",
    "def split_dataframe(df):\n",
    "    diff = np.absolute(df['timestamp'].diff().values)\n",
    "    diff[0] = 0.0\n",
    "    diff = pd.Series(diff.round(decimals=5))\n",
    "    split_indices = np.where(diff >= 1.0)[0]\n",
    "    split_indices = [0] + list(split_indices) + [len(diff)]\n",
    "    split_dfs = []\n",
    "\n",
    "    for i, split_index in enumerate(split_indices):\n",
    "        if i == len(split_indices) - 1:\n",
    "            break\n",
    "        start_index = split_index\n",
    "        end_index = split_indices[i + 1]\n",
    "        split_dfs.append(df.iloc[start_index:end_index])\n",
    "\n",
    "    return split_dfs\n",
    "\n",
    "# preprocessed_dict와 동일한 key를 가진 딕셔너리를 만들고, item으로는 분할된 데이터프레임이 append된 리스트를 넣음\n",
    "dict_split_tr_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_clean.items()}\n",
    "dict_split_tr_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_attack.items()}\n",
    "dict_split_te_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_te_clean.items()}\n",
    "dict_split_te_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_te_attack.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QkY6sGAvCCX"
   },
   "outputs": [],
   "source": [
    "# Check length of the splitted dataset\n",
    "for key, split_list in dict_split_tr_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_tr_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0y2p6KKvFL4"
   },
   "outputs": [],
   "source": [
    "def count_nan(df_list):\n",
    "    nan_counts = sum(df.isna().values.sum() for df in df_list)\n",
    "    return f'({nan_counts})'\n",
    "\n",
    "len_dict_tr_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_clean.items()}\n",
    "len_dict_tr_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_attack.items()}\n",
    "len_dict_te_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_clean.items()}\n",
    "len_dict_te_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_attack.items()}\n",
    "\n",
    "print(len_dict_tr_clean)\n",
    "print(len_dict_tr_attack)\n",
    "print(len_dict_te_clean)\n",
    "print(len_dict_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKR2i1E_IYke"
   },
   "source": [
    "## Trajectory Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kgNq3WpIpsQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D  # 범례를 위한 Line2D 클래스 임포트\n",
    "\n",
    "# 고도별로 그래프 생성 함수\n",
    "def plot_altitude_attack_type_grids(altitude, attack_types, dict_tr_clean, dict_tr_attack, dict_te_clean, dict_te_attack):\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax_list = []\n",
    "\n",
    "    for i, attack_type in enumerate(attack_types):\n",
    "        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "        ax_list.append(ax)\n",
    "\n",
    "        key = f'{altitude}_{attack_type}'\n",
    "        dict_clean = dict_te_clean if altitude == '35m' else dict_tr_clean\n",
    "        dict_attack = dict_te_attack if altitude == '35m' else dict_tr_attack\n",
    "\n",
    "        clean_line, attack_line = None, None\n",
    "\n",
    "        if key in dict_clean:\n",
    "            for df in dict_clean[key]:\n",
    "                linewidth = 4 if i == 0 else 2  # 첫 번째 그래프의 선 굵기만 늘림\n",
    "                clean_line, = ax.plot(df['lat'], df['lon'], df['alt'], color='#00acee', linewidth=linewidth)\n",
    "                if attack_type != 'stat':\n",
    "                    ax.scatter(df['lat'].iloc[0], df['lon'].iloc[0], df['alt'].iloc[0], color='#00acee', marker='^', s=100, facecolors='none', edgecolors='#00acee')  # 시작점\n",
    "                    ax.scatter(df['lat'].iloc[-1], df['lon'].iloc[-1], df['alt'].iloc[-1], color='#00acee', marker='o', s=100)  # 끝점\n",
    "\n",
    "        if key in dict_attack:\n",
    "            for df in dict_attack[key]:\n",
    "                linewidth = 4 if i == 0 else 2  # 첫 번째 그래프의 선 굵기만 늘림\n",
    "                attack_line, = ax.plot(df['lat'], df['lon'], df['alt'], color='#ff69b4', linewidth=linewidth)\n",
    "                if attack_type != 'stat':\n",
    "                    ax.scatter(df['lat'].iloc[0], df['lon'].iloc[0], df['alt'].iloc[0], color='#ff69b4', marker='^', s=100, facecolors='none', edgecolors='#ff69b4')  # 시작점\n",
    "                    ax.scatter(df['lat'].iloc[-1], df['lon'].iloc[-1], df['alt'].iloc[-1], color='#ff69b4', marker='o', s=100)  # 끝점\n",
    "\n",
    "        ax.set_xlabel('Latitude', fontsize=20)\n",
    "        ax.set_ylabel('Longitude', fontsize=20)\n",
    "        ax.set_zlabel('Altitude', fontsize=20)\n",
    "        ax.set_title('', fontsize=20)  # 타이틀 제거\n",
    "\n",
    "        # 축 숫자를 표시하지 않음\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_zticklabels([])\n",
    "\n",
    "        # 각 그래프마다 범례 추가\n",
    "        legend_lines = [\n",
    "            Line2D([0], [0], color='#00acee', lw=2, label='Authentic'),\n",
    "            Line2D([0], [0], color='#ff69b4', lw=2, label='Spoofed')\n",
    "        ]\n",
    "        ax.legend(handles=legend_lines, loc='upper right', prop={'size': 14})\n",
    "\n",
    "    # 레이아웃 조정\n",
    "    fig.subplots_adjust(top=0.85, bottom=0.1, left=0.05, right=0.95, wspace=0.3)\n",
    "\n",
    "    # 이미지 출력\n",
    "    plt.show()\n",
    "\n",
    "# 고도와 공격 유형 설정\n",
    "altitude = '20m'\n",
    "attack_types = ['stat', 'dyn1', 'dyn2']\n",
    "\n",
    "# 각 고도 및 공격 유형에 대해 그래프 생성 및 출력\n",
    "plot_altitude_attack_type_grids(altitude, attack_types, dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em6IEz5WXM9b"
   },
   "source": [
    "## Adding Sequenced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBfRCRJ8XQp4"
   },
   "outputs": [],
   "source": [
    "# Creating new columns = 'D_lon', 'D_lat', 'D_alt' : difference between two consecutive coordinates\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None) # <==== 경고를 끈다\n",
    "\n",
    "def add_diff(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 각 차분을 계산하고 'legitimate' 열 바로 앞에 삽입합니다.\n",
    "            df.insert(legitimate_index, 'D_lat', df['lat'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 1, 'D_lon', df['lon'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 2, 'D_alt', df['alt'].diff().fillna(0))\n",
    "\n",
    "add_diff(dict_split_tr_clean)\n",
    "add_diff(dict_split_tr_attack)\n",
    "add_diff(dict_split_te_clean)\n",
    "add_diff(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klRbmxdsXSIA"
   },
   "outputs": [],
   "source": [
    "# Creating new column = 'd_pos' : Ucladian distance between two consecutive coordinates\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def calculate_euc_distance(df):\n",
    "    coordinates = df[['lat', 'lon', 'alt']].to_numpy()\n",
    "    distances = [0.0]  # 첫 번째 행에는 0을 할당\n",
    "    for i in range(1, len(coordinates)):\n",
    "        distance = euclidean_distances([coordinates[i-1]], [coordinates[i]])[0][0]\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "def add_euc(dict_split):\n",
    "    # 각 데이터프레임에 대해 유클리드 거리 계산 및 'd_pos' 열 추가\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 유클리드 거리 계산\n",
    "            distances = calculate_euc_distance(df)\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'd_pos' 열을 'legitimate' 열 바로 전에 추가\n",
    "            df.insert(legitimate_index, 'd_pos', distances)\n",
    "\n",
    "add_euc(dict_split_tr_clean)\n",
    "add_euc(dict_split_tr_attack)\n",
    "add_euc(dict_split_te_clean)\n",
    "add_euc(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF0xsSdIXZJO"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'time_gap' : Time gap between two consecutive points\n",
    "\n",
    "def add_timeGap(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'timestamp' 열의 차분을 계산하고 'legitimate' 열 바로 앞에 삽입\n",
    "            df.insert(legitimate_index, 'time_gap', df['timestamp'].diff().fillna(0))\n",
    "    return dict_split\n",
    "\n",
    "add_timeGap(dict_split_tr_clean)\n",
    "add_timeGap(dict_split_tr_attack)\n",
    "add_timeGap(dict_split_te_clean)\n",
    "add_timeGap(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifdeqKFFvKv7"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KVnN22va_V"
   },
   "source": [
    "## Creating Dataset for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lr3PAa9vNk8"
   },
   "outputs": [],
   "source": [
    "series_value = 0  # 전역 변수로 series_value 정의\n",
    "\n",
    "def add_df_id(dict_split):\n",
    "    global series_value  # 전역 변수 series_value 사용\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            df_length = len(df)\n",
    "            # 임시 열 생성\n",
    "            series_id_col = [series_value] * df_length\n",
    "            measurement_id_col = list(range(df_length))\n",
    "            # 첫 번째와 두 번째 위치에 열 삽입\n",
    "            df.insert(0, 'series_ID', series_id_col)\n",
    "            df.insert(1, 'measurement_ID', measurement_id_col)\n",
    "            series_value += 1  # series_value 업데이트\n",
    "\n",
    "# 함수 호출\n",
    "add_df_id(dict_split_tr_clean)\n",
    "add_df_id(dict_split_tr_attack)\n",
    "add_df_id(dict_split_te_clean)\n",
    "add_df_id(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-3nZMYJvPMB"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "500hgaZYvUod"
   },
   "outputs": [],
   "source": [
    "# 데이터프레임 길이를 저장할 리스트\n",
    "df_lengths = []\n",
    "\n",
    "# 모든 딕셔너리와 데이터프레임을 순회하며 길이 저장\n",
    "for dict_split in [dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack]:\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            df_lengths.append(len(df))\n",
    "\n",
    "# 최소 길이 찾기\n",
    "min_length = min(df_lengths)\n",
    "\n",
    "# 각 데이터프레임을 min_length로 잘라내기\n",
    "def dflen_min(dict_split):\n",
    "    for split_list in dict_split.values():\n",
    "        for i in range(len(split_list)):\n",
    "            split_list[i] = split_list[i].iloc[:min_length]\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "dflen_min(dict_split_tr_clean)\n",
    "dflen_min(dict_split_tr_attack)\n",
    "dflen_min(dict_split_te_clean)\n",
    "dflen_min(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MQHWAI0vgX-"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFnob-mwvg8k"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from multiprocessing import cpu_count       # 현재 CPU에서 사용 가능한 코어 수를 반환\n",
    "from pathlib import Path        # 경로를 조작\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler       # 높은 학습률로 시작하여 점차 낮추면서 학습하도록 동적 스케줄링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IVgltI0WBVd"
   },
   "source": [
    "### Feature Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq6E1pFGvinO"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(0)\n",
    "\n",
    "# 제외할 열 정하기\n",
    "ID_COLS = ['series_ID', 'measurement_ID']  # 각 데이터 포인트를 구별하는 데 사용되는 열\n",
    "LOC_COLS = ['lat', 'lon', 'alt', 'alt_ellipsoid']   # location informations\n",
    "# LOC_COLS = ['alt_ellipsoid']\n",
    "AD_DROP_COLS = ['timestamp', 'noise_per_ms', 'jamming_indicator', 'c_variance_rad', 's_variance_m_s', 'epv', 'eph', 'hdop', 'vdop', 'satellites_used']\n",
    "# AD_DROP_COLS = ['timestamp', 'eph']\n",
    "\n",
    "# 지정된 열을 제외하고 데이터프레임을 리스트로 변환하는 함수\n",
    "def df_to_np(dict_split, drop_cols):\n",
    "    nested_lists = []\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            # 지정된 열 제외\n",
    "            df_dropped = df.drop(columns=drop_cols)\n",
    "            # 데이터프레임의 각 행을 리스트로 변환\n",
    "            list_of_rows = df_dropped.values.tolist()\n",
    "            # 변환된 리스트들을 포함하는 리스트 추가\n",
    "            nested_lists.append(list_of_rows)\n",
    "    return nested_lists\n",
    "\n",
    "drop_cols = ID_COLS + LOC_COLS + AD_DROP_COLS\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "list_np_tr_clean = np.array(df_to_np(dict_split_tr_clean, drop_cols))\n",
    "list_np_tr_attack = np.array(df_to_np(dict_split_tr_attack, drop_cols))\n",
    "list_np_te_clean = np.array(df_to_np(dict_split_te_clean, drop_cols))\n",
    "list_np_te_attack = np.array(df_to_np(dict_split_te_attack, drop_cols))\n",
    "\n",
    "# Training 데이터셋 합치기\n",
    "list_np_tr = np.concatenate([list_np_tr_clean, list_np_tr_attack])\n",
    "\n",
    "# Testing 데이터셋 합치기\n",
    "list_np_te = np.concatenate([list_np_te_clean, list_np_te_attack])\n",
    "\n",
    "remaining_features = [item for item in dict_split_te_attack['35m_dyn2'][3].columns if item not in drop_cols]\n",
    "\n",
    "# 결과 출력 (옵션)\n",
    "print(\"Remaining Features:\", remaining_features)\n",
    "print(\"Training Data Shape:\", list_np_tr.shape)\n",
    "print(\"Testing Data Shape:\", list_np_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqm2_1qvvolg"
   },
   "outputs": [],
   "source": [
    "def label_sep(X, time_dim_first=False):\n",
    "    # 라벨 분리 (마지막 열이 라벨)\n",
    "    y = X[:, -1, -1].reshape(-1, 1)\n",
    "    X = X[:, :, :-1]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_dataset(X, y, time_dim_first=False):\n",
    "    # 필요한 경우, 데이터의 차원을 재배열\n",
    "    if time_dim_first:\n",
    "        X = X.transpose(0, 2, 1)\n",
    "\n",
    "    # 데이터를 PyTorch 텐서로 변환\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    print(\"Shape of y after separating labels:\", y.shape)\n",
    "    print(\"Shape of X after separating labels:\", X.shape)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "\n",
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    # 훈련 및 검증 데이터 로더 생성\n",
    "    # bs는 배치 크기, jobs는 병렬 데이터 로딩을 위한 워커 수\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # 모델의 출력과 타깃 비교하여 정확도 계산\n",
    "    # 가장 높은 확률 값을 가진 클래스 선택\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuF5z3nR9uLP"
   },
   "outputs": [],
   "source": [
    "# 훈련 및 검증 데이터셋에서 라벨 분리\n",
    "np_tr_X, np_tr_y = label_sep(list_np_tr, time_dim_first=False)\n",
    "np_val_X, np_val_y = label_sep(list_np_te, time_dim_first=False)\n",
    "\n",
    "# PyTorch 텐서 데이터셋 생성\n",
    "trn_ds = create_dataset(np_tr_X, np_tr_y, time_dim_first=False)\n",
    "val_ds = create_dataset(np_val_X, np_val_y, time_dim_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K52b0PmX6aOW"
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH3Nz7t91Ycf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 형태에 따른 특성별 평균 계산 함수\n",
    "def calculate_feature_averages(X):\n",
    "    # 각 묶음 별로 평균을 계산\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에서 특성별 평균 계산\n",
    "X_train_avg = calculate_feature_averages(np_tr_X)\n",
    "X_test_avg = calculate_feature_averages(np_val_X)\n",
    "\n",
    "# 라벨의 차원을 조정 (예: [74, 1] -> [74])\n",
    "y_train = np_tr_y.flatten()\n",
    "y_test = np_val_y.flatten()\n",
    "\n",
    "# 모델 훈련\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_avg, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "test_accuracy = rf.score(X_test_avg, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 특성 이름 치환을 위한 딕셔너리 생성\n",
    "feature_name_mapping = {\n",
    "    'lat':r'$C_x$',\n",
    "    'lon':r'$C_y$',\n",
    "    'alt':r'$C_z$',\n",
    "    's_variance_m_s':r'$\\sigma_\\nu$',\n",
    "    'c_variance_rad':r'$\\sigma_g$',\n",
    "    'epv':r'$v_p$',\n",
    "    'hdop':r'$h_d$',\n",
    "    'vdop':r'$v_d$',\n",
    "    'noise_per_ms':r'$\\eta$',\n",
    "    'jamming_indicator':r'$\\vartheta$',\n",
    "    'vel_m_s':r'$\\nu$',\n",
    "    'vel_n_m_s':r'$\\nu_N$',\n",
    "    'vel_e_m_s':r'$\\nu_E$',\n",
    "    'vel_d_m_s':r'$\\nu_D$',\n",
    "    'cog_rad':r'$g$',\n",
    "    'satellites_used':r'$n_s$'\n",
    "}\n",
    "\n",
    "# 치환된 특성 이름 사용\n",
    "mapped_feature_names = [feature_name_mapping.get(feature, feature) for feature in remaining_features[:-1]]\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(4, 4))\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=mapped_feature_names)\n",
    "\n",
    "# 상위 몇개의 특성만 표기\n",
    "fts = 8                 # 표기할 특성의 개수\n",
    "top_features = feat_importances.sort_values(ascending=True).tail(fts)\n",
    "top_features.plot(kind='barh')\n",
    "plt.xlabel('Importance',fontsize=14)\n",
    "plt.ylabel('Features',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR2EH9mNvv3Y"
   },
   "outputs": [],
   "source": [
    "# y 레이블의 불순값 검사\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dl, valid_dl = create_loaders(trn_ds, val_ds, bs=147, jobs=cpu_count())\n",
    "\n",
    "# DataLoader에서 첫 번째 배치를 가져옴\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "\n",
    "# y_batch의 유일한 값들 출력\n",
    "unique_values = torch.unique(y_batch)\n",
    "print(\"Unique values in y_batch:\", unique_values)\n",
    "\n",
    "# 0과 1 이외의 값이 있는지 확인\n",
    "invalid_values = y_batch[(y_batch != 0) & (y_batch != 1)]\n",
    "if len(invalid_values) > 0:\n",
    "    print(\"Invalid values found in y_batch:\", invalid_values)\n",
    "else:\n",
    "    print(\"No invalid values found in y_batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5hL6HO1V4AU"
   },
   "source": [
    "### Sequence Length Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0AFil--U51l"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def create_seq_data(X, seq_len=2):\n",
    "    num_samples, num_timesteps, num_features = X.shape\n",
    "    num_seq = num_timesteps - seq_len + 1\n",
    "\n",
    "    new_X = np.zeros((num_samples * num_seq, seq_len, num_features - 1))\n",
    "    new_y = np.zeros((num_samples * num_seq, 1))\n",
    "    count_legit_0 = 0\n",
    "    count_legit_1 = 0\n",
    "\n",
    "    for i in range(num_seq):\n",
    "        new_X[i * num_samples:(i + 1) * num_samples, :, :] = X[:, i:i+seq_len, :-1]\n",
    "        labels = X[:, i+seq_len-1, -1]\n",
    "        new_y[i * num_samples:(i + 1) * num_samples, 0] = labels\n",
    "        count_legit_0 += np.sum(labels == 0)\n",
    "\n",
    "    new_X_tensor = torch.tensor(new_X, dtype=torch.float32)\n",
    "    new_y_tensor = torch.tensor(new_y, dtype=torch.long)\n",
    "\n",
    "    print(f\"Instances with legitimate=0: {count_legit_0}\")\n",
    "    print(f\"Instances with legitimate=1: {count_legit_1}\")\n",
    "\n",
    "    return TensorDataset(new_X_tensor, new_y_tensor), (count_legit_0, count_legit_1)\n",
    "\n",
    "# sequence length 지정\n",
    "seq_len = 75\n",
    "\n",
    "# 수행\n",
    "ts_seq_tr, counts_tr = create_seq_data(list_np_tr, seq_len)\n",
    "ts_seq_te, counts_te = create_seq_data(list_np_te, seq_len)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "# 데이터셋을 서로 바꿔서 수행\n",
    "# 원래는 훈련용 데이터였던 list_np_tr을 테스트로, 테스트용 데이터였던 list_np_te를 훈련으로 사용\n",
    "ts_seq_te, counts_te = create_seq_data(list_np_tr, seq_len)  # 원래 훈련 데이터 -> 테스트 데이터로 사용\n",
    "ts_seq_tr, counts_tr = create_seq_data(list_np_te, seq_len)  # 원래 테스트 데이터 -> 훈련 데이터로 사용\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "print(\"Training Data Tensor Size:\", ts_seq_tr.tensors[0].size())\n",
    "print(\"Training Labels Tensor Size:\", ts_seq_tr.tensors[1].size())\n",
    "print(\"Testing Data Tensor Size:\", ts_seq_te.tensors[0].size())\n",
    "print(\"Testing Labels Tensor Size:\", ts_seq_te.tensors[1].size())\n",
    "\n",
    "# train과 test TensorDataset을 결합하여 combined_dataset 생성\n",
    "combined_dataset = ConcatDataset([ts_seq_tr, ts_seq_te])\n",
    "\n",
    "# 결합된 데이터셋 크기 확인\n",
    "print(\"Combined Dataset - Total Instances:\", len(combined_dataset))\n",
    "print(\"First Entry - Data Shape:\", combined_dataset[0][0].shape)\n",
    "print(\"First Entry - Label Shape:\", combined_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBoM0E4wNZmG"
   },
   "source": [
    "### Masking (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ac5BJ_zNYp4"
   },
   "outputs": [],
   "source": [
    "# 마스킹 함수 정의 및 적용\n",
    "def mask_sequence_data(X, mask_percentage=0.15):\n",
    "    num_samples, seq_len, num_features = X.shape\n",
    "    mask_size = int(np.ceil(mask_percentage * seq_len))\n",
    "    masked_indices = np.random.randint(0, seq_len, (num_samples, mask_size))\n",
    "    X_masked = X.clone()\n",
    "    mask_array = torch.zeros_like(X, dtype=torch.bool)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        X_masked[i, masked_indices[i], :] = 0\n",
    "        mask_array[i, masked_indices[i], :] = True\n",
    "\n",
    "    return X_masked, mask_array\n",
    "\n",
    "def visualize_masking(X_masked, num_samples=5):\n",
    "    for i in range(num_samples):\n",
    "        print(\"Sample\", i, \":\")\n",
    "        print(X_masked[i])\n",
    "\n",
    "# 마스킹 함수 호출\n",
    "X_masked, _ = mask_sequence_data(torch.tensor(np.random.randn(10, 5, 3), dtype=torch.float32), mask_percentage=0.15) # 마스킹 비율\n",
    "visualize_masking(X_masked)\n",
    "\n",
    "# 마스킹 적용\n",
    "masked_tr_X, original_tr_X = mask_sequence_data(ts_seq_tr.tensors[0])\n",
    "masked_te_X, original_te_X = mask_sequence_data(ts_seq_te.tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65odnf1vv0md"
   },
   "source": [
    "## Building RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xheczHc1vy_k"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        # 생성자: 스케줄러를 초기화\n",
    "        assert callable(schedule)  # schedule이 호출 가능한 함수인지 확인\n",
    "        self.schedule = schedule   # 학습률을 조정하는 함수 저장\n",
    "        super().__init__(optimizer, last_epoch)  # 부모 클래스의 생성자 호출\n",
    "\n",
    "    def get_lr(self):\n",
    "        # 현재 에포크에 대한 학습률을 계산\n",
    "        # base_lrs (기본 학습률)에 대해 schedule 함수를 호출하여 새로운 학습률 계산\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C1ofDvsv38p"
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    # 코사인 학습률 스케줄링 함수를 생성하는 함수\n",
    "    # t_max: 학습률 순환 주기\n",
    "    # eta_min: 최소 학습률\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        # 스케줄러 함수: 각 에포크에 대한 학습률 계산\n",
    "        # epoch: 현재 에포크 번호\n",
    "        # base_lr: 기본 학습률\n",
    "\n",
    "        # 현재 순환 주기 내의 상대적 위치 계산\n",
    "        t = epoch % t_max\n",
    "\n",
    "        # 코사인 기반의 학습률 계산\n",
    "        # 학습률은 eta_min과 base_lr 사이에서 조정됨\n",
    "        return eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * t / t_max)) / 2\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIX-V-Gfv5oY"
   },
   "outputs": [],
   "source": [
    "# 코사인 학습률 스케줄러를 사용하기 위한 설정\n",
    "n = 100  # 순환 주기\n",
    "sched = cosine(n)  # 코사인 학습률 스케줄러 생성\n",
    "\n",
    "# 주어진 범위(n * 4)에 대해 각 에포크별로 학습률 계산\n",
    "lrs = [sched(t, 1) for t in range(n * 4)]  # t: 에포크, 1: 기본 학습률\n",
    "\n",
    "plt.plot(lrs)  # 학습률 변화 그래프 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNpSdhRXv7WS"
   },
   "outputs": [],
   "source": [
    "print('Preparing datasets')\n",
    "\n",
    "# Sequence Dataset 생성\n",
    "trn_ds = ts_seq_tr\n",
    "val_ds = ts_seq_te\n",
    "\n",
    "bs = 128  # 배치 크기 설정\n",
    "print(f'Creating data loaders with batch size: {bs}')\n",
    "# 데이터 로더 생성\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs=bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf6psYmK8wxu"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZgrZaSHv-aE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "# Random Seed 설정\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPU가 사용 가능한 경우, CUDA random seed도 설정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        # 생성자: 모듈 초기화\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[64, 32, 16, output_dim], activation_layer=nn.ELU) # 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순방향 패스 정의\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        # 초기 히든 상태와 셀 상태 생성\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            return [t.cuda() for t in (h0, c0)]\n",
    "        else:\n",
    "            return [h0, c0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnwiNEdP84Aq"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwphKfrXwmu8"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, 16, output_dim], activation_layer=nn.GELU)  # 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.init_hidden(x)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY0hWje66gQv"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFYLHXhG_MQU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import MLP\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Random Seed 설정\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPU가 사용 가능한 경우, CUDA random seed도 설정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# TransformerClassifier 클래스를 정의\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nhead, num_layers, output_dim, max_len=5000):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, output_dim], activation_layer=nn.LeakyReLU)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)\n",
    "        # 평균 풀링을 사용하여 전체 시퀀스 정보 활용\n",
    "        transformer_out = transformer_out.mean(dim=1)\n",
    "        output = self.mlp(transformer_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZtBC7tY9A3u"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAqrZN-T9AK0"
   },
   "outputs": [],
   "source": [
    "# 모델 구성에 필요한 파라미터 설정\n",
    "input_dim = 10  # 입력 특성의 수\n",
    "hidden_dim = 256  # Context vector size\n",
    "layer_dim = 3 # Stack number\n",
    "num_layers = layer_dim\n",
    "output_dim = 1  # 출력 차원: 이진 분류의 경우 1\n",
    "nhead = 4  # 멀티 헤드 어텐션의 헤드 수\n",
    "\n",
    "# 학습률 및 에포크 설정\n",
    "lr = 0.0005\n",
    "n_epochs = 1000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "# best_acc = 0\n",
    "# patience, trials = 100, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6re3Ab37Yeo9"
   },
   "source": [
    "# Train & Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TspvhfNM9Jyp"
   },
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxpxEprd9LYl"
   },
   "outputs": [],
   "source": [
    "분류 모델 인스턴스 생성\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = GRUClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = TransformerClassifier(input_dim, hidden_dim, nhead, num_layers, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4HzZ2yUKxaI"
   },
   "source": [
    "## Process Complete Alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9Ja8v9UK5GE"
   },
   "outputs": [],
   "source": [
    "# 텔레그램으로 학습완료 알림 오는 서비스\n",
    "# https://federicoraimondi.github.io/myProjects/Data_Stuff/knockknock_tutorial/Knock_Knock_tutorial.html\n",
    "!pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qggw4DjlK7PY"
   },
   "outputs": [],
   "source": [
    "# Put your token as a string\n",
    "your_token = \"token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "873GQw9zLC60"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's get your chat id! Be sure to have sent a message to your bot.\n",
    "url = 'https://api.telegram.org/bot'+str(your_token)+'/getUpdates'\n",
    "response = requests.get(url)\n",
    "myinfo = response.json()\n",
    "if response.status_code == 401:\n",
    "  raise NameError('Check if your token is correct.')\n",
    "\n",
    "try:\n",
    "  CHAT_ID: int = myinfo['result'][1]['message']['chat']['id']\n",
    "\n",
    "  print('This is your Chat ID:', CHAT_ID)\n",
    "\n",
    "except:\n",
    "  print('Have you sent a message to your bot? Telegram bot are quite shy 🤣.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05cHPA_tYPau"
   },
   "source": [
    "## Traning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CCG3t0ZMh42"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from telegram import Bot\n",
    "\n",
    "# Colab에서 asyncio 사용을 위한 준비\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 비동기로 메시지를 보내는 함수 정의\n",
    "async def async_send_telegram_message(text, chat_id, token):\n",
    "    bot = Bot(token=token)\n",
    "    await bot.send_message(chat_id=chat_id, text=text)\n",
    "\n",
    "# 비동기 함수를 실행하는 함수 정의\n",
    "def send_telegram_message(text, chat_id, token):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = loop.create_task(async_send_telegram_message(text, chat_id, token))\n",
    "    loop.run_until_complete(task)\n",
    "\n",
    "# 모델 학습 함수\n",
    "# 모델 학습 함수에서 손실 계산 부분 수정\n",
    "def train_model(model, trn_dl, val_dl, lr, n_epochs, chat_id, token, masked=False):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc = 0  # 최고 정확도 초기화\n",
    "    patience, trials = 100, 0\n",
    "\n",
    "    print('Start model training')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "            if masked:\n",
    "                x_batch, mask_array = mask_sequence_data(x_batch)\n",
    "                mask_array = mask_array.cuda() if torch.cuda.is_available() else mask_array\n",
    "\n",
    "            x_batch = x_batch.cuda() if torch.cuda.is_available() else x_batch\n",
    "            y_batch = y_batch.cuda() if torch.cuda.is_available() else y_batch\n",
    "\n",
    "            opt.zero_grad()\n",
    "            out = model(x_batch)\n",
    "            loss = criterion(out, y_batch.float())\n",
    "            if masked:\n",
    "                loss = (loss * mask_array.float()).sum() / mask_array.float().sum()\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # 실제 레이블과 예측 레이블을 저장할 리스트\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # 검증 데이터셋을 이용한 평가\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        correct, total = 0, 0\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = [t.cuda() for t in (x_val, y_val)]  # 데이터를 CUDA로 이동\n",
    "            out = model(x_val)\n",
    "            probs = torch.sigmoid(out)  # 시그모이드 함수 적용하여 확률 계산\n",
    "            preds = probs > 0.5  # 임계값(0.5)을 기준으로 예측값 결정\n",
    "            # preds = F.log_softmax(out, dim=1).argmax(dim=1)  # 예측값 계산\n",
    "            total += y_val.size(0)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "\n",
    "        acc = correct / total  # 정확도 계산\n",
    "\n",
    "        # 에포크별 진행 상황 출력\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch: {epoch:3d}. Loss: {loss.item():.8f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "        # 최고 정확도 갱신 및 모델 저장\n",
    "        if acc > best_acc:\n",
    "            trials = 0\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best.pth')\n",
    "            print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    # 훈련이 완료되었음을 알리고, 가장 좋은 성능을 보였던 모델의 가중치를 복원\n",
    "    print('The training is finished! Restoring the best model weights')\n",
    "\n",
    "    # 가장 좋은 모델의 가중치를 불러옴\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "\n",
    "    final_message = '다해떠'\n",
    "    send_telegram_message(final_message, chat_id, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y9_kybywCor"
   },
   "outputs": [],
   "source": [
    "train_model(model, trn_dl, val_dl, lr, n_epochs, CHAT_ID, your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffm2UI8rOeUY"
   },
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FFuNfQ91EAA"
   },
   "outputs": [],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOR-KTV8wDz0"
   },
   "outputs": [],
   "source": [
    "import time  # 시간 측정을 위한 모듈 추가\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "test_predictions = []  # 예측 결과를 저장할 리스트\n",
    "print('Predicting on test dataset')\n",
    "\n",
    "start_time = time.time()  # 테스트 시작 시간 기록\n",
    "\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    for x_val, _ in val_dl:  # 테스트 데이터 로더(val_dl) 사용\n",
    "        x_val = x_val.cuda() if torch.cuda.is_available() else x_val  # GPU 사용 가능 시, 데이터를 GPU로 이동\n",
    "        out = model(x_val)  # 모델에 배치 데이터 전달 및 예측 수행\n",
    "        probs = torch.sigmoid(out)  # 시그모이드 함수 적용하여 확률 계산\n",
    "        preds = probs > 0.5  # 임계값(0.5)을 기준으로 예측값 결정\n",
    "        preds = preds.long()  # Bool 타입을 Long 타입으로 변환 (예측값을 0 또는 1로 변환)\n",
    "        test_predictions += preds.cpu().tolist()  # 예측 결과를 CPU로 이동한 후 리스트에 추가\n",
    "\n",
    "end_time = time.time()  # 테스트 종료 시간 기록\n",
    "test_time = end_time - start_time  # 테스트에 걸린 시간 계산\n",
    "print(f\"Test time: {test_time:.4f} seconds\")\n",
    "\n",
    "# 실제 레이블과 예측 레이블 준비\n",
    "y_true = []\n",
    "y_pred = test_predictions  # 예측 레이블 (위의 예측 코드 결과)\n",
    "\n",
    "for _, y_batch in val_dl:  # 실제 레이블 추출\n",
    "    y_true += y_batch.tolist()\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# F1 점수 계산\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53625qr_qFi"
   },
   "outputs": [],
   "source": [
    "# 잘못 분류된 데이터 포인트의 인덱스 찾기\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "\n",
    "# 잘못 분류된 데이터 포인트의 수 출력\n",
    "print(f\"Number of misclassified points: {len(misclassified_indices)}\")\n",
    "\n",
    "# numpy는 column 이름이 없었기 때문에 column 이름 다시 만들어주기\n",
    "column_names = dict_split_te_attack['35m_dyn2'][3].columns\n",
    "column_list = list(column_names)\n",
    "value_to_remove = drop_cols + ['legitimate']\n",
    "for values in value_to_remove:\n",
    "    if values in column_list:\n",
    "        column_list.remove(values)\n",
    "\n",
    "# 잘못 분류된 모든 데이터 포인트에 대한 정보 출력\n",
    "for index in misclassified_indices:\n",
    "    # 'val_ds[index]'로부터 데이터와 레이블 분리\n",
    "    data_tensor, label_tensor = val_ds[index]\n",
    "\n",
    "    # 데이터 텐서를 NumPy 배열로 변환\n",
    "    numpy_data = data_tensor.numpy()\n",
    "\n",
    "    # NumPy 배열을 Pandas 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(numpy_data, columns=column_list)\n",
    "\n",
    "    # 레이블 확인\n",
    "    label = label_tensor.item()\n",
    "\n",
    "    print(f\"Data Point Index: {index}\")\n",
    "    print(\"Data Frame:\\n\", df)\n",
    "    print(\"Label:\", label)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilDANzgxoak_"
   },
   "source": [
    "# K-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MB0jDbqoZ2y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 훈련 및 테스트 비율 지정\n",
    "ratio_tr = 0.8  # 훈련 데이터 비율\n",
    "ratio_te = 1 - ratio_tr  # 테스트 데이터 비율\n",
    "\n",
    "# k-fold 교차 검증 파라미터\n",
    "kf = 5\n",
    "k = kf  # 지정된 k-fold 수\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# 성능 지표 저장용 리스트\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_fprs = []\n",
    "fold_fnrs = []\n",
    "\n",
    "# K-Fold 교차 검증 시작\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(combined_dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k}\")\n",
    "\n",
    "    # 지정된 비율로 훈련과 검증 데이터 분리\n",
    "    num_train = int(len(train_idx) * ratio_tr)  # 훈련 데이터 비율에 따라 인덱스 분할\n",
    "    tr_idx, val_idx = train_idx[:num_train], train_idx[num_train:]  # tr_idx: 훈련, val_idx: 검증\n",
    "\n",
    "    # Subset을 통해 train과 validation 데이터 생성\n",
    "    trn_ds = Subset(combined_dataset, tr_idx)\n",
    "    val_ds = Subset(combined_dataset, val_idx)\n",
    "\n",
    "    # 데이터 로더 생성 (배치 크기 등 기존 코드 사용)\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=cpu_count())\n",
    "    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=cpu_count())\n",
    "\n",
    "    # 모델 초기화 (원하는 모델로 설정)\n",
    "    # model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)  # 예: LSTM\n",
    "    # model = GRUClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    model = TransformerClassifier(input_dim, hidden_dim, nhead, num_layers, output_dim)\n",
    "\n",
    "    # 모델 훈련\n",
    "    train_model(model, trn_dl, val_dl, lr, n_epochs, CHAT_ID, your_token)\n",
    "\n",
    "    # 모델 평가\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "            out = model(x_val)\n",
    "            probs = torch.sigmoid(out)\n",
    "            preds = (probs > 0.5).long()\n",
    "            y_true += y_val.cpu().tolist()\n",
    "            y_pred += preds.cpu().tolist()\n",
    "\n",
    "    # fold 성능 계산 및 저장\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Confusion matrix에서 TN, FP, FN, TP 추출하여 FPR과 FNR 계산\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0\n",
    "\n",
    "    # 성능 지표 저장\n",
    "    fold_accuracies.append(acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_fprs.append(fpr)\n",
    "    fold_fnrs.append(fnr)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# 최종 결과 요약\n",
    "print(\"\\nK-Fold Cross Validation Results:\")\n",
    "print(f\"Mean Accuracy: {sum(fold_accuracies) / k:.4f}\")\n",
    "print(f\"Mean F1 Score: {sum(fold_f1_scores) / k:.4f}\")\n",
    "print(f\"Mean Precision: {sum(fold_precisions) / k:.4f}\")\n",
    "print(f\"Mean Recall: {sum(fold_recalls) / k:.4f}\")\n",
    "print(f\"Mean FPR: {sum(fold_fprs) / k:.4f}\")\n",
    "print(f\"Mean FNR: {sum(fold_fnrs) / k:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMf1ak8j7Fml8S0sL0NXEJu",
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
