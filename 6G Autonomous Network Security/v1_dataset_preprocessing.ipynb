{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks7Yzqp2EN-k"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbvEHtqpZH2R"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/to/project_directory')\n",
    "\n",
    "from config import *\n",
    "from util import get_center_points, is_same_length, is_list_strings, move_to_zero, move_to_zero_all, get_parsed_timestamp, process_dataframe\n",
    "# from Collected_dataset.dataloader import\n",
    "# from Collected_dataset.model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_hWSKKdHtyP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDNN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlsTaVPUEc7t"
   },
   "source": [
    "# File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM8DwYPsZ6E5"
   },
   "outputs": [],
   "source": [
    "dict_path_tr_clean = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/clean/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/clean/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/clean/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/clean/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/clean/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/clean/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/clean/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/clean/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/clean/70m_dyn2.csv'),\n",
    "}\n",
    "dict_path_tr_attack = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/attack/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/attack/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/attack/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/attack/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/attack/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/attack/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/attack/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/attack/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/attack/70m_dyn2.csv'),\n",
    "}\n",
    "dict_path_te_clean = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/clean/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/clean/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/clean/35m_dyn2.csv'),\n",
    "}\n",
    "dict_path_te_attack = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/attack/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/attack/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/attack/35m_dyn2.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfhzbADsExoh"
   },
   "outputs": [],
   "source": [
    "#placeholder dictionary for collected dataframes\n",
    "dict_df_tr_clean={}\n",
    "dict_df_tr_attack={}\n",
    "dict_df_te_clean={}\n",
    "dict_df_te_attack={}\n",
    "\n",
    "#control variables\n",
    "dict_path=dict_path_tr_clean #path variable\n",
    "dict_df=dict_df_tr_clean #dict variable to collect dataframes read from paths\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_clean.items()})\n",
    "\n",
    "dict_path=dict_path_tr_attack\n",
    "dict_df=dict_df_tr_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_attack.items()})\n",
    "\n",
    "dict_path=dict_path_te_clean\n",
    "dict_df=dict_df_te_clean\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_clean.items()})\n",
    "\n",
    "dict_path=dict_path_te_attack\n",
    "dict_df=dict_df_te_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_attack.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbhJ_EMKNEhj"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh_jbaEYclAG"
   },
   "outputs": [],
   "source": [
    "# Make a new dictionary to store preprocessed df\n",
    "preprocessed_dict_tr_clean={}\n",
    "preprocessed_dict_tr_attack={}\n",
    "preprocessed_dict_te_clean={}\n",
    "preprocessed_dict_te_attack={}\n",
    "\n",
    "# Call def process_dataframe and store df\n",
    "point_numbers=0\n",
    "for key, df in dict_df_tr_clean.items():\n",
    "    if key not in preprocessed_dict_tr_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "        point_numbers+=len(df)\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_clean.items()})\n",
    "\n",
    "for key, df in dict_df_tr_attack.items():\n",
    "    if key not in preprocessed_dict_tr_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_attack.items()})\n",
    "\n",
    "for key, df in dict_df_te_clean.items():\n",
    "    if key not in preprocessed_dict_te_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_clean.items()})\n",
    "\n",
    "for key, df in dict_df_te_attack.items():\n",
    "    if key not in preprocessed_dict_te_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_attack.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DoFtXN5YVa8"
   },
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfmLCUgYB9QX"
   },
   "outputs": [],
   "source": [
    "# labeling 0 for clean, 1 for attack\n",
    "for key, df in preprocessed_dict_tr_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_tr_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1\n",
    "for key, df in preprocessed_dict_te_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_te_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uckQf5_qNkY"
   },
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4SPRZTLmxWV"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dict_tr_clean.keys())\n",
    "print(preprocessed_dict_tr_attack.keys())\n",
    "print(preprocessed_dict_te_clean.keys())\n",
    "print(preprocessed_dict_te_attack.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b3iS0bQqQkL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import split_dataframe\n",
    "\n",
    "# split function... 왜인지 자꾸 import 에러 나서 여기 둠\n",
    "def split_dataframe(df):\n",
    "    diff = np.absolute(df['timestamp'].diff().values)\n",
    "    diff[0] = 0.0\n",
    "    diff = pd.Series(diff.round(decimals=5))\n",
    "    split_indices = np.where(diff >= 1.0)[0]\n",
    "    split_indices = [0] + list(split_indices) + [len(diff)]\n",
    "    split_dfs = []\n",
    "\n",
    "    for i, split_index in enumerate(split_indices):\n",
    "        if i == len(split_indices) - 1:\n",
    "            break\n",
    "        start_index = split_index\n",
    "        end_index = split_indices[i + 1]\n",
    "        split_dfs.append(df.iloc[start_index:end_index])\n",
    "\n",
    "    return split_dfs\n",
    "\n",
    "# preprocessed_dict와 동일한 key를 가진 딕셔너리를 만들고, item으로는 분할된 데이터프레임이 append된 리스트를 넣음\n",
    "dict_split_tr_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_clean.items()}\n",
    "dict_split_tr_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_attack.items()}\n",
    "dict_split_te_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_te_clean.items()}\n",
    "dict_split_te_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_te_attack.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yu0gBf-7PdJ"
   },
   "outputs": [],
   "source": [
    "# Check length of the splitted dataset\n",
    "for key, split_list in dict_split_tr_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_tr_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSNVnjtHYIFM"
   },
   "outputs": [],
   "source": [
    "def count_nan(df_list):\n",
    "    nan_counts = sum(df.isna().values.sum() for df in df_list)\n",
    "    return f'({nan_counts})'\n",
    "\n",
    "len_dict_tr_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_clean.items()}\n",
    "len_dict_tr_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_attack.items()}\n",
    "len_dict_te_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_clean.items()}\n",
    "len_dict_te_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_attack.items()}\n",
    "\n",
    "print(len_dict_tr_clean)\n",
    "print(len_dict_tr_attack)\n",
    "print(len_dict_te_clean)\n",
    "print(len_dict_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBZNFHaqS_YZ"
   },
   "source": [
    "# Creating new Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMcSoTtGsMzl"
   },
   "outputs": [],
   "source": [
    "# Creating new columns = 'D_lon', 'D_lat', 'D_alt' : difference between two consecutive coordinates\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None) # <==== 경고를 끈다\n",
    "\n",
    "def add_diff(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 각 차분을 계산하고 'legitimate' 열 바로 앞에 삽입합니다.\n",
    "            df.insert(legitimate_index, 'D_lat', df['lat'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 1, 'D_lon', df['lon'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 2, 'D_alt', df['alt'].diff().fillna(0))\n",
    "    return dict_split\n",
    "\n",
    "\n",
    "add_diff(dict_split_tr_clean)\n",
    "add_diff(dict_split_tr_attack)\n",
    "add_diff(dict_split_te_clean)\n",
    "add_diff(dict_split_te_attack)\n",
    "\n",
    "dict_split_tr_clean['20m_dyn1'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dETcVgd44_Zb"
   },
   "outputs": [],
   "source": [
    "# Creating new column = 'd_pos' : Ucladian distance between two consecutive coordinates\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def calculate_euc_distance(df):\n",
    "    coordinates = df[['lat', 'lon', 'alt']].to_numpy()\n",
    "    distances = [0.0]  # 첫 번째 행에는 0을 할당\n",
    "    for i in range(1, len(coordinates)):\n",
    "        distance = euclidean_distances([coordinates[i-1]], [coordinates[i]])[0][0]\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "def add_euc(dict_split):\n",
    "    # 각 데이터프레임에 대해 유클리드 거리 계산 및 'd_pos' 열 추가\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 유클리드 거리 계산\n",
    "            distances = calculate_euc_distance(df)\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'd_pos' 열을 'legitimate' 열 바로 전에 추가\n",
    "            df.insert(legitimate_index, 'd_pos', distances)\n",
    "    return dict_split\n",
    "\n",
    "add_euc(dict_split_tr_clean)\n",
    "add_euc(dict_split_tr_attack)\n",
    "add_euc(dict_split_te_clean)\n",
    "add_euc(dict_split_te_attack)\n",
    "\n",
    "dict_split_tr_clean['20m_dyn1'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l42ALKjlHMu_"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'time_gap' : Time gap between two consecutive points\n",
    "\n",
    "def add_timeGap(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'timestamp' 열의 차분을 계산하고 'legitimate' 열 바로 앞에 삽입\n",
    "            df.insert(legitimate_index, 'time_gap', df['timestamp'].diff().fillna(0))\n",
    "    return dict_split\n",
    "\n",
    "add_timeGap(dict_split_tr_clean)\n",
    "add_timeGap(dict_split_tr_attack)\n",
    "add_timeGap(dict_split_te_clean)\n",
    "add_timeGap(dict_split_te_attack)\n",
    "\n",
    "dict_split_tr_attack['20m_dyn1'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTkoU4wDYR1i"
   },
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp7S6kadYb8Z"
   },
   "outputs": [],
   "source": [
    "# lists for concatenating items in dictionaries\n",
    "list_tr_clean=[]\n",
    "list_tr_attack=[]\n",
    "list_te_clean=[]\n",
    "list_te_attack=[]\n",
    "\n",
    "def merge_df(dict_split, dict_total):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            dict_total.append(df)\n",
    "    return dict_total\n",
    "\n",
    "list_tr_clean = []\n",
    "list_tr_attack = []\n",
    "list_te_clean = []\n",
    "list_te_attack = []\n",
    "\n",
    "merge_df(dict_split_tr_clean, list_tr_clean)\n",
    "merge_df(dict_split_tr_attack, list_tr_attack)\n",
    "merge_df(dict_split_te_clean, list_te_clean)\n",
    "merge_df(dict_split_te_attack, list_te_attack)\n",
    "\n",
    "# 'D_lat', 'D_lon', 'D_alt', 'd_pos', 'time_gap','vel_n_m_s','vel_e_m_s','vel_d_m_s','cog_rad',\n",
    "# feature 2개만 사용하고도 purdue처럼 높은 정확도를 낼 수 있다는걸 보여주기 위한 추가 drop list\n",
    "list_additional_drop = ['D_lat', 'D_lon', 'D_alt', 'd_pos', 'time_gap', 'vdop', 'hdop', 'epv', 'c_variance_rad','eph', 's_variance_m_s', 'noise_per_ms', 'jamming_indicator', 'satellites_used'] # ['s_variance_m_s', 'epv', 'eph', 'hdop', 'vdop','vel_m_s', 'vel_n_m_s', 'vel_e_m_s', 'vel_d_m_s', 'cog_rad', 'satellites_used']\n",
    "# concatenating all dataframes of each training, test sets\n",
    "# drop timestamp and locations column\n",
    "# if 'timestamp' in list_tr_clean.columns:     # safety\n",
    "df_concat_tr=pd.concat(list_tr_clean+list_tr_attack, axis=0).reset_index(drop=True).drop(['timestamp', 'lat','lon','alt','alt_ellipsoid']+list_additional_drop, axis=1, inplace=False)\n",
    "# if 'timestamp' in list_te_clean.columns:     # safety\n",
    "df_concat_te=pd.concat(list_te_clean+list_te_attack, axis=0).reset_index(drop=True).drop(['timestamp', 'lat','lon','alt','alt_ellipsoid']+list_additional_drop, axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9slXTetyJhrF"
   },
   "outputs": [],
   "source": [
    "df_concat_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryZLMxSjY9O8"
   },
   "source": [
    "# File Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkB2wvs4ZMQd"
   },
   "outputs": [],
   "source": [
    "tr_fin=df_concat_tr\n",
    "te_fin=df_concat_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhjGymKtY_75"
   },
   "outputs": [],
   "source": [
    "# exporting raw\n",
    "from config import path_merged, path_split\n",
    "if not osex(path_merged):\n",
    "    mkdir(path_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDc3NQ90Z7LE"
   },
   "outputs": [],
   "source": [
    "tr_fin.to_csv(ospj(path_merged, 'tr.csv'))\n",
    "tr_x=tr_fin.iloc[:, :-1]\n",
    "tr_x.to_csv(ospj(path_merged, 'tr_x.csv'))\n",
    "tr_y=tr_fin.iloc[:, -1]\n",
    "tr_y.to_csv(ospj(path_merged, 'tr_y.csv'))\n",
    "te_fin.to_csv(ospj(path_merged, 'te.csv'))\n",
    "te_x=te_fin.iloc[:, :-1]\n",
    "te_x.to_csv(ospj(path_merged, 'te_x.csv'))\n",
    "te_y=te_fin.iloc[:, -1]\n",
    "te_y.to_csv(ospj(path_merged, 'te_y.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuxrNsmiafAY"
   },
   "source": [
    "# Evaluation with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXvDS5tNaiFN"
   },
   "outputs": [],
   "source": [
    "from re import VERBOSE\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "seed=0\n",
    "models=dict(\n",
    "    svc=SVC(random_state=seed),\n",
    "    knn=KNeighborsClassifier(),\n",
    "    rfc=RandomForestClassifier(random_state=seed),\n",
    "    xgb=XGBClassifier(random_state=seed, use_label_encoder=False, verbosity=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr0JrWtSoYC5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 필요한 딕셔너리들\n",
    "dict_val_pred = {}\n",
    "dict_test_pred = {}\n",
    "dict_f1_scores = {m_name: [] for m_name in models.keys()}  # 모델별 F1 점수를 저장할 딕셔너리\n",
    "dict_test_time = {m_name: [] for m_name in models.keys()}  # 모델별 테스트 시간을 저장할 딕셔너리\n",
    "dict_fpr = {m_name: [] for m_name in models.keys()}  # 모델별 FPR을 저장할 딕셔너리\n",
    "dict_confusion_matrix = {}\n",
    "\n",
    "# 각 모델에 대해 5번 훈련 및 테스트 진행\n",
    "for i in range(5):\n",
    "    print(f\"Starting iteration {i+1}\\n\")\n",
    "\n",
    "    for m_name, model in models.items():\n",
    "        print(f\"Training {m_name} - Run {i+1}\")\n",
    "\n",
    "        # 모델 훈련\n",
    "        model.fit(tr_x, tr_y)\n",
    "\n",
    "        # Validation 예측\n",
    "        dict_val_pred[m_name] = model.predict(tr_x)\n",
    "\n",
    "        # 테스트 시간 측정 시작\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Test 예측\n",
    "        dict_test_pred[m_name] = model.predict(te_x)\n",
    "\n",
    "        # 테스트 시간 측정 종료\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time  # 예측에 걸린 시간 계산\n",
    "        test_time = int(test_time * 1000) / 1000  # 소수점 3자리까지만 자름\n",
    "        dict_test_time[m_name].append(test_time)  # 테스트 시간 저장\n",
    "\n",
    "        # F1 Score 계산\n",
    "        f1 = f1_score(te_y, dict_test_pred[m_name], average='weighted')\n",
    "        f1 = int(f1 * 1000) / 1000  # 소수점 3자리까지만 자름\n",
    "        dict_f1_scores[m_name].append(f1)  # F1 점수 저장\n",
    "\n",
    "        # Confusion Matrix 계산 및 출력 (옵션)\n",
    "        cm = confusion_matrix(te_y, dict_test_pred[m_name])\n",
    "        dict_confusion_matrix[m_name] = cm\n",
    "\n",
    "        # Confusion Matrix 요소 추출\n",
    "        TN = cm[0, 0]\n",
    "        FP = cm[0, 1]\n",
    "\n",
    "        # FPR 계산\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        fpr = int(fpr * 1000) / 1000  # 소수점 3자리까지만 자름\n",
    "        dict_fpr[m_name].append(fpr)  # FPR 저장\n",
    "\n",
    "        # Confusion Matrix 출력\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"{m_name} - Run {i+1} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # 현재 실행 결과 출력\n",
    "        print(f\"{m_name} - Run {i+1} F1 Score: {f1}\")\n",
    "        print(f\"{m_name} - Run {i+1} Test Time: {test_time:.3f} seconds\")\n",
    "        print(f\"{m_name} - Run {i+1} FPR: {fpr}\")\n",
    "        print()\n",
    "\n",
    "# 모든 모델의 F1 점수, 테스트 시간, FPR 출력\n",
    "print(\"\\nFinal F1 Scores, Test Times, and FPRs for Each Model:\")\n",
    "for m_name in models.keys():\n",
    "    print(f\"{m_name} - F1 Scores: {dict_f1_scores[m_name]}\")\n",
    "    print(f\"{m_name} - Test Times: {dict_test_time[m_name]}\")\n",
    "    print(f\"{m_name} - FPR: {dict_fpr[m_name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maG-Syj-hQFJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCJJ33QYfPsr"
   },
   "outputs": [],
   "source": [
    "plt.plot(dict_val_pred['svc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6z5YdRthVwr"
   },
   "outputs": [],
   "source": [
    "plt.plot(dict_val_pred['xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bbeEw7AirgS"
   },
   "source": [
    "# DataLoader Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pi6DTC9wjA5m"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataBuilder(Dataset): #version 4 #I think DataLoader should be merged here with kwargs and returned as DataLoader - DataBuilder: No need\n",
    "    def __init__(self, dataset:np.ndarray,\n",
    "             device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        self.x=torch.from_numpy(dataset[:, :-1]).float()\n",
    "        self.y=torch.from_numpy(dataset[:, -1]).float().reshape(-1, 1)\n",
    "        self.len=dataset.shape[0]\n",
    "        self.x.to(device)\n",
    "        self.y.to(device)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaBLrzACsR_P"
   },
   "outputs": [],
   "source": [
    "tr_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyb8_U3pitcr"
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_dataset=DataBuilder(tr_fin.values)\n",
    "train_loader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, drop_last=True,) # num_workers=4, worker_init_fn=seed_worker, generator=g)\n",
    "test_dataset=DataBuilder(te_fin.values)\n",
    "test_loader=DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True,) # num_workers=4, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3353-fYi1SP"
   },
   "source": [
    "# Evaluation with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sJfaKAwkWD4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "from torch import optim\n",
    "torch.manual_seed(1234) # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwqYLzU1ivuo"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 D_in,\n",
    "                 D_l1,\n",
    "                 D_l2,\n",
    "                 D_l3,\n",
    "                 D_l4,\n",
    "                 D_l5,\n",
    "                 lr=1e-3,\n",
    "                 dropout=0.0,\n",
    "                 random_seed=1234):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.random_seed = random_seed  # random_seed를 클래스 변수로 설정\n",
    "\n",
    "        self.l1=nn.Linear(D_in, D_l1)\n",
    "        self.b1=nn.BatchNorm1d(D_l1)\n",
    "        self.a1=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.l2=nn.Linear(D_l1, D_l2)\n",
    "        self.b2=nn.BatchNorm1d(D_l2)\n",
    "        self.a2=nn.ReLU()\n",
    "        self.l3=nn.Linear(D_l2, D_l3)\n",
    "        self.b3=nn.BatchNorm1d(D_l3)\n",
    "        self.a3=nn.ReLU()\n",
    "        self.l4=nn.Linear(D_l3, D_l4)\n",
    "        self.b4=nn.BatchNorm1d(D_l4)\n",
    "        self.a4=nn.ReLU()\n",
    "        self.l5=nn.Linear(D_l4, D_l5)\n",
    "        self.b5=nn.BatchNorm1d(D_l5)\n",
    "        self.a5=nn.ReLU()\n",
    "        self.output_layer = nn.Linear(D_l5, 1)\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer=optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):   #to call: mlp(data)\n",
    "        x=self.a1(self.b1(self.l1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x=self.a2(self.b2(self.l2(x)))\n",
    "        x=self.a3(self.b3(self.l3(x)))\n",
    "        x=self.a4(self.b4(self.l4(x)))\n",
    "        x=self.a5(self.b5(self.l5(x)))\n",
    "        #return self.softmax(x)\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "\n",
    "    def train(self, dataLoader, epochs, print_every=10, valid_every=100):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, (x, label) in enumerate(dataLoader):\n",
    "                self.optimizer.zero_grad()\n",
    "                output=self(x)\n",
    "                loss=self.criterion(output, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            average_loss = total_loss / len(dataLoader)\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "            if (epoch + 1) % valid_every == 0:\n",
    "                self.test(dataLoader)\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        validation_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = self(inputs)  # Forward pass\n",
    "                predicted = (outputs >= 0.5).int()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_predictions.extend(predicted.tolist())\n",
    "                all_labels.extend(labels.tolist())\n",
    "                # Calculate validation loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                validation_loss += loss.item()\n",
    "        # Calculate Precision\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        # Calculate Recall\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        print('Accuracy: {:.2f}%'.format(100 * correct / total))       # Calculate Accuracy\n",
    "        print('Validation Loss: {:.2f}'.format(validation_loss / len(test_loader)))  # Calculate Validation Loss\n",
    "        print('F1 Score:', f1)\n",
    "        #print('Confusion Matrix:')\n",
    "        # print(cm)\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"MLP - Confusion Matrix\")\n",
    "        plt.show()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPjX8nagkJvn"
   },
   "outputs": [],
   "source": [
    "#MLP learning\n",
    "D_in=5\n",
    "mlp=MLP(D_in,\n",
    "        D_l1=5,\n",
    "        dropout=0.0,\n",
    "        D_l2=4,\n",
    "        D_l3=3,\n",
    "        D_l4=2,\n",
    "        D_l5=1,\n",
    "        lr=1e-3,\n",
    "        random_seed=1234)\n",
    "\n",
    "mlp.train(train_loader, epochs=1000, print_every=50, valid_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFAMlYcofPic"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "mlp.test(test_loader)\n",
    "# run_testing_only(mlp, test_loader, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO7pGW12JrtP"
   },
   "source": [
    "# Evaluation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRD7qRphpTOz"
   },
   "outputs": [],
   "source": [
    "series_value = 0  # 전역 변수로 series_value 정의\n",
    "\n",
    "def add_df_id(dict_split):\n",
    "    global series_value  # 전역 변수 series_value 사용\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            df_length = len(df)\n",
    "            # 임시 열 생성\n",
    "            series_id_col = [series_value] * df_length\n",
    "            measurement_id_col = list(range(df_length))\n",
    "            # 첫 번째와 두 번째 위치에 열 삽입\n",
    "            df.insert(0, 'series_ID', series_id_col)\n",
    "            df.insert(1, 'measurement_ID', measurement_id_col)\n",
    "            series_value += 1  # series_value 업데이트\n",
    "\n",
    "# 함수 호출\n",
    "add_df_id(dict_split_tr_clean)\n",
    "add_df_id(dict_split_tr_attack)\n",
    "add_df_id(dict_split_te_clean)\n",
    "add_df_id(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wl_BvZRfq3s8"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZMFynaQug8e"
   },
   "outputs": [],
   "source": [
    "# 데이터프레임 길이를 저장할 리스트\n",
    "df_lengths = []\n",
    "\n",
    "# 모든 딕셔너리와 데이터프레임을 순회하며 길이 저장\n",
    "for dict_split in [dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack]:\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            df_lengths.append(len(df))\n",
    "\n",
    "# 최소 길이 찾기\n",
    "min_length = min(df_lengths)\n",
    "\n",
    "# 각 데이터프레임을 min_length로 잘라내기\n",
    "def dflen_min(dict_split):\n",
    "    for split_list in dict_split.values():\n",
    "        for i in range(len(split_list)):\n",
    "            split_list[i] = split_list[i].iloc[:min_length]\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "dflen_min(dict_split_tr_clean)\n",
    "dflen_min(dict_split_tr_attack)\n",
    "dflen_min(dict_split_te_clean)\n",
    "dflen_min(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icGN3bB9a5Av"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RbDl8FiAtPs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from multiprocessing import cpu_count       # 현재 CPU에서 사용 가능한 코어 수를 반환\n",
    "from pathlib import Path        # 경로를 조작\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler       # 높은 학습률로 시작하여 점차 낮추면서 학습하도록 동적 스케줄링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDOfVKJZizBU"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "ID_COLS = ['series_ID', 'measurement_ID']  # 각 데이터 포인트를 구별하는 데 사용되는 열\n",
    "LOC_COLS = ['lat', 'lon', 'alt', 'alt_ellipsoid']\n",
    "\n",
    "# 지정된 열을 제외하고 데이터프레임을 리스트로 변환하는 함수\n",
    "def df_to_np(dict_split, drop_cols):\n",
    "    nested_lists = []\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            # 지정된 열 제외\n",
    "            df_dropped = df.drop(columns=drop_cols)\n",
    "            # 데이터프레임의 각 행을 리스트로 변환\n",
    "            list_of_rows = df_dropped.values.tolist()\n",
    "            # 변환된 리스트들을 포함하는 리스트 추가\n",
    "            nested_lists.append(list_of_rows)\n",
    "    return nested_lists\n",
    "\n",
    "drop_cols = ID_COLS + LOC_COLS\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "list_np_tr_clean = np.array(df_to_np(dict_split_tr_clean, drop_cols))\n",
    "list_np_tr_attack = np.array(df_to_np(dict_split_tr_attack, drop_cols))\n",
    "list_np_te_clean = np.array(df_to_np(dict_split_te_clean, drop_cols))\n",
    "list_np_te_attack = np.array(df_to_np(dict_split_te_attack, drop_cols))\n",
    "\n",
    "# Training 데이터셋 합치기\n",
    "list_np_tr = np.concatenate([list_np_tr_clean, list_np_tr_attack])\n",
    "\n",
    "# Testing 데이터셋 합치기\n",
    "list_np_te = np.concatenate([list_np_te_clean, list_np_te_attack])\n",
    "\n",
    "# 결과 출력 (옵션)\n",
    "print(\"Training Data Shape:\", list_np_tr.shape)\n",
    "print(\"Testing Data Shape:\", list_np_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6DyZSO2jU-m"
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, time_dim_first=False):\n",
    "    # 라벨 분리 (마지막 열이 라벨)\n",
    "    y = X[:, -1, -1]#.reshape(-1, 1)\n",
    "    X = X[:, :, :-1]\n",
    "\n",
    "    # 필요한 경우, 데이터의 차원을 재배열\n",
    "    if time_dim_first:\n",
    "        X = X.transpose(0, 2, 1)\n",
    "\n",
    "    # 데이터를 PyTorch 텐서로 변환\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    print(\"Shape of y after separating labels:\", y.shape)\n",
    "    print(\"Shape of X after separating labels:\", X.shape)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "\n",
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    # 훈련 및 검증 데이터 로더 생성\n",
    "    # bs는 배치 크기, jobs는 병렬 데이터 로딩을 위한 워커 수\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # 모델의 출력과 타깃 비교하여 정확도 계산\n",
    "    # 가장 높은 확률 값을 가진 클래스 선택\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt3qyNAyAFVf"
   },
   "outputs": [],
   "source": [
    "# y 레이블의 불순값 검사\n",
    "\n",
    "# 훈련 및 검증 데이터셋 생성\n",
    "trn_ds = create_dataset(list_np_tr, time_dim_first=False)\n",
    "val_ds = create_dataset(list_np_te, time_dim_first=False)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dl, valid_dl = create_loaders(trn_ds, val_ds, bs=147, jobs=cpu_count())\n",
    "\n",
    "# DataLoader에서 첫 번째 배치를 가져옵니다.\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "\n",
    "# y_batch의 유일한 값들 출력\n",
    "unique_values = torch.unique(y_batch)\n",
    "print(\"Unique values in y_batch:\", unique_values)\n",
    "\n",
    "# 0과 1 이외의 값이 있는지 확인\n",
    "invalid_values = y_batch[(y_batch != 0) & (y_batch != 1)]\n",
    "if len(invalid_values) > 0:\n",
    "    print(\"Invalid values found in y_batch:\", invalid_values)\n",
    "else:\n",
    "    print(\"No invalid values found in y_batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv-V-XcLrnWF"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        # 생성자: 스케줄러를 초기화\n",
    "        assert callable(schedule)  # schedule이 호출 가능한 함수인지 확인\n",
    "        self.schedule = schedule   # 학습률을 조정하는 함수 저장\n",
    "        super().__init__(optimizer, last_epoch)  # 부모 클래스의 생성자 호출\n",
    "\n",
    "    def get_lr(self):\n",
    "        # 현재 에포크에 대한 학습률을 계산\n",
    "        # base_lrs (기본 학습률)에 대해 schedule 함수를 호출하여 새로운 학습률 계산\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYWN6eHfrpKD"
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    # 코사인 학습률 스케줄링 함수를 생성하는 함수\n",
    "    # t_max: 학습률 순환 주기\n",
    "    # eta_min: 최소 학습률\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        # 스케줄러 함수: 각 에포크에 대한 학습률 계산\n",
    "        # epoch: 현재 에포크 번호\n",
    "        # base_lr: 기본 학습률\n",
    "\n",
    "        # 현재 순환 주기 내의 상대적 위치 계산\n",
    "        t = epoch % t_max\n",
    "\n",
    "        # 코사인 기반의 학습률 계산\n",
    "        # 학습률은 eta_min과 base_lr 사이에서 조정됨\n",
    "        return eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * t / t_max)) / 2\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZE9vpbCrrmT"
   },
   "outputs": [],
   "source": [
    "# 코사인 학습률 스케줄러를 사용하기 위한 설정\n",
    "n = 100  # 순환 주기\n",
    "sched = cosine(n)  # 코사인 학습률 스케줄러 생성\n",
    "\n",
    "# 주어진 범위(n * 4)에 대해 각 에포크별로 학습률 계산\n",
    "lrs = [sched(t, 1) for t in range(n * 4)]  # t: 에포크, 1: 기본 학습률\n",
    "\n",
    "plt.plot(lrs)  # 학습률 변화 그래프 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PTR7uPctblV"
   },
   "outputs": [],
   "source": [
    "print('Preparing datasets')\n",
    "# 훈련 및 검증 데이터셋 생성\n",
    "trn_ds = create_dataset(list_np_tr, time_dim_first=False)\n",
    "val_ds = create_dataset(list_np_te, time_dim_first=False)\n",
    "\n",
    "bs = 147  # 배치 크기 설정\n",
    "print(f'Creating data loaders with batch size: {bs}')\n",
    "# 데이터 로더 생성\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs=bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5icwVW1rua-"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        # 생성자: 모듈 초기화\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 히든 레이어 차원\n",
    "        self.layer_dim = layer_dim    # LSTM 레이어의 수\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)  # LSTM 네트워크\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 출력을 위한 선형 레이어\n",
    "        self.batch_size = None  # 배치 사이즈 (초기값 None)\n",
    "        self.hidden = None      # 히든 상태 (초기값 None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순방향 패스 정의\n",
    "        h0, c0 = self.init_hidden(x)       # 초기 히든 상태와 셀 상태\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))  # LSTM 네트워크 실행\n",
    "        out = self.fc(out[:, -1, :])       # 마지막 시간 단계의 출력을 선형 레이어에 통과\n",
    "        out = F.softmax(out, dim=1)  # Softmax 적용\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        # 초기 히든 상태와 셀 상태 생성\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)  # 히든 상태 초기화\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)  # 셀 상태 초기화\n",
    "        return [t.cuda() for t in (h0, c0)]  # GPU 사용 시, CUDA로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LCtEwvwvNZn"
   },
   "outputs": [],
   "source": [
    "# 모델 구성에 필요한 파라미터 설정\n",
    "input_dim = 15  # 입력 특성의 수\n",
    "hidden_dim = 256  # LSTM 숨겨진 계층의 크기\n",
    "layer_dim = 3  # LSTM 계층의 수\n",
    "output_dim = 2  # 출력 차원: 이진 분류의 경우 1\n",
    "\n",
    "# 학습률 및 에포크 설정\n",
    "lr = 0.0005\n",
    "n_epochs = 1000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "best_acc = 0\n",
    "patience, trials = 100, 0\n",
    "\n",
    "# LSTM 분류 모델 인스턴스 생성\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.BCELoss()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "# 훈련 과정\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    # 훈련 데이터셋을 이용한 학습\n",
    "    for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "        model.train()\n",
    "        x_batch = x_batch.cuda() if torch.cuda.is_available() else x_batch\n",
    "        y_batch = y_batch.cuda() if torch.cuda.is_available() else y_batch\n",
    "        opt.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        # loss = criterion(out, y_batch.float())\n",
    "        loss = criterion(out[:, 1], y_batch.float())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # 실제 레이블과 예측 레이블을 저장할 리스트\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # 검증 데이터셋을 이용한 평가\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct, total = 0, 0\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]  # 데이터를 CUDA로 이동\n",
    "        out = model(x_val)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)  # 예측값 계산\n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "\n",
    "    acc = correct / total  # 정확도 계산\n",
    "\n",
    "    # 에포크별 진행 상황 출력\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "    # 최고 정확도 갱신 및 모델 저장\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apRuN9L54yjC"
   },
   "outputs": [],
   "source": [
    "print('The training is finished! Restoring the best model weights')\n",
    "# 훈련이 완료되었음을 알리고, 가장 좋은 성능을 보였던 모델의 가중치를 복원\n",
    "\n",
    "# 가장 좋은 모델의 가중치를 불러옴\n",
    "model.load_state_dict(torch.load('best.pth'))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTv8iXEJ40EI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "test_predictions = []  # 예측 결과를 저장할 리스트\n",
    "print('Predicting on test dataset')\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    for x_val, _ in val_dl:  # 테스트 데이터 로더(val_dl) 사용\n",
    "        x_val = x_val.cuda() if torch.cuda.is_available() else x_val  # GPU 사용 가능 시, 데이터를 GPU로 이동\n",
    "        out = model(x_val)  # 모델에 배치 데이터 전달 및 예측 수행\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        preds = preds.long()  # Bool 타입을 Long 타입으로 변환 (예측값을 0 또는 1로 변환)\n",
    "        test_predictions += preds.cpu().tolist()  # 예측 결과를 CPU로 이동한 후 리스트에 추가\n",
    "\n",
    "\n",
    "# 실제 레이블과 예측 레이블 준비\n",
    "y_true = []\n",
    "y_pred = test_predictions  # 예측 레이블 (위의 예측 코드 결과)\n",
    "\n",
    "# 실제 레이블 추출\n",
    "for _, y_batch in val_dl:\n",
    "    y_true += y_batch.tolist()\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# F1 점수 계산\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"LSTM - Confusion Matrix\")\n",
    "plt.show()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1v8nT_yX5dDLCh7eOPM279rtfyjVKN4C4",
     "timestamp": 1699884181819
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
