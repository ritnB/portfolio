{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import"],"metadata":{"id":"Ks7Yzqp2EN-k"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"zKA3PIMEB9eh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbvEHtqpZH2R"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/purdueExtension')\n","\n","from config import *\n","from util import get_center_points, is_same_length, is_list_strings, move_to_zero, move_to_zero_all, get_parsed_timestamp, process_dataframe\n","# from Collected_dataset.dataloader import\n","# from Collected_dataset.model import"]},{"cell_type":"markdown","source":["# File Loading"],"metadata":{"id":"QlsTaVPUEc7t"}},{"cell_type":"code","source":["dict_path_tr_clean={\n","    '20m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/20m_static.csv'),\n","    '20m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/20m_dyn1.csv'),\n","    '20m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/20m_dyn2.csv'),\n","    '50m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/50m_static.csv'),\n","    '50m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/50m_dyn1.csv'),\n","    '50m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/50m_dyn2.csv'),\n","    '70m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/70m_static.csv'),\n","    '70m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/70m_dyn1.csv'),\n","    '70m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/clean/70m_dyn2.csv'),\n","}\n","dict_path_tr_attack={\n","    '20m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/20m_static.csv'),\n","    '20m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/20m_dyn1.csv'),\n","    '20m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/20m_dyn2.csv'),\n","    '50m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/50m_static.csv'),\n","    '50m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/50m_dyn1.csv'),\n","    '50m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/50m_dyn2.csv'),\n","    '70m_stat':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/70m_static.csv'),\n","    '70m_dyn1':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/70m_dyn1.csv'),\n","    '70m_dyn2':ospj(path_workspace, r'Collected_dataset/Training & Validation/attack/70m_dyn2.csv'),\n","}\n","dict_path_te_clean={\n","    '35m_stat':ospj(path_workspace, r'Collected_dataset/Testing/clean/35m_stat.csv'),\n","    '35m_dyn1':ospj(path_workspace, r'Collected_dataset/Testing/clean/35m_dyn1.csv'),\n","    '35m_dyn2':ospj(path_workspace, r'Collected_dataset/Testing/clean/35m_dyn2.csv'),\n","}\n","dict_path_te_attack={\n","    '35m_stat':ospj(path_workspace, r'Collected_dataset/Testing/attack/35m_stat.csv'),\n","    '35m_dyn1':ospj(path_workspace, r'Collected_dataset/Testing/attack/35m_dyn1.csv'),\n","    '35m_dyn2':ospj(path_workspace, r'Collected_dataset/Testing/attack/35m_dyn2.csv'),\n","}"],"metadata":{"id":"IM8DwYPsZ6E5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#placeholder dictionary for collected dataframes\n","dict_df_tr_clean={}\n","dict_df_tr_attack={}\n","dict_df_te_clean={}\n","dict_df_te_attack={}\n","\n","#control variables\n","dict_path=dict_path_tr_clean #path variable\n","dict_df=dict_df_tr_clean #dict variable to collect dataframes read from paths\n","for key, path in dict_path.items():\n","\tdict_df[key]=pd.read_csv(path)\n","dict_df_tr_clean=dict_df #안전장치\n","print({key: len(value) for key, value in dict_df_tr_clean.items()})\n","\n","dict_path=dict_path_tr_attack\n","dict_df=dict_df_tr_attack\n","for key, path in dict_path.items():\n","\tdict_df[key]=pd.read_csv(path)\n","dict_df_tr_attack=dict_df #안전장치\n","print({key: len(value) for key, value in dict_df_tr_attack.items()})\n","\n","dict_path=dict_path_te_clean\n","dict_df=dict_df_te_clean\n","for key, path in dict_path.items():\n","\tdict_df[key]=pd.read_csv(path)\n","dict_df_te_clean=dict_df #안전장치\n","print({key: len(value) for key, value in dict_df_te_clean.items()})\n","\n","dict_path=dict_path_te_attack\n","dict_df=dict_df_te_attack\n","for key, path in dict_path.items():\n","\tdict_df[key]=pd.read_csv(path)\n","dict_df_te_attack=dict_df #안전장치\n","print({key: len(value) for key, value in dict_df_te_attack.items()})"],"metadata":{"id":"NfhzbADsExoh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"tbhJ_EMKNEhj"}},{"cell_type":"code","source":["# Make a new dictionary to store preprocessed df\n","preprocessed_dict_tr_clean={}\n","preprocessed_dict_tr_attack={}\n","preprocessed_dict_te_clean={}\n","preprocessed_dict_te_attack={}\n","\n","# Call def process_dataframe and store df\n","point_numbers=0\n","for key, df in dict_df_tr_clean.items():\n","    if key not in preprocessed_dict_tr_clean:  # Check if already processed\n","        preprocessed_df=process_dataframe(df)\n","        preprocessed_dict_tr_clean[key]=preprocessed_df\n","        nan_counts=preprocessed_df.isna().values.sum()\n","        print('NaN: ', nan_counts, end='\\t')\n","        point_numbers+=len(df)\n","print()\n","print({key: len(value) for key, value in preprocessed_dict_tr_clean.items()})\n","\n","for key, df in dict_df_tr_attack.items():\n","    if key not in preprocessed_dict_tr_attack:  # Check if already processed\n","        preprocessed_df=process_dataframe(df)\n","        preprocessed_dict_tr_attack[key]=preprocessed_df\n","        nan_counts=preprocessed_df.isna().values.sum()\n","        print('NaN: ', nan_counts, end='\\t')\n","print()\n","print({key: len(value) for key, value in preprocessed_dict_tr_attack.items()})\n","\n","for key, df in dict_df_te_clean.items():\n","    if key not in preprocessed_dict_te_clean:  # Check if already processed\n","        preprocessed_df=process_dataframe(df)\n","        preprocessed_dict_te_clean[key]=preprocessed_df\n","        nan_counts=preprocessed_df.isna().values.sum()\n","        print('NaN: ', nan_counts, end='\\t')\n","print()\n","print({key: len(value) for key, value in preprocessed_dict_te_clean.items()})\n","\n","for key, df in dict_df_te_attack.items():\n","    if key not in preprocessed_dict_te_attack:  # Check if already processed\n","        preprocessed_df=process_dataframe(df)\n","        preprocessed_dict_te_attack[key]=preprocessed_df\n","        nan_counts=preprocessed_df.isna().values.sum()\n","        print('NaN: ', nan_counts, end='\\t')\n","print()\n","print({key: len(value) for key, value in preprocessed_dict_te_attack.items()})"],"metadata":{"id":"Gh_jbaEYclAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dict_df=dict_df_tr_clean\n","# for key, df in dict_df.items():\n","# \t# string timestamp to float timestamp\n","# \tdf_parsed=get_parsed_timestamp(df, inplace=False)\n","# \t# drop useless columns\n","# \tdf_drop=df_parsed.drop(['time_utc_usec', 'fix_type', 'jamming_state', 'vel_ned_valid', 'timestamp_time_relative', 'heading', 'heading_offset', 'selected'], axis=1, inplace=False)\n","# \t# move to center (regularization)\n","# \tdf_regu=move_to_zero_all(df_drop, _target=['lat','lon','alt','alt_ellipsoid'])\n","# \t# drop timestamp, utc\n","# \tdf_pre_tr_clean=df_regu.drop(['timestamp'],axis=1,inplace=False)\n","\n","# dict_df=dict_df_tr_attack\n","# for key, df in dict_df.items():\n","# \t# string timestamp to float timestamp\n","# \tdf_parsed=get_parsed_timestamp(df, inplace=False)\n","# \t# drop useless columns\n","# \tdf_drop=df_parsed.drop(['time_utc_usec', 'fix_type', 'jamming_state', 'vel_ned_valid', 'timestamp_time_relative', 'heading', 'heading_offset', 'selected'], axis=1, inplace=False)\n","# \t# move to center (regularization)\n","# \tdf_regu=move_to_zero_all(df_drop, _target=['lat','lon','alt','alt_ellipsoid'])\n","# \t# drop timestamp, utc\n","# \tdf_pre_tr_attack=df_regu.drop(['timestamp'],axis=1,inplace=False)\n","\n","# dict_df=dict_df_te_clean\n","# for key, df in dict_df.items():\n","# \t# string timestamp to float timestamp\n","# \tdf_parsed=get_parsed_timestamp(df, inplace=False)\n","# \t# drop useless columns\n","# \tdf_drop=df_parsed.drop(['time_utc_usec', 'fix_type', 'jamming_state', 'vel_ned_valid', 'timestamp_time_relative', 'heading', 'heading_offset', 'selected'], axis=1, inplace=False)\n","# \t# move to center (regularization)\n","# \tdf_regu=move_to_zero_all(df_drop, _target=['lat','lon','alt','alt_ellipsoid'])\n","# \t# drop timestamp, utc\n","# \tdf_pre_te_clean=df_regu.drop(['timestamp'],axis=1,inplace=False)\n","\n","# dict_df=dict_df_te_attack\n","# for key, df in dict_df.items():\n","# \t# string timestamp to float timestamp\n","# \tdf_parsed=get_parsed_timestamp(df, inplace=False)\n","# \t# drop useless columns\n","# \tdf_drop=df_parsed.drop(['time_utc_usec', 'fix_type', 'jamming_state', 'vel_ned_valid', 'timestamp_time_relative', 'heading', 'heading_offset', 'selected'], axis=1, inplace=False)\n","# \t# move to center (regularization)\n","# \tdf_regu=move_to_zero_all(df_drop, _target=['lat','lon','alt','alt_ellipsoid'])\n","# \t# drop timestamp, utc\n","# \tdf_pre_te_attack=df_regu.drop(['timestamp'],axis=1,inplace=False)\n","\n","# print(len(df_pre_tr_clean))\n","# print(len(df_pre_tr_attack))\n","# print(len(df_pre_te_clean))\n","# print(len(df_pre_te_attack))"],"metadata":{"id":"sDYsNrThNC-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Labeling"],"metadata":{"id":"-DoFtXN5YVa8"}},{"cell_type":"code","source":["# labeling 0 for clean, 1 for attack\n","for key, df in preprocessed_dict_tr_clean.items():\n","    if 'legitimate' not in df.columns:      # safety\n","        df['legitimate']=0\n","for key, df in preprocessed_dict_tr_attack.items():\n","    if 'legitimate' not in df.columns:      # safety\n","        df['legitimate']=1\n","for key, df in preprocessed_dict_te_clean.items():\n","    if 'legitimate' not in df.columns:      # safety\n","        df['legitimate']=0\n","for key, df in preprocessed_dict_te_attack.items():\n","    if 'legitimate' not in df.columns:      # safety\n","        df['legitimate']=1"],"metadata":{"id":"ZfmLCUgYB9QX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split Dataset"],"metadata":{"id":"2uckQf5_qNkY"}},{"cell_type":"code","source":["print(preprocessed_dict_tr_clean.keys())\n","print(preprocessed_dict_tr_attack.keys())\n","print(preprocessed_dict_te_clean.keys())\n","print(preprocessed_dict_te_attack.keys())"],"metadata":{"id":"R4SPRZTLmxWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from util import split_dataframe\n","\n","# split function... 왜인지 자꾸 import 에러 나서 여기 둠\n","def split_dataframe(df):\n","    diff = np.absolute(df['timestamp'].diff().values)\n","    diff[0] = 0.0\n","    diff = pd.Series(diff.round(decimals=5))\n","    split_indices = np.where(diff >= 1.0)[0]\n","    split_indices = [0] + list(split_indices) + [len(diff)]\n","    split_dfs = []\n","\n","    for i, split_index in enumerate(split_indices):\n","        if i == len(split_indices) - 1:\n","            break\n","        start_index = split_index\n","        end_index = split_indices[i + 1]\n","        split_dfs.append(df.iloc[start_index:end_index])\n","\n","    return split_dfs\n","\n","# preprocessed_dict와 동일한 key를 가진 딕셔너리를 만들고, item으로는 분할된 데이터프레임이 append된 리스트를 넣음\n","dict_split_tr_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_clean.items()}\n","dict_split_tr_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_attack.items()}\n","dict_split_te_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_te_clean.items()}\n","dict_split_te_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_te_attack.items()}"],"metadata":{"id":"-b3iS0bQqQkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check length of the splitted dataset\n","for key, split_list in dict_split_tr_clean.items():\n","    print(f'Key: {key}, Number of Items: {len(split_list)}')\n","\n","for key, split_list in dict_split_tr_attack.items():\n","    print(f'Key: {key}, Number of Items: {len(split_list)}')\n","\n","for key, split_list in dict_split_te_clean.items():\n","    print(f'Key: {key}, Number of Items: {len(split_list)}')\n","\n","for key, split_list in dict_split_te_attack.items():\n","    print(f'Key: {key}, Number of Items: {len(split_list)}')"],"metadata":{"id":"-yu0gBf-7PdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_nan(df_list):\n","    nan_counts = sum(df.isna().values.sum() for df in df_list)\n","    return f'({nan_counts})'\n","\n","len_dict_tr_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_clean.items()}\n","len_dict_tr_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_attack.items()}\n","len_dict_te_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_clean.items()}\n","len_dict_te_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_attack.items()}\n","\n","print(len_dict_tr_clean)\n","print(len_dict_tr_attack)\n","print(len_dict_te_clean)\n","print(len_dict_te_attack)"],"metadata":{"id":"lSNVnjtHYIFM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Merging"],"metadata":{"id":"zTkoU4wDYR1i"}},{"cell_type":"code","source":["# lists for concatenating items in dictionaries\n","list_tr_clean=[]\n","list_tr_attack=[]\n","list_te_clean=[]\n","list_te_attack=[]\n","\n","for key, df in preprocessed_dict_tr_clean.items():\n","    # if len(df_tr_clean)!=0:break      # safety\n","    list_tr_clean.append(df)\n","# list_tr_clean=pd.DataFrame(ITERABLE, columns=COLUMNS)    # list to df\n","for key, df in preprocessed_dict_tr_attack.items():\n","    # if len(df_tr_attack)!=0:break     # safety\n","    list_tr_attack.append(df)\n","# f_tr_attack = pd.concat(df_tr_attack)    # list to df\n","for key, df in preprocessed_dict_te_clean.items():\n","    # if len(df_te_clean)!=0:break      # safety\n","    list_te_clean.append(df)\n","# df_te_clean = pd.concat(df_te_clean)    # list to df\n","for key, df in preprocessed_dict_te_attack.items():\n","    #if len(df_te_attack)!=0:break      # safety\n","    list_te_attack.append(df)\n","# df_te_attack = pd.concat(df_te_attack)    # list to df\n","\n","# concatenating all dataframes of each training, test sets\n","# drop timestamp and locations column\n","# if 'timestamp' in list_tr_clean.columns:     # safety\n","df_concat_tr=pd.concat(list_tr_clean+list_tr_attack, axis=0).reset_index(drop=True).drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)\n","# if 'timestamp' in list_te_clean.columns:     # safety\n","df_concat_te=pd.concat(list_te_clean+list_te_attack, axis=0).reset_index(drop=True).drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)"],"metadata":{"id":"Mp7S6kadYb8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_concat_te"],"metadata":{"id":"9slXTetyJhrF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# File Exporting"],"metadata":{"id":"ryZLMxSjY9O8"}},{"cell_type":"code","source":["tr_fin=df_concat_tr\n","te_fin=df_concat_te"],"metadata":{"id":"fkB2wvs4ZMQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# exporting raw\n","from config import path_merged, path_split\n","if not osex(path_merged):\n","    mkdir(path_merged)"],"metadata":{"id":"jhjGymKtY_75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_fin.to_csv(ospj(path_merged, 'tr.csv'))\n","tr_x=tr_fin.iloc[:, :-1]\n","tr_x.to_csv(ospj(path_merged, 'tr_x.csv'))\n","tr_y=tr_fin.iloc[:, -1]\n","tr_y.to_csv(ospj(path_merged, 'tr_y.csv'))\n","te_fin.to_csv(ospj(path_merged, 'te.csv'))\n","te_x=te_fin.iloc[:, :-1]\n","te_x.to_csv(ospj(path_merged, 'te_x.csv'))\n","te_y=te_fin.iloc[:, -1]\n","te_y.to_csv(ospj(path_merged, 'te_y.csv'))\n","\n","# if osex(path_split):"],"metadata":{"id":"EDc3NQ90Z7LE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation with Scikit-Learn"],"metadata":{"id":"KuxrNsmiafAY"}},{"cell_type":"code","source":["from re import VERBOSE\n","from sklearn.metrics import f1_score, confusion_matrix\n","import seaborn as sns\n","\n","seed=0\n","models=dict(\n","    svc=SVC(random_state=seed),\n","    knn=KNeighborsClassifier(),\n","    rfc=RandomForestClassifier(random_state=seed),\n","    xgb=XGBClassifier(random_state=seed, use_label_encoder=False, verbosity=0)\n",")"],"metadata":{"id":"BXvDS5tNaiFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_val_pred={}\n","dict_test_pred={}\n","dict_f1_score = {}\n","dict_confusion_matrix = {}\n","dict_valid_confusion_matrix = {}\n","\n","for m_name, model in models.items():\n","    model.fit(tr_x, tr_y)\n","    dict_val_pred[m_name]=model.predict(tr_x)    # validation\n","    dict_test_pred[m_name]=model.predict(te_x)    # testing\n","\n","    f1 = f1_score(te_y, dict_test_pred[m_name])\n","    dict_f1_score[m_name] = f1\n","\n","    vf1 = f1_score(tr_y, dict_val_pred[m_name])\n","    # Calculate Confusion Matrix\n","    vcm = confusion_matrix(tr_y, dict_val_pred[m_name])\n","    dict_valid_confusion_matrix[m_name] = vcm\n","    cm = confusion_matrix(te_y, dict_test_pred[m_name])\n","    dict_confusion_matrix[m_name] = cm\n","\n","    print(f\"{m_name} - F1 Score: {vf1}\")\n","    plt.figure(figsize=(4, 3))\n","    sns.heatmap(vcm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.title(f\"{m_name} - Validation Confusion Matrix\")\n","    plt.show()\n","    print()\n","\n","    print(f\"{m_name} - F1 Score: {f1}\")\n","    # print(f\"{m_name} - Confusion Matrix:\")\n","    # print(cm)\n","    plt.figure(figsize=(4, 3))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.title(f\"{m_name} - Confusion Matrix\")\n","    plt.show()\n","    print()"],"metadata":{"id":"ZI86A5Wdb8Qa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(tr_y)"],"metadata":{"id":"maG-Syj-hQFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(dict_val_pred['svc'])"],"metadata":{"id":"nCJJ33QYfPsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(dict_val_pred['xgb'])"],"metadata":{"id":"_6z5YdRthVwr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataLoader Builder"],"metadata":{"id":"5bbeEw7AirgS"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class DataBuilder(Dataset): #version 4 #I think DataLoader should be merged here with kwargs and returned as DataLoader - DataBuilder: No need\n","    def __init__(self, dataset:np.ndarray,\n","             device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n","        self.x=torch.from_numpy(dataset[:, :-1]).float()\n","        self.y=torch.from_numpy(dataset[:, -1]).float().reshape(-1, 1)\n","        #self.y=torch.from_numpy(dataset[:, -1]).float().unsqueeze(1)\n","        #self.len=self.x.shape[0]\n","        self.len=dataset.shape[0]\n","        self.x.to(device)\n","        self.y.to(device)\n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index]\n","    def __len__(self):\n","        return self.len"],"metadata":{"id":"pi6DTC9wjA5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_fin"],"metadata":{"id":"gaBLrzACsR_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=128\n","train_dataset=DataBuilder(tr_fin.values)\n","train_loader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, drop_last=True,) # num_workers=4, worker_init_fn=seed_worker, generator=g)\n","test_dataset=DataBuilder(te_fin.values)\n","test_loader=DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True,) # num_workers=4, worker_init_fn=seed_worker, generator=g)"],"metadata":{"id":"xyb8_U3pitcr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation with MLP"],"metadata":{"id":"F3353-fYi1SP"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n","from torch import optim\n","torch.manual_seed(1234) # reproducibility"],"metadata":{"id":"1sJfaKAwkWD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self,\n","                 D_in,\n","                 D_l1,\n","                 D_l2,\n","                 D_l3,\n","                 D_l4,\n","                 D_l5,\n","                 lr=1e-4,\n","                 dropout=0.0,\n","                 random_seed=1234):\n","\n","        super(MLP, self).__init__()\n","        self.random_seed = random_seed  # random_seed를 클래스 변수로 설정\n","\n","        self.l1=nn.Linear(D_in, D_l1)\n","        self.b1=nn.BatchNorm1d(D_l1)\n","        self.a1=nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.l2=nn.Linear(D_l1, D_l2)\n","        self.b2=nn.BatchNorm1d(D_l2)\n","        self.a2=nn.ReLU()\n","        self.l3=nn.Linear(D_l2, D_l3)\n","        self.b3=nn.BatchNorm1d(D_l3)\n","        self.a3=nn.ReLU()\n","        self.l4=nn.Linear(D_l3, D_l4)\n","        self.b4=nn.BatchNorm1d(D_l4)\n","        self.a4=nn.ReLU()\n","        self.l5=nn.Linear(D_l4, D_l5)\n","        self.b5=nn.BatchNorm1d(D_l5)\n","        self.a5=nn.ReLU()\n","        self.output_layer = nn.Linear(D_l5, 1)\n","\n","        #self.softmax=nn.Softmax(dim=1)\n","        #self.criterion=nn.CrossEntropyLoss()\n","        self.criterion = nn.BCELoss()\n","        self.optimizer=optim.Adam(self.parameters(), lr=lr)\n","\n","        #self.to('cpu')\n","        #self.to('cuda')\n","\n","    def forward(self, x):   #to call: mlp(data)\n","        x=self.a1(self.b1(self.l1(x)))\n","        x = self.dropout(x)\n","        x=self.a2(self.b2(self.l2(x)))\n","        x=self.a3(self.b3(self.l3(x)))\n","        x=self.a4(self.b4(self.l4(x)))\n","        x=self.a5(self.b5(self.l5(x)))\n","        #return self.softmax(x)\n","        return torch.sigmoid(self.output_layer(x))\n","\n","    def train(self, dataLoader, epochs, print_every=10, valid_every=100):\n","        for epoch in range(epochs):\n","            total_loss = 0.0\n","            for i, (x, label) in enumerate(dataLoader):\n","                #label = label.view(-1, 1).float()\n","                self.optimizer.zero_grad()\n","                # print(x.shape)\n","                output=self(x)\n","                # print(output.shape, label.shape)\n","                # print(output[:3])\n","                # print(label[:3])\n","                loss=self.criterion(output, label)\n","                # print(loss)\n","                # self.train_losses=loss if self.train_losses==None else torch.vstack((self.train_losses, loss))\n","                loss.backward()\n","                self.optimizer.step()\n","                total_loss += loss.item()\n","            # print average loss for the epoch\n","            average_loss = total_loss / len(dataLoader)\n","            if (epoch + 1) % print_every == 0:\n","                print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n","            if (epoch + 1) % valid_every == 0:\n","                self.test(dataLoader)\n","\n","    # def test(self, dataLoader):\n","    #     with torch.no_grad():\n","    #         for i, (x, label) in enumerate(dataLoader):\n","    #             output=self(x)\n","    #             loss=self.criterion(output, label)\n","                # self.test_losses=loss if self.test_losses==None else torch.vstack((self.test_losses, loss))\n","\n","    def test(self, test_loader):\n","        # self.eval()  # Set the model to evaluation mode\n","        correct = 0\n","        total = 0\n","        validation_loss = 0\n","        all_predictions = []\n","        all_labels = []\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                outputs = self(inputs)  # Forward pass\n","                #_, predicted = torch.max(outputs.data, 1)  # Get the predicted class labels\n","                predicted = (outputs >= 0.5).int()\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","                all_predictions.extend(predicted.tolist())\n","                all_labels.extend(labels.tolist())\n","                # Calculate validation loss\n","                loss = self.criterion(outputs, labels)\n","                validation_loss += loss.item()\n","        # Calculate Precision\n","        precision = precision_score(all_labels, all_predictions)\n","        # Calculate Recall\n","        recall = recall_score(all_labels, all_predictions)\n","        # Calculate F1 score\n","        f1 = f1_score(all_labels, all_predictions)\n","        # Calculate confusion matrix\n","        cm = confusion_matrix(all_labels, all_predictions)\n","        print('Accuracy: {:.2f}%'.format(100 * correct / total))       # Calculate Accuracy\n","        print('Validation Loss: {:.2f}'.format(validation_loss / len(test_loader)))  # Calculate Validation Loss\n","        print('F1 Score:', f1)\n","        #print('Confusion Matrix:')\n","        # print(cm)\n","        plt.figure(figsize=(4, 3))\n","        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","        plt.xlabel(\"Predicted\")\n","        plt.ylabel(\"True\")\n","        plt.title(f\"MLP - Confusion Matrix\")\n","        plt.show()\n","        print()"],"metadata":{"id":"uwqYLzU1ivuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MLP learning\n","D_in=14\n","mlp=MLP(D_in,\n","        D_l1=12,\n","        dropout=0.0,\n","        D_l2=8,\n","        D_l3=6,\n","        D_l4=4,\n","        D_l5=2,\n","        lr=1e-3,\n","        random_seed=1234)\n","\n","mlp.train(train_loader, epochs=3000, print_every=100, valid_every=1000)"],"metadata":{"id":"UPjX8nagkJvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","mlp.test(test_loader)"],"metadata":{"id":"FFAMlYcofPic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation with LSTM"],"metadata":{"id":"WO7pGW12JrtP"}},{"cell_type":"code","source":["df_merged_tr=[]\n","df_merged_te=[]\n","\n","for key, split_list in dict_split_tr_clean.items():\n","    for df in split_list:\n","        df_merged_tr.append(df)\n","\n","for key, split_list in dict_split_tr_attack.items():\n","    for df in split_list:\n","        df_merged_tr.append(df)\n","\n","for key, split_list in dict_split_te_clean.items():\n","    for df in split_list:\n","        df_merged_te.append(df)\n","\n","for key, split_list in dict_split_te_attack.items():\n","    for df in split_list:\n","        df_merged_te.append(df)\n","\n","df_merged_tr = pd.concat(df_merged_tr).reset_index(drop=True).drop(['timestamp', 'lat', 'lon', 'alt', 'alt_ellipsoid'], axis=1)\n","df_merged_te = pd.concat(df_merged_te).reset_index(drop=True).drop(['timestamp', 'lat', 'lon', 'alt', 'alt_ellipsoid'], axis=1)\n","\n","print(len(df_merged_tr))\n","print(len(df_merged_te))\n","df_merged_tr"],"metadata":{"id":"NVMsQbrYCK3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Convert into 2-Sequence Data"],"metadata":{"id":"1B1Skd1RJ6f7"}},{"cell_type":"code","source":["# dict_split_tr_clean['20m_stat'][1]"],"metadata":{"id":"-0HuTFohKelr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for key, split_list in dict_split_tr_clean.items():\n","#     print(f'Key: {key}')\n","#     for df in split_list:\n","#         print(df)\n","\n","# for key, split_list in dict_split_tr_attack.items():\n","#     print(f'Key: {key}')\n","#     for df in split_list:\n","#         print(df)\n","\n","# for key, split_list in dict_split_te_clean.items():\n","#     print(f'Key: {key}')\n","#     for df in split_list:\n","#         print(df)\n","\n","# for key, split_list in dict_split_te_attack.items():\n","#     print(f'Key: {key}')\n","#     for df in split_list:\n","#         print(df)"],"metadata":{"id":"F06fLMisEYu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Initialize new dictionary\n","# dict_2seq_tr_clean={}\n","# dict_2seq_tr_attack={}\n","# dict_2seq_te_clean={}\n","# dict_2seq_te_attack={}\n","\n","# # Convert to 2-sequence dataframe and add to new list\n","# for key, split_list in dict_split_tr_clean.items():\n","#     new_split_list=[]  # initialize new list\n","#     for df in split_list:\n","#         df=df.drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)  # Drop location features and timestamp\n","#         # Split dataframe into 2-sequence data\n","#         seq_len=2  # Sequence length\n","#         seq_data=[]\n","#         for i in range(len(df) - seq_len + 1):\n","#             seq=df[i:i + seq_len]\n","#             seq_data.append(seq)\n","#         new_df=pd.concat(seq_data, ignore_index=True)\n","#         new_split_list.append(new_df)\n","#     dict_2seq_tr_clean[key]=new_split_list\n","\n","# for key, split_list in dict_split_tr_attack.items():\n","#     new_split_list=[]  # initialize new list\n","#     for df in split_list:\n","#         df=df.drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)  # Drop location features and timestamp\n","#         # Split dataframe into 2-sequence data\n","#         seq_len=2  # Sequence length\n","#         seq_data=[]\n","#         for i in range(len(df) - seq_len + 1):\n","#             seq=df[i:i + seq_len]\n","#             seq_data.append(seq)\n","#         new_df=pd.concat(seq_data, ignore_index=True)\n","#         new_split_list.append(new_df)\n","#     dict_2seq_tr_attack[key]=new_split_list\n","\n","# for key, split_list in dict_split_te_clean.items():\n","#     new_split_list=[]  # initialize new list\n","#     for df in split_list:\n","#         df=df.drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)  # Drop location features and timestamp\n","#         # Split dataframe into 2-sequence data\n","#         seq_len=2  # Sequence length\n","#         seq_data=[]\n","#         for i in range(len(df) - seq_len + 1):\n","#             seq=df[i:i + seq_len]\n","#             seq_data.append(seq)\n","#         new_df=pd.concat(seq_data, ignore_index=True)\n","#         new_split_list.append(new_df)\n","#     dict_2seq_te_clean[key]=new_split_list\n","\n","# for key, split_list in dict_split_te_attack.items():\n","#     new_split_list=[]  # initialize new list\n","#     for df in split_list:\n","#         df=df.drop(['timestamp','lat','lon','alt','alt_ellipsoid'], axis=1, inplace=False)  # Drop location features and timestamp\n","#         # Split dataframe into 2-sequence data\n","#         seq_len=2  # Sequence length\n","#         seq_data=[]\n","#         for i in range(len(df) - seq_len + 1):\n","#             seq=df[i:i + seq_len]\n","#             seq_data.append(seq)\n","#         new_df=pd.concat(seq_data, ignore_index=True)\n","#         new_split_list.append(new_df)\n","#     dict_2seq_te_attack[key]=new_split_list\n","\n","# # Print new dictionary and values\n","# # for key, split_list in dict_2seq_tr_clean.items():\n","# #     print(f'Key: {key}')\n","# #     for new_df in split_list:\n","# #         print(new_df)"],"metadata":{"id":"IoGpjFRYO42v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dict_2seq_tr_clean['20m_stat'][1]"],"metadata":{"id":"Yzne9KCkQChM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Concatenation\n","# list_concat_tr = []\n","# list_concat_te = []\n","\n","# for key, split_list in dict_2seq_tr_clean.items():\n","#     for df in split_list:\n","#         list_concat_tr.append(df)\n","\n","# for key, split_list in dict_2seq_tr_attack.items():\n","#     for df in split_list:\n","#         list_concat_tr.append(df)\n","\n","# for key, split_list in dict_2seq_te_clean.items():\n","#     for df in split_list:\n","#         list_concat_te.append(df)\n","\n","# for key, split_list in dict_2seq_te_attack.items():\n","#     for df in split_list:\n","#         list_concat_te.append(df)\n","\n","# df_concat_2seq_tr= pd.concat(list_concat_tr, ignore_index=True)\n","# df_concat_2seq_te= pd.concat(list_concat_te, ignore_index=True)\n","\n","# df_concat_2seq_te"],"metadata":{"id":"oeHy7dsCgCUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # The Number of Points Checking\n","# # Initialize variables to store the total sum for clean and attack dataframes\n","# total_sum_tr_clean = 0\n","# total_sum_tr_attack = 0\n","# total_sum_te_clean = 0\n","# total_sum_te_attack = 0\n","\n","# # Sum the lengths of dataframes in dict_2seq_tr_clean\n","# for key, split_list in dict_split_tr_clean.items():\n","#     for df in split_list:\n","#         total_sum_tr_clean += (len(df) - 1) * 2\n","\n","# # Sum the lengths of dataframes in dict_2seq_tr_attack\n","# for key, split_list in dict_split_tr_attack.items():\n","#     for df in split_list:\n","#         total_sum_tr_attack += (len(df) - 1) * 2\n","\n","# # Sum the lengths of dataframes in dict_2seq_te_clean\n","# for key, split_list in dict_split_te_clean.items():\n","#     for df in split_list:\n","#         total_sum_te_clean += (len(df) - 1) * 2\n","\n","# # Sum the lengths of dataframes in dict_2seq_te_attack\n","# for key, split_list in dict_split_te_attack.items():\n","#     for df in split_list:\n","#         total_sum_te_attack += (len(df) - 1) * 2\n","\n","# # Print the total sums for clean and attack dataframes\n","# print(\"Expected Length of TR Clean Dataframes:\", total_sum_tr_clean)\n","# print(\"Expected Length of TR Attack Dataframes:\", total_sum_tr_attack)\n","# print(\"Expected Length of TR Dataframes:\", total_sum_tr_clean + total_sum_tr_attack)\n","# print(\"Actual Length of TR Dataframe:\", len(df_concat_2seq_tr))\n","# print()\n","# print(\"Expected Length of Clean Dataframes:\", total_sum_te_clean)\n","# print(\"Expected Length of Attack Dataframes:\", total_sum_te_attack)\n","# print(\"Expected Length of Dataframes:\", total_sum_te_clean + total_sum_te_attack)\n","# print(\"Actual Length of TE Dataframe:\", len(df_concat_2seq_te))"],"metadata":{"id":"zwvDagKLiQup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM Modeling"],"metadata":{"id":"pXWEAG4LAUsK"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import seaborn as sns"],"metadata":{"id":"h732uk5iohBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_data = df_concat_2seq_tr\n","# test_data = df_concat_2seq_te\n","train_data=df_merged_tr\n","test_data=df_merged_te\n","\n","X_train = train_data.drop(columns=['legitimate']).values\n","y_train = train_data['legitimate'].values\n","X_test = test_data.drop(columns=['legitimate']).values\n","y_test = test_data['legitimate'].values\n","\n","# Convert dataframe into Tensor\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32)"],"metadata":{"id":"DUNrADlQOCiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n","test_dataset = TensorDataset(X_test, y_test)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"bye8R7qMO8Yj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self,\n","                 input_size,\n","                 hidden_size,\n","                 num_layers,\n","                 output_size,\n","                 random_seed=1234):     #reproducibility\n","        super(LSTM, self).__init__()\n","        self.random_seed = random_seed  # random_seed를 클래스 변수로 설정\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])  # 마지막 시퀀스에서만 결과 추출\n","        out = self.sigmoid(out)\n","        return out\n","\n","input_size = 14\n","hidden_size = 64\n","num_layers = 3\n","output_size = 1\n","\n","model = LSTM(input_size, hidden_size, num_layers, output_size)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Train\n","def train(train_loader, epochs, print_every=1, valid_every=1):\n","    for epoch in range(epochs):\n","        total_loss = 0.0\n","        total_samples = 0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs.reshape(-1, 1, 14))  # LSTM 모델에 입력\n","            loss = criterion(outputs, labels.reshape(-1, 1))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item() * len(inputs)\n","            total_samples += len(inputs)\n","        avg_loss = total_loss / total_samples\n","        if (epoch + 1) % print_every == 0:\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.10f}')\n","        if (epoch + 1) % valid_every == 0:\n","            test(train_loader)\n","\n","# Test\n","# def test(test_loader):\n","#     outputs = model(test_loader.dataset.tensors[0].reshape(-1,1,14))\n","#     predicted = (outputs > 0.5).float()\n","#     # Accuracy\n","#     accuracy = (predicted == test_loader.dataset.tensors[1].reshape(-1,1)).float().mean()\n","#     print(f'Accuracy: {accuracy.item():.8f}')\n","#     # Loss\n","#     loss = criterion(outputs, test_loader.dataset.tensors[1].reshape(-1,1))\n","#     print(f'Validation Loss: {loss.item():.10f}')\n","#     # F1 score\n","#     f1 = f1_score(test_loader.dataset.tensors[1], predicted)\n","#     print(f'F1 score: {f1:.4f}')\n","#     # Confusion matrix\n","#     cm = confusion_matrix(test_loader.dataset.tensors[1], predicted)\n","#     print('Confusion Matrix:')\n","#     plt.figure(figsize=(4, 3))\n","#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","#     plt.xlabel(\"Predicted\")\n","#     plt.ylabel(\"True\")\n","#     plt.title(f\"LSTM - Confusion Matrix\")\n","#     plt.show()\n","#     print()\n","\n","def test(test_loader):\n","    total_loss = 0.0\n","    total_samples = 0\n","    outputs_list = []\n","\n","    with torch.no_grad():\n","        outputs = model(test_loader.dataset.tensors[0].reshape(-1, 1, 14))\n","        loss = criterion(outputs, test_loader.dataset.tensors[1].reshape(-1, 1))\n","        total_loss += loss.item() * len(test_loader.dataset)\n","        total_samples += len(test_loader.dataset)\n","        outputs_list.append(outputs)\n","\n","    avg_loss = total_loss / total_samples\n","\n","    # Combine outputs for all samples\n","    all_outputs = torch.cat(outputs_list, dim=0)\n","\n","    # Calculate other metrics as needed (accuracy, F1 score, confusion matrix, etc.)\n","    predicted = (all_outputs > 0.5).float()\n","    accuracy = (predicted == test_loader.dataset.tensors[1].reshape(-1, 1)).float().mean()\n","    f1 = f1_score(test_loader.dataset.tensors[1], predicted)\n","    cm = confusion_matrix(test_loader.dataset.tensors[1], predicted)\n","\n","    print(f'Average Loss: {avg_loss:.10f}')\n","    print(f'Accuracy: {accuracy.item():.8f}')\n","    print(f'F1 score: {f1:.4f}')\n","\n","    # Find the index of the last mismatch between predicted and true labels\n","    mismatch_idx = torch.nonzero(predicted != test_loader.dataset.tensors[1].reshape(-1, 1), as_tuple=False).numpy()\n","    if len(mismatch_idx) > 0:\n","        last_mismatch_idx = mismatch_idx[-1, 0]\n","        print(f'Correct from index: {last_mismatch_idx + 1}')\n","    else:\n","        print('No mismatches found.')\n","\n","    # Plot the predicted labels over time\n","    print()\n","    print('Predicted vs True Labels')\n","    plt.figure(figsize=(4, 2))\n","    plt.plot(predicted.numpy(), label='Predicted Labels')\n","    plt.plot(test_loader.dataset.tensors[1].numpy(), label='True Labels')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Label')\n","    plt.title('Predicted vs True Labels Over Time')\n","    plt.legend()\n","    plt.show()\n","    # Plot the confusion matrix\n","    print('Confusion Matrix:')\n","    plt.figure(figsize=(4, 3))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.title(f\"LSTM - Confusion Matrix\")\n","    plt.show()\n","    print()"],"metadata":{"id":"sVJXuiZC6y36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","model = LSTM(input_size, hidden_size, num_layers, output_size, random_seed=1234)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","train(train_loader, epochs=3000, print_every=100, valid_every=1000)\n","\n","# Evaluation\n","test(test_loader)"],"metadata":{"id":"f4frJsGb-U-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_tr = df_concat_2seq_tr\n","# df_te = df_concat_2seq_te\n","\n","# X_train = df_tr.drop(columns=['legitimate']).values\n","# y_train = df_tr['legitimate'].values\n","# X_test = df_te.drop(columns=['legitimate']).values\n","# y_test = df_te['legitimate'].values\n","\n","# X_train = torch.tensor(X_train, dtype=torch.float32)\n","# y_train = torch.tensor(y_train, dtype=torch.float32)\n","# X_test = torch.tensor(X_test, dtype=torch.float32)\n","# y_test = torch.tensor(y_test, dtype=torch.float32)"],"metadata":{"id":"7F6fx2t7oi5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # DataLoader 생성\n","# train_dataset = TensorDataset(X_train, y_train)\n","# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n","# test_dataset = TensorDataset(X_test, y_test)\n","# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"WMYtTJs_I6s2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class LSTM(nn.Module):\n","#     # def __init__(self,\n","#     #              input_size,\n","#     #              hidden_size,\n","#     #              num_layers,\n","#     #              output_size,\n","#     #              random_seed=1234):\n","\n","#     #     super(LSTM, self).__init__()\n","#     #     self.random_seed = random_seed  # random_seed를 클래스 변수로 설정\n","#     #     self.hidden_size = hidden_size\n","#     #     self.num_layers = num_layers\n","#     #     self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","#     #     self.fc = nn.Linear(hidden_size, output_size)\n","#     #     self.sigmoid = nn.Sigmoid()\n","\n","#     # def forward(self, x):\n","#     #     out, _ = self.lstm(x)\n","#     #     out = self.fc(out[:, -1, :])  # 마지막 시퀀스에서만 결과 추출\n","#     #     out = self.sigmoid(out)\n","#     #     return out\n","#     def __init__(self, input_size, hidden_size, num_layers, output_size, random_seed=1234):\n","#         super(LSTM, self).__init__()\n","#         self.random_seed = random_seed  # random_seed를 클래스 변수로 설정\n","#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","#         self.fc = nn.Linear(hidden_size, output_size)\n","#         self.sigmoid = nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         out, _ = self.lstm(x)\n","#         out = self.fc(out[:, -1, :])  # 마지막 시퀀스에서만 결과 추출\n","#         out = self.sigmoid(out)\n","#         return out\n","# # 모델 인스턴스 생성\n","# input_size = 14\n","# hidden_size = 64\n","# num_layers = 1\n","# output_size = 1\n","# model = LSTM(input_size, hidden_size, num_layers, output_size)\n","\n","# # 손실 함수 및 옵티마이저 설정\n","# criterion = nn.BCELoss()\n","# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# # 모델 훈련\n","# def train(epochs=3000, print_every=100, valid_every=1000):\n","#     model.train()\n","#     # for epoch in range(epochs):\n","#     #     total_loss = 0.0\n","#     #     for inputs, labels in train_loader:\n","#     #         optimizer.zero_grad()\n","#     #         outputs = model(inputs)\n","#     #         loss = criterion(outputs, labels)\n","#     #         loss.backward()\n","#     #         optimizer.step()\n","#     #         total_loss += loss.item()\n","#     for epoch in range(epochs):\n","#         for inputs, labels in train_loader:\n","#             optimizer.zero_grad()\n","#             outputs = model(inputs.unsqueeze(1))  # LSTM 모델에 입력\n","#             loss = criterion(outputs, labels.unsqueeze(1))\n","#             loss.backward()\n","#             optimizer.step()\n","#         if (epoch + 1) % print_every == 0:\n","#             print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n","#         if (epoch + 1) % valid_every == 0:\n","#             test(train_loader)\n","\n","\n","# # 모델 평가\n","# def test(test_loader):\n","#     outputs = model(X_test.unsqueeze(1))\n","#     predicted = (outputs > 0.5).float()\n","#     accuracy = (predicted == y_test.unsqueeze(1)).float().mean()\n","#     print(f'테스트 데이터 정확도: {accuracy.item():.4f}')\n","#     # model.eval()\n","#     # with torch.no_grad():\n","#     #     y_true = []\n","#     #     y_pred = []\n","#     #     for inputs, labels in test_loader:\n","#     #         outputs = model(inputs)\n","#     #         predicted = (outputs > 0.5).float()\n","#     #         y_pred.extend(predicted.cpu().numpy())\n","#     #         y_true.extend(labels.cpu().numpy())\n","\n","#     #     accuracy = accuracy_score(y_true, y_pred)\n","#     #     f1 = f1_score(y_true, y_pred)\n","#     #     cm = confusion_matrix(y_true, y_pred)\n","\n","#     #     print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","#     #     print(f'F1 Score: {f1:.2f}')\n","#     #     print('Confusion Matrix:')\n","#     #     # print(cm)\n","#     #     plt.figure(figsize=(4, 3))\n","#     #     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n","#     #     plt.xlabel(\"Predicted\")\n","#     #     plt.ylabel(\"True\")\n","#     #     plt.title(f\"LSTM - Confusion Matrix\")\n","#     #     plt.show()\n","#     #     print()\n","\n","#         # return accuracy, f1, cm\n"],"metadata":{"id":"QNMTYmQGlMEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # LSTM learning\n","# model = LSTM(input_size, hidden_size, num_layers, output_size, random_seed=1234)\n","# criterion = nn.BCELoss()\n","# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# train(epochs=50, print_every=1, valid_every=1)"],"metadata":{"id":"YFO6YhbVqbcf"},"execution_count":null,"outputs":[]}]}