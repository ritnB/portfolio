{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIs94wuvufre"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgYWBBQtugtz"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E45Uq1PuVRN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/to/project_directory')\n",
    "\n",
    "from config import *\n",
    "from util import get_center_points, is_same_length, is_list_strings, move_to_zero, move_to_zero_all, get_parsed_timestamp, process_dataframe\n",
    "# from Collected_dataset.dataloader import\n",
    "# from Collected_dataset.model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35AtLwAYuW23"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDNN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvtjLvjculnd"
   },
   "source": [
    "# File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pR3ErlDuYkq"
   },
   "outputs": [],
   "source": [
    "dict_path_tr_clean = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/clean/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/clean/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/clean/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/clean/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/clean/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/clean/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/clean/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/clean/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/clean/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_tr_attack = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/attack/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/attack/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/attack/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/attack/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/attack/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/attack/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/attack/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/attack/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/attack/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_clean = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/clean/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/clean/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/clean/35m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_attack = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/attack/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/attack/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/attack/35m_dyn2.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Cj6d5BuuapT"
   },
   "outputs": [],
   "source": [
    "#placeholder dictionary for collected dataframes\n",
    "dict_df_tr_clean={}\n",
    "dict_df_tr_attack={}\n",
    "dict_df_te_clean={}\n",
    "dict_df_te_attack={}\n",
    "\n",
    "#control variables\n",
    "dict_path=dict_path_tr_clean #path variable\n",
    "dict_df=dict_df_tr_clean #dict variable to collect dataframes read from paths\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_clean.items()})\n",
    "\n",
    "dict_path=dict_path_tr_attack\n",
    "dict_df=dict_df_tr_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_tr_attack.items()})\n",
    "\n",
    "dict_path=dict_path_te_clean\n",
    "dict_df=dict_df_te_clean\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_clean=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_clean.items()})\n",
    "\n",
    "dict_path=dict_path_te_attack\n",
    "dict_df=dict_df_te_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_attack=dict_df #안전장치\n",
    "print({key: len(value) for key, value in dict_df_te_attack.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_kP_HuVutdU"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YteIXxFuvhl"
   },
   "outputs": [],
   "source": [
    "# Make a new dictionary to store preprocessed df\n",
    "preprocessed_dict_tr_clean={}\n",
    "preprocessed_dict_tr_attack={}\n",
    "preprocessed_dict_te_clean={}\n",
    "preprocessed_dict_te_attack={}\n",
    "\n",
    "# Call def process_dataframe and store df\n",
    "point_numbers=0\n",
    "for key, df in dict_df_tr_clean.items():\n",
    "    if key not in preprocessed_dict_tr_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "        point_numbers+=len(df)\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_clean.items()})\n",
    "\n",
    "for key, df in dict_df_tr_attack.items():\n",
    "    if key not in preprocessed_dict_tr_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_attack.items()})\n",
    "\n",
    "for key, df in dict_df_te_clean.items():\n",
    "    if key not in preprocessed_dict_te_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_clean.items()})\n",
    "\n",
    "for key, df in dict_df_te_attack.items():\n",
    "    if key not in preprocessed_dict_te_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_attack.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx0H6Bu_uxXQ"
   },
   "outputs": [],
   "source": [
    "# labeling 0 for clean, 1 for attack\n",
    "for key, df in preprocessed_dict_tr_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_tr_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1\n",
    "for key, df in preprocessed_dict_te_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_te_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udl-IOtQu7d7"
   },
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C22sDvUuu6MW"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dict_tr_clean.keys())\n",
    "print(preprocessed_dict_tr_attack.keys())\n",
    "print(preprocessed_dict_te_clean.keys())\n",
    "print(preprocessed_dict_te_attack.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHEwzP3PvA3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import split_dataframe\n",
    "\n",
    "# split function... 왜인지 자꾸 import 에러 나서 여기 둠\n",
    "def split_dataframe(df):\n",
    "    diff = np.absolute(df['timestamp'].diff().values)\n",
    "    diff[0] = 0.0\n",
    "    diff = pd.Series(diff.round(decimals=5))\n",
    "    split_indices = np.where(diff >= 1.0)[0]\n",
    "    split_indices = [0] + list(split_indices) + [len(diff)]\n",
    "    split_dfs = []\n",
    "\n",
    "    for i, split_index in enumerate(split_indices):\n",
    "        if i == len(split_indices) - 1:\n",
    "            break\n",
    "        start_index = split_index\n",
    "        end_index = split_indices[i + 1]\n",
    "        split_dfs.append(df.iloc[start_index:end_index])\n",
    "\n",
    "    return split_dfs\n",
    "\n",
    "# preprocessed_dict와 동일한 key를 가진 딕셔너리를 만들고, item으로는 분할된 데이터프레임이 append된 리스트를 넣음\n",
    "dict_split_tr_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_clean.items()}\n",
    "dict_split_tr_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_attack.items()}\n",
    "dict_split_te_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_te_clean.items()}\n",
    "dict_split_te_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_te_attack.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QkY6sGAvCCX"
   },
   "outputs": [],
   "source": [
    "# Check length of the splitted dataset\n",
    "for key, split_list in dict_split_tr_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_tr_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "925n1XmDj11w"
   },
   "outputs": [],
   "source": [
    "# test_clean와 train_clean을 나누기 위한 key 필터링\n",
    "test_clean_keys = [key for key in dict_split_te_clean if key.startswith('35m')]\n",
    "train_clean_keys = [key for key in dict_split_tr_clean if not key.startswith('35m')]\n",
    "\n",
    "# test_attack와 train_attack을 나누기 위한 key 필터링\n",
    "test_attack_keys = [key for key in dict_split_te_attack if key.startswith('35m')]\n",
    "train_attack_keys = [key for key in dict_split_tr_attack if not key.startswith('35m')]\n",
    "\n",
    "# 각 데이터셋의 비행경로 개수 계산\n",
    "num_test_clean_paths = len(test_clean_keys)\n",
    "num_train_clean_paths = len(train_clean_keys)\n",
    "num_test_attack_paths = len(test_attack_keys)\n",
    "num_train_attack_paths = len(train_attack_keys)\n",
    "\n",
    "# 각 비행경로의 instance 개수 계산\n",
    "test_clean_instances_per_path = {key: [len(df) for df in dict_split_te_clean[key]] for key in test_clean_keys}\n",
    "train_clean_instances_per_path = {key: [len(df) for df in dict_split_tr_clean[key]] for key in train_clean_keys}\n",
    "test_attack_instances_per_path = {key: [len(df) for df in dict_split_te_attack[key]] for key in test_attack_keys}\n",
    "train_attack_instances_per_path = {key: [len(df) for df in dict_split_tr_attack[key]] for key in train_attack_keys}\n",
    "\n",
    "# Test_clean 데이터에서 각 비행경로 안의 instance 개수 출력\n",
    "print(\"Test_clean 데이터에서 각 비행경로의 instance 수:\")\n",
    "for key, instances in test_clean_instances_per_path.items():\n",
    "    print(f'Key: {key}, Instance Counts: {instances}')\n",
    "\n",
    "# Train_clean 데이터에서 각 비행경로 안의 instance 개수 출력\n",
    "print(\"\\nTrain_clean 데이터에서 각 비행경로의 instance 수:\")\n",
    "for key, instances in train_clean_instances_per_path.items():\n",
    "    print(f'Key: {key}, Instance Counts: {instances}')\n",
    "\n",
    "# Test_attack 데이터에서 각 비행경로 안의 instance 개수 출력\n",
    "print(\"\\nTest_attack 데이터에서 각 비행경로의 instance 수:\")\n",
    "for key, instances in test_attack_instances_per_path.items():\n",
    "    print(f'Key: {key}, Instance Counts: {instances}')\n",
    "\n",
    "# Train_attack 데이터에서 각 비행경로 안의 instance 개수 출력\n",
    "print(\"\\nTrain_attack 데이터에서 각 비행경로의 instance 수:\")\n",
    "for key, instances in train_attack_instances_per_path.items():\n",
    "    print(f'Key: {key}, Instance Counts: {instances}')\n",
    "\n",
    "# 소수점 아래 세 자리까지 자르기 위한 함수\n",
    "def truncate(value, decimals=3):\n",
    "    factor = 10.0 ** decimals\n",
    "    return np.floor(value * factor) / factor\n",
    "\n",
    "# Test_clean 데이터의 비행경로 당 평균, 최소, 최대 instance 수 계산\n",
    "avg_test_clean_instances_per_path = truncate(np.mean([np.mean(instances) for instances in test_clean_instances_per_path.values()]))\n",
    "min_test_clean_instances_per_path = min([min(instances) for instances in test_clean_instances_per_path.values()])\n",
    "max_test_clean_instances_per_path = max([max(instances) for instances in test_clean_instances_per_path.values()])\n",
    "\n",
    "# Train_clean 데이터의 비행경로 당 평균, 최소, 최대 instance 수 계산\n",
    "avg_train_clean_instances_per_path = truncate(np.mean([np.mean(instances) for instances in train_clean_instances_per_path.values()]))\n",
    "min_train_clean_instances_per_path = min([min(instances) for instances in train_clean_instances_per_path.values()])\n",
    "max_train_clean_instances_per_path = max([max(instances) for instances in train_clean_instances_per_path.values()])\n",
    "\n",
    "# Test_attack 데이터의 비행경로 당 평균, 최소, 최대 instance 수 계산\n",
    "avg_test_attack_instances_per_path = truncate(np.mean([np.mean(instances) for instances in test_attack_instances_per_path.values()]))\n",
    "min_test_attack_instances_per_path = min([min(instances) for instances in test_attack_instances_per_path.values()])\n",
    "max_test_attack_instances_per_path = max([max(instances) for instances in test_attack_instances_per_path.values()])\n",
    "\n",
    "# Train_attack 데이터의 비행경로 당 평균, 최소, 최대 instance 수 계산\n",
    "avg_train_attack_instances_per_path = truncate(np.mean([np.mean(instances) for instances in train_attack_instances_per_path.values()]))\n",
    "min_train_attack_instances_per_path = min([min(instances) for instances in train_attack_instances_per_path.values()])\n",
    "max_train_attack_instances_per_path = max([max(instances) for instances in train_attack_instances_per_path.values()])\n",
    "\n",
    "# Test 데이터 전체의 평균 instance 수 계산 (clean + attack)\n",
    "avg_test_instances_overall = truncate(np.mean([np.mean(instances) for instances in list(test_clean_instances_per_path.values()) + list(test_attack_instances_per_path.values())]))\n",
    "\n",
    "# Train 데이터 전체의 평균 instance 수 계산 (clean + attack)\n",
    "avg_train_instances_overall = truncate(np.mean([np.mean(instances) for instances in list(train_clean_instances_per_path.values()) + list(train_attack_instances_per_path.values())]))\n",
    "\n",
    "# Test_clean와 Test_attack 데이터의 전체 min과 max instance 수 계산\n",
    "min_test_instances_overall = min([min(instances) for instances in list(test_clean_instances_per_path.values()) + list(test_attack_instances_per_path.values())])\n",
    "max_test_instances_overall = max([max(instances) for instances in list(test_clean_instances_per_path.values()) + list(test_attack_instances_per_path.values())])\n",
    "\n",
    "# Train_clean와 Train_attack 데이터의 전체 min과 max instance 수 계산\n",
    "min_train_instances_overall = min([min(instances) for instances in list(train_clean_instances_per_path.values()) + list(train_attack_instances_per_path.values())])\n",
    "max_train_instances_overall = max([max(instances) for instances in list(train_clean_instances_per_path.values()) + list(train_attack_instances_per_path.values())])\n",
    "\n",
    "# Test, Train, 전체 데이터의 평균, 최소, 최대 instance 수 계산 (test + train)\n",
    "min_instances_overall = min(min_test_instances_overall, min_train_instances_overall)\n",
    "max_instances_overall = max(max_test_instances_overall, max_train_instances_overall)\n",
    "\n",
    "# Test, Train, 전체 데이터의 평균, 최소, 최대 instance 수 출력\n",
    "print(f\"\\nTest 데이터 전체의 비행경로당 평균 instance 수: {avg_test_instances_overall}\")\n",
    "print(f\"Test 데이터 전체의 비행경로당 최소 instance 수: {min_test_instances_overall}\")\n",
    "print(f\"Test 데이터 전체의 비행경로당 최대 instance 수: {max_test_instances_overall}\")\n",
    "\n",
    "print(f\"Train 데이터 전체의 비행경로당 평균 instance 수: {avg_train_instances_overall}\")\n",
    "print(f\"Train 데이터 전체의 비행경로당 최소 instance 수: {min_train_instances_overall}\")\n",
    "print(f\"Train 데이터 전체의 비행경로당 최대 instance 수: {max_train_instances_overall}\")\n",
    "\n",
    "print(f\"\\n전체 데이터의 비행경로당 평균 instance 수: {avg_test_instances_overall}\")\n",
    "print(f\"전체 데이터의 비행경로당 최소 instance 수: {min_instances_overall}\")\n",
    "print(f\"전체 데이터의 비행경로당 최대 instance 수: {max_instances_overall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0y2p6KKvFL4"
   },
   "outputs": [],
   "source": [
    "def count_nan(df_list):\n",
    "    nan_counts = sum(df.isna().values.sum() for df in df_list)\n",
    "    return f'({nan_counts})'\n",
    "\n",
    "len_dict_tr_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_clean.items()}\n",
    "len_dict_tr_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_attack.items()}\n",
    "len_dict_te_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_clean.items()}\n",
    "len_dict_te_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_attack.items()}\n",
    "\n",
    "print(len_dict_tr_clean)\n",
    "print(len_dict_tr_attack)\n",
    "print(len_dict_te_clean)\n",
    "print(len_dict_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKR2i1E_IYke"
   },
   "source": [
    "## Trajectory Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xzZh62D-iy9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D  # 범례를 위한 Line2D 클래스 임포트\n",
    "\n",
    "# 고도별로 그래프 생성 함수\n",
    "def plot_altitude_attack_type_grids(altitudes, attack_types, dict_tr_clean, dict_tr_attack, dict_te_clean, dict_te_attack):\n",
    "    fig, axs = plt.subplots(len(altitudes), len(attack_types), subplot_kw={'projection': '3d'}, figsize=(12, 16))\n",
    "    legend_lines = [Line2D([0], [0], color='#00acee', lw=4), Line2D([0], [0], color='#ff69b4', lw=4)]\n",
    "\n",
    "    for i, altitude in enumerate(altitudes):\n",
    "        for j, attack_type in enumerate(attack_types):\n",
    "            ax = axs[i, j]\n",
    "            key = f'{altitude}_{attack_type}'\n",
    "            dict_clean = dict_te_clean if altitude == '35m' else dict_tr_clean\n",
    "            dict_attack = dict_te_attack if altitude == '35m' else dict_tr_attack\n",
    "\n",
    "            if key in dict_clean:\n",
    "                for df in dict_clean[key]:\n",
    "                    ax.plot(df['lat'], df['lon'], df['alt'], color='#00acee')\n",
    "\n",
    "            if key in dict_attack:\n",
    "                for df in dict_attack[key]:\n",
    "                    ax.plot(df['lat'], df['lon'], df['alt'], color='#ff69b4')\n",
    "\n",
    "            ax.set_title(f'{altitude} {attack_type}')\n",
    "            # labelpad 값을 조정하여 축 레이블 위치 변경\n",
    "            ax.set_xlabel('Lat', labelpad=0)\n",
    "            ax.set_ylabel('Lon', labelpad=0)\n",
    "            ax.set_zlabel('Alt', labelpad=0)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_zticklabels([])\n",
    "\n",
    "    fig.legend(legend_lines, ['Clean', 'Attack'], loc='upper center', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 고도와 공격 유형 설정\n",
    "altitudes = ['20m', '35m', '50m', '70m']\n",
    "attack_types = ['stat', 'dyn1', 'dyn2']\n",
    "\n",
    "# 함수 호출\n",
    "plot_altitude_attack_type_grids(altitudes, attack_types, dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kgNq3WpIpsQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_2d_lat_lon_grids(altitudes, attack_types, dict_tr_clean, dict_tr_attack, dict_te_clean, dict_te_attack):\n",
    "    fig, axs = plt.subplots(len(altitudes), len(attack_types), figsize=(12, 16))\n",
    "    legend_lines = [Line2D([0], [0], color='#00acee', lw=4), Line2D([0], [0], color='#ff69b4', lw=4)]\n",
    "\n",
    "    for i, altitude in enumerate(altitudes):\n",
    "        for j, attack_type in enumerate(attack_types):\n",
    "            ax = axs[i, j]\n",
    "            key = f'{altitude}_{attack_type}'\n",
    "            dict_clean = dict_te_clean if altitude == '35m' else dict_tr_clean\n",
    "            dict_attack = dict_te_attack if altitude == '35m' else dict_tr_attack\n",
    "\n",
    "            # Clean 데이터 플로팅\n",
    "            if key in dict_clean:\n",
    "                for df in dict_clean[key]:\n",
    "                    ax.plot(df['lat'], df['lon'], color='#00acee')\n",
    "\n",
    "            # Attack 데이터 플로팅\n",
    "            if key in dict_attack:\n",
    "                for df in dict_attack[key]:\n",
    "                    ax.plot(df['lat'], df['lon'], color='#ff69b4')\n",
    "\n",
    "            ax.set_title(f'{altitude} {attack_type}')\n",
    "            ax.set_xlabel('Lat')\n",
    "            ax.set_ylabel('Lon')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "# 고도와 공격 유형 설정\n",
    "altitudes = ['20m', '35m', '50m', '70m']\n",
    "attack_types = ['stat', 'dyn1', 'dyn2']\n",
    "\n",
    "# 함수 호출\n",
    "plot_2d_lat_lon_grids(altitudes, attack_types, dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnFJwpdxBfZ9"
   },
   "outputs": [],
   "source": [
    "# 원하는 고도와 공격 유형의 조합에 대한 그래프만 생성하는 함수, 제목 수정 포함\n",
    "def plot_selected_combinations_grids_with_titles(altitudes, attack_types, dict_tr_clean, dict_tr_attack, dict_te_clean, dict_te_attack):\n",
    "    # 1행 3열의 subplot 배열 생성\n",
    "    fig, axs = plt.subplots(1, 3, subplot_kw={'projection': '3d'}, figsize=(18, 6))\n",
    "    legend_lines = [Line2D([0], [0], color='#00acee', lw=4), Line2D([0], [0], color='#ff69b4', lw=4)]\n",
    "\n",
    "    # 공격 유형에 따른 제목 매핑\n",
    "    title_mapping = {'stat': 'Static', 'dyn1': 'Dynamic 1', 'dyn2': 'Dynamic 2'}\n",
    "\n",
    "    # 원하는 조합만 그리기\n",
    "    desired_combinations = [('20m', 'stat'), ('20m', 'dyn1'), ('70m', 'dyn2')]\n",
    "    for idx, (altitude, attack_type) in enumerate(desired_combinations):\n",
    "        ax = axs[idx]\n",
    "        key = f'{altitude}_{attack_type}'\n",
    "        dict_clean = dict_te_clean if altitude == '35m' else dict_tr_clean\n",
    "        dict_attack = dict_te_attack if altitude == '35m' else dict_tr_attack\n",
    "\n",
    "        if key in dict_clean:\n",
    "            for df in dict_clean[key]:\n",
    "                ax.plot(df['lat'], df['lon'], df['alt'], color='#00acee')\n",
    "\n",
    "        if key in dict_attack:\n",
    "            for df in dict_attack[key]:\n",
    "                ax.plot(df['lat'], df['lon'], df['alt'], color='#ff69b4')\n",
    "\n",
    "        # 제목 설정\n",
    "        ax.set_title(title_mapping[attack_type])\n",
    "        ax.set_xlabel('Lat', labelpad=0)\n",
    "        ax.set_ylabel('Lon', labelpad=0)\n",
    "        ax.set_zlabel('Alt', labelpad=0)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_zticklabels([])\n",
    "\n",
    "    # 범례 및 출력 조정\n",
    "    fig.legend(legend_lines, ['Clean', 'Spoofed'], loc='upper center', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 함수 호출\n",
    "plot_selected_combinations_grids_with_titles(altitudes, attack_types, dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em6IEz5WXM9b"
   },
   "source": [
    "## Adding Sequenced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBfRCRJ8XQp4"
   },
   "outputs": [],
   "source": [
    "# Creating new columns = 'D_lon', 'D_lat', 'D_alt' : difference between two consecutive coordinates\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None) # <==== 경고를 끈다\n",
    "\n",
    "def add_diff(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 각 차분을 계산하고 'legitimate' 열 바로 앞에 삽입합니다.\n",
    "            df.insert(legitimate_index, 'D_lat', df['lat'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 1, 'D_lon', df['lon'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 2, 'D_alt', df['alt'].diff().fillna(0))\n",
    "\n",
    "add_diff(dict_split_tr_clean)\n",
    "add_diff(dict_split_tr_attack)\n",
    "add_diff(dict_split_te_clean)\n",
    "add_diff(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klRbmxdsXSIA"
   },
   "outputs": [],
   "source": [
    "# Creating new column = 'd_pos' : Ucladian distance between two consecutive coordinates\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def calculate_euc_distance(df):\n",
    "    coordinates = df[['lat', 'lon', 'alt']].to_numpy()\n",
    "    distances = [0.0]  # 첫 번째 행에는 0을 할당\n",
    "    for i in range(1, len(coordinates)):\n",
    "        distance = euclidean_distances([coordinates[i-1]], [coordinates[i]])[0][0]\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "def add_euc(dict_split):\n",
    "    # 각 데이터프레임에 대해 유클리드 거리 계산 및 'd_pos' 열 추가\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 유클리드 거리 계산\n",
    "            distances = calculate_euc_distance(df)\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'd_pos' 열을 'legitimate' 열 바로 전에 추가\n",
    "            df.insert(legitimate_index, 'd_pos', distances)\n",
    "\n",
    "add_euc(dict_split_tr_clean)\n",
    "add_euc(dict_split_tr_attack)\n",
    "add_euc(dict_split_te_clean)\n",
    "add_euc(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF0xsSdIXZJO"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'time_gap' : Time gap between two consecutive points\n",
    "\n",
    "def add_timeGap(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' 열의 인덱스를 가져오기.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'timestamp' 열의 차분을 계산하고 'legitimate' 열 바로 앞에 삽입\n",
    "            df.insert(legitimate_index, 'time_gap', df['timestamp'].diff().fillna(0))\n",
    "    return dict_split\n",
    "\n",
    "add_timeGap(dict_split_tr_clean)\n",
    "add_timeGap(dict_split_tr_attack)\n",
    "add_timeGap(dict_split_te_clean)\n",
    "add_timeGap(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifdeqKFFvKv7"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KVnN22va_V"
   },
   "source": [
    "## Creating Dataset for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lr3PAa9vNk8"
   },
   "outputs": [],
   "source": [
    "series_value = 0  # 전역 변수로 series_value 정의\n",
    "\n",
    "def add_df_id(dict_split):\n",
    "    global series_value  # 전역 변수 series_value 사용\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            df_length = len(df)\n",
    "            # 임시 열 생성\n",
    "            series_id_col = [series_value] * df_length\n",
    "            measurement_id_col = list(range(df_length))\n",
    "            # 첫 번째와 두 번째 위치에 열 삽입\n",
    "            df.insert(0, 'series_ID', series_id_col)\n",
    "            df.insert(1, 'measurement_ID', measurement_id_col)\n",
    "            series_value += 1  # series_value 업데이트\n",
    "\n",
    "# 함수 호출\n",
    "add_df_id(dict_split_tr_clean)\n",
    "add_df_id(dict_split_tr_attack)\n",
    "add_df_id(dict_split_te_clean)\n",
    "add_df_id(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-3nZMYJvPMB"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "500hgaZYvUod"
   },
   "outputs": [],
   "source": [
    "# 데이터프레임 길이를 저장할 리스트\n",
    "df_lengths = []\n",
    "\n",
    "# 모든 딕셔너리와 데이터프레임을 순회하며 길이 저장\n",
    "for dict_split in [dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack]:\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            df_lengths.append(len(df))\n",
    "\n",
    "# 최소 길이 찾기\n",
    "min_length = min(df_lengths)\n",
    "\n",
    "# 각 데이터프레임을 min_length로 잘라내기\n",
    "def dflen_min(dict_split):\n",
    "    for split_list in dict_split.values():\n",
    "        for i in range(len(split_list)):\n",
    "            split_list[i] = split_list[i].iloc[:min_length]\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "dflen_min(dict_split_tr_clean)\n",
    "dflen_min(dict_split_tr_attack)\n",
    "dflen_min(dict_split_te_clean)\n",
    "dflen_min(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MQHWAI0vgX-"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFnob-mwvg8k"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from multiprocessing import cpu_count       # 현재 CPU에서 사용 가능한 코어 수를 반환\n",
    "from pathlib import Path        # 경로를 조작\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler       # 높은 학습률로 시작하여 점차 낮추면서 학습하도록 동적 스케줄링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IVgltI0WBVd"
   },
   "source": [
    "### Feature Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq6E1pFGvinO"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(0)\n",
    "\n",
    "# 제외할 열 정하기\n",
    "ID_COLS = ['series_ID', 'measurement_ID']  # 각 데이터 포인트를 구별하는 데 사용되는 열\n",
    "LOC_COLS = ['lat', 'lon', 'alt', 'alt_ellipsoid']   # location informations\n",
    "AD_DROP_COLS = ['D_lat', 'D_lon', 'D_alt', 'd_pos', 'time_gap', 'timestamp', 'noise_per_ms', 'jamming_indicator', 'c_variance_rad', 's_variance_m_s', 'epv', 'eph', 'hdop', 'vdop', 'satellites_used']\n",
    "# 'D_lat', 'D_lon', 'D_alt', 'd_pos', 'time_gap', 'cog_rad', 'vel_m_s', 'vel_n_m_s', 'vel_e_m_s', 'vel_d_m_s',\n",
    "\n",
    "# 지정된 열을 제외하고 데이터프레임을 리스트로 변환하는 함수\n",
    "def df_to_np(dict_split, drop_cols):\n",
    "    nested_lists = []\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            # 지정된 열 제외\n",
    "            df_dropped = df.drop(columns=drop_cols)\n",
    "            # 데이터프레임의 각 행을 리스트로 변환\n",
    "            list_of_rows = df_dropped.values.tolist()\n",
    "            # 변환된 리스트들을 포함하는 리스트 추가\n",
    "            nested_lists.append(list_of_rows)\n",
    "    return nested_lists\n",
    "\n",
    "drop_cols = ID_COLS + LOC_COLS + AD_DROP_COLS\n",
    "\n",
    "# 각 딕셔너리에 대해 함수 호출\n",
    "list_np_tr_clean = np.array(df_to_np(dict_split_tr_clean, drop_cols))\n",
    "list_np_tr_attack = np.array(df_to_np(dict_split_tr_attack, drop_cols))\n",
    "list_np_te_clean = np.array(df_to_np(dict_split_te_clean, drop_cols))\n",
    "list_np_te_attack = np.array(df_to_np(dict_split_te_attack, drop_cols))\n",
    "\n",
    "# Training 데이터셋 합치기\n",
    "list_np_tr = np.concatenate([list_np_tr_clean, list_np_tr_attack])\n",
    "\n",
    "# Testing 데이터셋 합치기\n",
    "list_np_te = np.concatenate([list_np_te_clean, list_np_te_attack])\n",
    "\n",
    "remaining_features = [item for item in dict_split_te_attack['35m_dyn2'][3].columns if item not in drop_cols]\n",
    "\n",
    "# 결과 출력 (옵션)\n",
    "print(\"Remaining Features:\", remaining_features)\n",
    "print(\"Training Data Shape:\", list_np_tr.shape)\n",
    "print(\"Testing Data Shape:\", list_np_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqm2_1qvvolg"
   },
   "outputs": [],
   "source": [
    "def label_sep(X, time_dim_first=False):\n",
    "    # 라벨 분리 (마지막 열이 라벨)\n",
    "    y = X[:, -1, -1].reshape(-1, 1)\n",
    "    X = X[:, :, :-1]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_dataset(X, y, time_dim_first=False):\n",
    "    # 필요한 경우, 데이터의 차원을 재배열\n",
    "    if time_dim_first:\n",
    "        X = X.transpose(0, 2, 1)\n",
    "\n",
    "    # 데이터를 PyTorch 텐서로 변환\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    print(\"Shape of y after separating labels:\", y.shape)\n",
    "    print(\"Shape of X after separating labels:\", X.shape)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "\n",
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    # 훈련 및 검증 데이터 로더 생성\n",
    "    # bs는 배치 크기, jobs는 병렬 데이터 로딩을 위한 워커 수\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # 모델의 출력과 타깃 비교하여 정확도 계산\n",
    "    # 가장 높은 확률 값을 가진 클래스 선택\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuF5z3nR9uLP"
   },
   "outputs": [],
   "source": [
    "# 훈련 및 검증 데이터셋에서 라벨 분리\n",
    "np_tr_X, np_tr_y = label_sep(list_np_tr, time_dim_first=False)\n",
    "np_val_X, np_val_y = label_sep(list_np_te, time_dim_first=False)\n",
    "\n",
    "# PyTorch 텐서 데이터셋 생성\n",
    "trn_ds = create_dataset(np_tr_X, np_tr_y, time_dim_first=False)\n",
    "val_ds = create_dataset(np_val_X, np_val_y, time_dim_first=False)\n",
    "\n",
    "# # 훈련 및 검증 데이터셋 생성\n",
    "# trn_ds = create_dataset(list_np_tr, time_dim_first=False)\n",
    "# val_ds = create_dataset(list_np_te, time_dim_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K52b0PmX6aOW"
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH3Nz7t91Ycf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 형태에 따른 특성별 평균 계산 함수\n",
    "def calculate_feature_averages(X):\n",
    "    # 각 묶음 별로 평균을 계산\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에서 특성별 평균 계산\n",
    "X_train_avg = calculate_feature_averages(np_tr_X)\n",
    "X_test_avg = calculate_feature_averages(np_val_X)\n",
    "\n",
    "# 라벨의 차원을 조정 (예: [74, 1] -> [74])\n",
    "y_train = np_tr_y.flatten()\n",
    "y_test = np_val_y.flatten()\n",
    "\n",
    "# 모델 훈련\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_avg, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "test_accuracy = rf.score(X_test_avg, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 특성 이름 치환을 위한 딕셔너리 생성\n",
    "feature_name_mapping = {\n",
    "    'lat':r'$C_x$',\n",
    "    'lon':r'$C_y$',\n",
    "    'alt':r'$C_z$',\n",
    "    's_variance_m_s':r'$\\sigma_\\nu$',\n",
    "    'c_variance_rad':r'$\\sigma_g$',\n",
    "    'epv':r'$v_p$',\n",
    "    'hdop':r'$h_d$',\n",
    "    'vdop':r'$v_d$',\n",
    "    'noise_per_ms':r'$\\eta$',\n",
    "    'jamming_indicator':r'$\\vartheta$',\n",
    "    'vel_m_s':r'$\\nu$',\n",
    "    'vel_n_m_s':r'$\\nu_N$',\n",
    "    'vel_e_m_s':r'$\\nu_E$',\n",
    "    'vel_d_m_s':r'$\\nu_D$',\n",
    "    'cog_rad':r'$g$',\n",
    "    'satellites_used':r'$n_s$'\n",
    "}\n",
    "\n",
    "# 치환된 특성 이름 사용\n",
    "mapped_feature_names = [feature_name_mapping.get(feature, feature) for feature in remaining_features[:-1]]\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(4, 4))\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=mapped_feature_names)\n",
    "\n",
    "# 상위 몇개의 특성만 표기\n",
    "fts = 8                 # 표기할 특성의 개수\n",
    "top_features = feat_importances.sort_values(ascending=True).tail(fts)\n",
    "top_features.plot(kind='barh')\n",
    "plt.xlabel('Importance',fontsize=14)\n",
    "plt.ylabel('Features',fontsize=14)\n",
    "\n",
    "# # 각 막대에 특성 중요도 값 표시\n",
    "# for index, value in enumerate(top_features):\n",
    "#     plt.text(value, index, f\"{value:.4f}\", va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR2EH9mNvv3Y"
   },
   "outputs": [],
   "source": [
    "# y 레이블의 불순값 검사\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dl, valid_dl = create_loaders(trn_ds, val_ds, bs=147, jobs=cpu_count())\n",
    "\n",
    "# DataLoader에서 첫 번째 배치를 가져옴\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "\n",
    "# y_batch의 유일한 값들 출력\n",
    "unique_values = torch.unique(y_batch)\n",
    "print(\"Unique values in y_batch:\", unique_values)\n",
    "\n",
    "# 0과 1 이외의 값이 있는지 확인\n",
    "invalid_values = y_batch[(y_batch != 0) & (y_batch != 1)]\n",
    "if len(invalid_values) > 0:\n",
    "    print(\"Invalid values found in y_batch:\", invalid_values)\n",
    "else:\n",
    "    print(\"No invalid values found in y_batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5hL6HO1V4AU"
   },
   "source": [
    "### Sequence Length Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0AFil--U51l"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset\n",
    "def create_seq_data(X, seq_len=2):\n",
    "    num_samples, num_timesteps, num_features = X.shape\n",
    "    num_seq = num_timesteps - seq_len + 1\n",
    "\n",
    "    new_X = np.zeros((num_samples * num_seq, seq_len, num_features - 1))\n",
    "    new_y = np.zeros((num_samples * num_seq, 1))\n",
    "    count_legit_0 = 0\n",
    "    count_legit_1 = 0\n",
    "\n",
    "    for i in range(num_seq):\n",
    "        new_X[i * num_samples:(i + 1) * num_samples, :, :] = X[:, i:i+seq_len, :-1]\n",
    "        labels = X[:, i+seq_len-1, -1]\n",
    "        new_y[i * num_samples:(i + 1) * num_samples, 0] = labels\n",
    "        count_legit_0 += np.sum(labels == 0)\n",
    "        count_legit_1 += np.sum(labels == 1)\n",
    "\n",
    "    new_X_tensor = torch.tensor(new_X, dtype=torch.float32)\n",
    "    new_y_tensor = torch.tensor(new_y, dtype=torch.long)\n",
    "\n",
    "    print(f\"Instances with legitimate=0: {count_legit_0}\")\n",
    "    print(f\"Instances with legitimate=1: {count_legit_1}\")\n",
    "\n",
    "    return TensorDataset(new_X_tensor, new_y_tensor), (count_legit_0, count_legit_1)\n",
    "\n",
    "# sequence length 지정\n",
    "seq_len = 60\n",
    "\n",
    "# 수행\n",
    "ts_seq_tr, counts_tr = create_seq_data(list_np_tr, seq_len)\n",
    "ts_seq_te, counts_te = create_seq_data(list_np_te, seq_len)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "print(\"Training Data Tensor Size:\", ts_seq_tr.tensors[0].size())\n",
    "print(\"Training Labels Tensor Size:\", ts_seq_tr.tensors[1].size())\n",
    "print(\"Testing Data Tensor Size:\", ts_seq_te.tensors[0].size())\n",
    "print(\"Testing Labels Tensor Size:\", ts_seq_te.tensors[1].size())\n",
    "\n",
    "\n",
    "# # 데이터셋을 서로 바꿔서 수행\n",
    "# # 원래는 훈련용 데이터였던 list_np_tr을 테스트로, 테스트용 데이터였던 list_np_te를 훈련으로 사용\n",
    "# ts_seq_te, counts_te = create_seq_data(list_np_tr, seq_len)  # 원래 훈련 데이터 -> 테스트 데이터로 사용\n",
    "# ts_seq_tr, counts_tr = create_seq_data(list_np_te, seq_len)  # 원래 테스트 데이터 -> 훈련 데이터로 사용\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "# print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "# print(\"Training Data Tensor Size:\", ts_seq_tr.tensors[0].size())\n",
    "# print(\"Training Labels Tensor Size:\", ts_seq_tr.tensors[1].size())\n",
    "# print(\"Testing Data Tensor Size:\", ts_seq_te.tensors[0].size())\n",
    "# print(\"Testing Labels Tensor Size:\", ts_seq_te.tensors[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65odnf1vv0md"
   },
   "source": [
    "## Building RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xheczHc1vy_k"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        # 생성자: 스케줄러를 초기화\n",
    "        assert callable(schedule)  # schedule이 호출 가능한 함수인지 확인\n",
    "        self.schedule = schedule   # 학습률을 조정하는 함수 저장\n",
    "        super().__init__(optimizer, last_epoch)  # 부모 클래스의 생성자 호출\n",
    "\n",
    "    def get_lr(self):\n",
    "        # 현재 에포크에 대한 학습률을 계산\n",
    "        # base_lrs (기본 학습률)에 대해 schedule 함수를 호출하여 새로운 학습률 계산\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C1ofDvsv38p"
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    # 코사인 학습률 스케줄링 함수를 생성하는 함수\n",
    "    # t_max: 학습률 순환 주기\n",
    "    # eta_min: 최소 학습률\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        # 스케줄러 함수: 각 에포크에 대한 학습률 계산\n",
    "        # epoch: 현재 에포크 번호\n",
    "        # base_lr: 기본 학습률\n",
    "\n",
    "        # 현재 순환 주기 내의 상대적 위치 계산\n",
    "        t = epoch % t_max\n",
    "\n",
    "        # 코사인 기반의 학습률 계산\n",
    "        # 학습률은 eta_min과 base_lr 사이에서 조정됨\n",
    "        return eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * t / t_max)) / 2\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIX-V-Gfv5oY"
   },
   "outputs": [],
   "source": [
    "# 코사인 학습률 스케줄러를 사용하기 위한 설정\n",
    "n = 100  # 순환 주기\n",
    "sched = cosine(n)  # 코사인 학습률 스케줄러 생성\n",
    "\n",
    "# 주어진 범위(n * 4)에 대해 각 에포크별로 학습률 계산\n",
    "lrs = [sched(t, 1) for t in range(n * 4)]  # t: 에포크, 1: 기본 학습률\n",
    "\n",
    "plt.plot(lrs)  # 학습률 변화 그래프 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNpSdhRXv7WS"
   },
   "outputs": [],
   "source": [
    "print('Preparing datasets')\n",
    "# # 훈련 및 검증 데이터셋 생성\n",
    "# trn_ds = create_dataset(list_np_tr, time_dim_first=False)\n",
    "# val_ds = create_dataset(list_np_te, time_dim_first=False)\n",
    "\n",
    "# Sequence Dataset 생성\n",
    "trn_ds = ts_seq_tr\n",
    "val_ds = ts_seq_te\n",
    "\n",
    "bs = 128  # 배치 크기 설정\n",
    "print(f'Creating data loaders with batch size: {bs}')\n",
    "# 데이터 로더 생성\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs=bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf6psYmK8wxu"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZgrZaSHv-aE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "# Random Seed 설정\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPU가 사용 가능한 경우, CUDA random seed도 설정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        # 생성자: 모듈 초기화\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[64, 32, 16, output_dim], activation_layer=nn.ReLU) # 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순방향 패스 정의\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        # 초기 히든 상태와 셀 상태 생성\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            return [t.cuda() for t in (h0, c0)]\n",
    "        else:\n",
    "            return [h0, c0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnwiNEdP84Aq"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwphKfrXwmu8"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, 16, output_dim], activation_layer=nn.Tanh)  # 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.init_hidden(x)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY0hWje66gQv"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFYLHXhG_MQU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import MLP\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Random Seed 설정\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPU가 사용 가능한 경우, CUDA random seed도 설정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# TransformerClassifier 클래스를 정의\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nhead, num_layers, output_dim, max_len=5000):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, output_dim], activation_layer=nn.LeakyReLU)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)\n",
    "        # 평균 풀링을 사용하여 전체 시퀀스 정보 활용\n",
    "        transformer_out = transformer_out.mean(dim=1)\n",
    "        output = self.mlp(transformer_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZtBC7tY9A3u"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAqrZN-T9AK0"
   },
   "outputs": [],
   "source": [
    "# 모델 구성에 필요한 파라미터 설정\n",
    "input_dim = len(remaining_features)-1  # 입력 특성의 수\n",
    "hidden_dim = 128  # Context vector size\n",
    "layer_dim = 3  # Stack number\n",
    "num_layers = layer_dim\n",
    "output_dim = 1  # 출력 차원: 이진 분류의 경우 1\n",
    "nhead = 4  # 멀티 헤드 어텐션의 헤드 수\n",
    "\n",
    "# 학습률 및 에포크 설정\n",
    "lr = 0.0005\n",
    "n_epochs = 1000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "# best_acc = 0\n",
    "# patience, trials = 100, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TspvhfNM9Jyp"
   },
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxpxEprd9LYl"
   },
   "outputs": [],
   "source": [
    "# 분류 모델 인스턴스 생성\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# model = GRUClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# model = TransformerClassifier(input_dim, hidden_dim, nhead, num_layers, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4HzZ2yUKxaI"
   },
   "source": [
    "## Process Complete Alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9Ja8v9UK5GE"
   },
   "outputs": [],
   "source": [
    "# 텔레그램으로 학습완료 알림 오는 서비스\n",
    "# https://federicoraimondi.github.io/myProjects/Data_Stuff/knockknock_tutorial/Knock_Knock_tutorial.html\n",
    "!pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qggw4DjlK7PY"
   },
   "outputs": [],
   "source": [
    "# Put your token as a string\n",
    "your_token = \"token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "873GQw9zLC60"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's get your chat id! Be sure to have sent a message to your bot.\n",
    "url = 'https://api.telegram.org/bot'+str(your_token)+'/getUpdates'\n",
    "response = requests.get(url)\n",
    "myinfo = response.json()\n",
    "if response.status_code == 401:\n",
    "  raise NameError('Check if your token is correct.')\n",
    "\n",
    "try:\n",
    "  CHAT_ID: int = myinfo['result'][1]['message']['chat']['id']\n",
    "\n",
    "  print('This is your Chat ID:', CHAT_ID)\n",
    "\n",
    "except:\n",
    "  print('Have you sent a message to your bot? Telegram bot are quite shy 🤣.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwgWGEGw_hWN"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CCG3t0ZMh42"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from telegram import Bot\n",
    "\n",
    "# Colab에서 asyncio 사용을 위한 준비\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 비동기로 메시지를 보내는 함수 정의\n",
    "async def async_send_telegram_message(text, chat_id, token):\n",
    "    bot = Bot(token=token)\n",
    "    await bot.send_message(chat_id=chat_id, text=text)\n",
    "\n",
    "# 비동기 함수를 실행하는 함수 정의\n",
    "def send_telegram_message(text, chat_id, token):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = loop.create_task(async_send_telegram_message(text, chat_id, token))\n",
    "    loop.run_until_complete(task)\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(model, trn_dl, val_dl, lr, n_epochs, chat_id, token):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc = 0\n",
    "    patience, trials = 100, 0\n",
    "\n",
    "    print('Start model training')\n",
    "    # 훈련 과정\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        # 훈련 데이터셋을 이용한 학습\n",
    "        for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "            model.train()\n",
    "            x_batch = x_batch.cuda() if torch.cuda.is_available() else x_batch\n",
    "            y_batch = y_batch.cuda() if torch.cuda.is_available() else y_batch\n",
    "            opt.zero_grad()\n",
    "            out = model(x_batch)\n",
    "            loss = criterion(out, y_batch.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # 실제 레이블과 예측 레이블을 저장할 리스트\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # 검증 데이터셋을 이용한 평가\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        correct, total = 0, 0\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = [t.cuda() for t in (x_val, y_val)]  # 데이터를 CUDA로 이동\n",
    "            out = model(x_val)\n",
    "            probs = torch.sigmoid(out)  # 시그모이드 함수 적용하여 확률 계산\n",
    "            preds = probs > 0.5  # 임계값(0.5)을 기준으로 예측값 결정\n",
    "            # preds = F.log_softmax(out, dim=1).argmax(dim=1)  # 예측값 계산\n",
    "            total += y_val.size(0)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "\n",
    "        acc = correct / total  # 정확도 계산\n",
    "\n",
    "        # 에포크별 진행 상황 출력\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch: {epoch:3d}. Loss: {loss.item():.8f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "        # 최고 정확도 갱신 및 모델 저장\n",
    "        if acc > best_acc:\n",
    "            trials = 0\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best.pth')\n",
    "            print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    # 훈련이 완료되었음을 알리고, 가장 좋은 성능을 보였던 모델의 가중치를 복원\n",
    "    print('The training is finished! Restoring the best model weights')\n",
    "\n",
    "    # 가장 좋은 모델의 가중치를 불러옴\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "\n",
    "    final_message = '다해떠'\n",
    "    send_telegram_message(final_message, chat_id, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y9_kybywCor"
   },
   "outputs": [],
   "source": [
    "train_model(model, trn_dl, val_dl, lr, n_epochs, CHAT_ID, your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffm2UI8rOeUY"
   },
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FFuNfQ91EAA"
   },
   "outputs": [],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yFDmwZcXNRy"
   },
   "outputs": [],
   "source": [
    "import time  # 시간 측정을 위한 모듈 추가\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "test_predictions = []  # 예측 결과를 저장할 리스트\n",
    "print('Predicting on test dataset')\n",
    "\n",
    "start_time = time.time()  # 테스트 시작 시간 기록\n",
    "\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    for x_val, _ in val_dl:  # 테스트 데이터 로더(val_dl) 사용\n",
    "        x_val = x_val.cuda() if torch.cuda.is_available() else x_val  # GPU 사용 가능 시, 데이터를 GPU로 이동\n",
    "        out = model(x_val)  # 모델에 배치 데이터 전달 및 예측 수행\n",
    "        probs = torch.sigmoid(out)  # 시그모이드 함수 적용하여 확률 계산\n",
    "        preds = probs > 0.5  # 임계값(0.5)을 기준으로 예측값 결정\n",
    "        preds = preds.long()  # Bool 타입을 Long 타입으로 변환 (예측값을 0 또는 1로 변환)\n",
    "        test_predictions += preds.cpu().tolist()  # 예측 결과를 CPU로 이동한 후 리스트에 추가\n",
    "\n",
    "end_time = time.time()  # 테스트 종료 시간 기록\n",
    "test_time = end_time - start_time  # 테스트에 걸린 시간 계산\n",
    "print(f\"Test time: {test_time:.4f} seconds\")\n",
    "\n",
    "# 실제 레이블과 예측 레이블 준비\n",
    "y_true = []\n",
    "y_pred = test_predictions  # 예측 레이블 (위의 예측 코드 결과)\n",
    "\n",
    "for _, y_batch in val_dl:  # 실제 레이블 추출\n",
    "    y_true += y_batch.tolist()\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# F1 점수 계산\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# FPR 계산\n",
    "tn, fp, fn, tp = cm.ravel()  # 혼동 행렬에서 TN, FP, FN, TP 추출\n",
    "fpr = fp / (fp + tn)  # FPR 공식 적용\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "\n",
    "# FNR (False Negative Rate) 계산\n",
    "fnr = fn / (fn + tp)  # FNR 공식 적용\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "\n",
    "# Precision 계산\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall 계산\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53625qr_qFi"
   },
   "outputs": [],
   "source": [
    "# 잘못 분류된 데이터 포인트의 인덱스 찾기\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "\n",
    "# 잘못 분류된 데이터 포인트의 수 출력\n",
    "print(f\"Number of misclassified points: {len(misclassified_indices)}\")\n",
    "\n",
    "# numpy는 column 이름이 없었기 때문에 column 이름 다시 만들어주기\n",
    "column_names = dict_split_te_attack['35m_dyn2'][3].columns\n",
    "column_list = list(column_names)\n",
    "value_to_remove = drop_cols + ['legitimate']\n",
    "for values in value_to_remove:\n",
    "    if values in column_list:\n",
    "        column_list.remove(values)\n",
    "\n",
    "# 잘못 분류된 모든 데이터 포인트에 대한 정보 출력\n",
    "for index in misclassified_indices:\n",
    "    # 'val_ds[index]'로부터 데이터와 레이블 분리\n",
    "    data_tensor, label_tensor = val_ds[index]\n",
    "\n",
    "    # 데이터 텐서를 NumPy 배열로 변환\n",
    "    numpy_data = data_tensor.numpy()\n",
    "\n",
    "    # NumPy 배열을 Pandas 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(numpy_data, columns=column_list)\n",
    "\n",
    "    # 레이블 확인\n",
    "    label = label_tensor.item()\n",
    "\n",
    "    print(f\"Data Point Index: {index}\")\n",
    "    print(\"Data Frame:\\n\", df)\n",
    "    print(\"Label:\", label)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE9lRiUK1vGq"
   },
   "source": [
    "## Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IB0fWj61yNH"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 현재 시간을 기반으로 파일명 생성\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_path = f'path/to/project_directory.pth'\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNVhFdzLQZL+Yzw0mt5bihP",
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
