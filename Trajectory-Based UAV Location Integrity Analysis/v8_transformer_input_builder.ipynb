{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgYWBBQtugtz"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E45Uq1PuVRN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/to/project_directory')\n",
    "\n",
    "from config import *\n",
    "from util import get_center_points, is_same_length, is_list_strings, move_to_zero, move_to_zero_all, get_parsed_timestamp, process_dataframe\n",
    "# from Collected_dataset.dataloader import\n",
    "# from Collected_dataset.model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35AtLwAYuW23"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDNN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvtjLvjculnd"
   },
   "source": [
    "# File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pR3ErlDuYkq"
   },
   "outputs": [],
   "source": [
    "dict_path_tr_clean = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/clean/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/clean/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/clean/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/clean/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/clean/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/clean/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/clean/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/clean/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/clean/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_tr_attack = {\n",
    "    '20m_stat': ospj(path_workspace, 'path/to/train/attack/20m_stat.csv'),\n",
    "    '20m_dyn1': ospj(path_workspace, 'path/to/train/attack/20m_dyn1.csv'),\n",
    "    '20m_dyn2': ospj(path_workspace, 'path/to/train/attack/20m_dyn2.csv'),\n",
    "    '50m_stat': ospj(path_workspace, 'path/to/train/attack/50m_stat.csv'),\n",
    "    '50m_dyn1': ospj(path_workspace, 'path/to/train/attack/50m_dyn1.csv'),\n",
    "    '50m_dyn2': ospj(path_workspace, 'path/to/train/attack/50m_dyn2.csv'),\n",
    "    '70m_stat': ospj(path_workspace, 'path/to/train/attack/70m_stat.csv'),\n",
    "    '70m_dyn1': ospj(path_workspace, 'path/to/train/attack/70m_dyn1.csv'),\n",
    "    '70m_dyn2': ospj(path_workspace, 'path/to/train/attack/70m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_clean = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/clean/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/clean/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/clean/35m_dyn2.csv'),\n",
    "}\n",
    "\n",
    "dict_path_te_attack = {\n",
    "    '35m_stat': ospj(path_workspace, 'path/to/test/attack/35m_stat.csv'),\n",
    "    '35m_dyn1': ospj(path_workspace, 'path/to/test/attack/35m_dyn1.csv'),\n",
    "    '35m_dyn2': ospj(path_workspace, 'path/to/test/attack/35m_dyn2.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Cj6d5BuuapT"
   },
   "outputs": [],
   "source": [
    "#placeholder dictionary for collected dataframes\n",
    "dict_df_tr_clean={}\n",
    "dict_df_tr_attack={}\n",
    "dict_df_te_clean={}\n",
    "dict_df_te_attack={}\n",
    "\n",
    "#control variables\n",
    "dict_path=dict_path_tr_clean #path variable\n",
    "dict_df=dict_df_tr_clean #dict variable to collect dataframes read from paths\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_clean=dict_df #ì•ˆì „ì¥ì¹˜\n",
    "print({key: len(value) for key, value in dict_df_tr_clean.items()})\n",
    "\n",
    "dict_path=dict_path_tr_attack\n",
    "dict_df=dict_df_tr_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_tr_attack=dict_df #ì•ˆì „ì¥ì¹˜\n",
    "print({key: len(value) for key, value in dict_df_tr_attack.items()})\n",
    "\n",
    "dict_path=dict_path_te_clean\n",
    "dict_df=dict_df_te_clean\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_clean=dict_df #ì•ˆì „ì¥ì¹˜\n",
    "print({key: len(value) for key, value in dict_df_te_clean.items()})\n",
    "\n",
    "dict_path=dict_path_te_attack\n",
    "dict_df=dict_df_te_attack\n",
    "for key, path in dict_path.items():\n",
    "\tdict_df[key]=pd.read_csv(path)\n",
    "dict_df_te_attack=dict_df #ì•ˆì „ì¥ì¹˜\n",
    "print({key: len(value) for key, value in dict_df_te_attack.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_kP_HuVutdU"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YteIXxFuvhl"
   },
   "outputs": [],
   "source": [
    "# Make a new dictionary to store preprocessed df\n",
    "preprocessed_dict_tr_clean={}\n",
    "preprocessed_dict_tr_attack={}\n",
    "preprocessed_dict_te_clean={}\n",
    "preprocessed_dict_te_attack={}\n",
    "\n",
    "# Call def process_dataframe and store df\n",
    "point_numbers=0\n",
    "for key, df in dict_df_tr_clean.items():\n",
    "    if key not in preprocessed_dict_tr_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "        point_numbers+=len(df)\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_clean.items()})\n",
    "\n",
    "for key, df in dict_df_tr_attack.items():\n",
    "    if key not in preprocessed_dict_tr_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_tr_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_tr_attack.items()})\n",
    "\n",
    "for key, df in dict_df_te_clean.items():\n",
    "    if key not in preprocessed_dict_te_clean:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_clean[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_clean.items()})\n",
    "\n",
    "for key, df in dict_df_te_attack.items():\n",
    "    if key not in preprocessed_dict_te_attack:  # Check if already processed\n",
    "        preprocessed_df=process_dataframe(df)\n",
    "        preprocessed_dict_te_attack[key]=preprocessed_df\n",
    "        nan_counts=preprocessed_df.isna().values.sum()\n",
    "        print('NaN: ', nan_counts, end='\\t')\n",
    "print()\n",
    "print({key: len(value) for key, value in preprocessed_dict_te_attack.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx0H6Bu_uxXQ"
   },
   "outputs": [],
   "source": [
    "# labeling 0 for clean, 1 for attack\n",
    "for key, df in preprocessed_dict_tr_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_tr_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1\n",
    "for key, df in preprocessed_dict_te_clean.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=0\n",
    "for key, df in preprocessed_dict_te_attack.items():\n",
    "    if 'legitimate' not in df.columns:      # safety\n",
    "        df['legitimate']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udl-IOtQu7d7"
   },
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C22sDvUuu6MW"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dict_tr_clean.keys())\n",
    "print(preprocessed_dict_tr_attack.keys())\n",
    "print(preprocessed_dict_te_clean.keys())\n",
    "print(preprocessed_dict_te_attack.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHEwzP3PvA3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import split_dataframe\n",
    "\n",
    "# split function... ì™œì¸ì§€ ìê¾¸ import ì—ëŸ¬ ë‚˜ì„œ ì—¬ê¸° ë‘ \n",
    "def split_dataframe(df):\n",
    "    diff = np.absolute(df['timestamp'].diff().values)\n",
    "    diff[0] = 0.0\n",
    "    diff = pd.Series(diff.round(decimals=5))\n",
    "    split_indices = np.where(diff >= 1.0)[0]\n",
    "    split_indices = [0] + list(split_indices) + [len(diff)]\n",
    "    split_dfs = []\n",
    "\n",
    "    for i, split_index in enumerate(split_indices):\n",
    "        if i == len(split_indices) - 1:\n",
    "            break\n",
    "        start_index = split_index\n",
    "        end_index = split_indices[i + 1]\n",
    "        split_dfs.append(df.iloc[start_index:end_index])\n",
    "\n",
    "    return split_dfs\n",
    "\n",
    "# preprocessed_dictì™€ ë™ì¼í•œ keyë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ê³ , itemìœ¼ë¡œëŠ” ë¶„í• ëœ ë°ì´í„°í”„ë ˆì„ì´ appendëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ìŒ\n",
    "dict_split_tr_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_clean.items()}\n",
    "dict_split_tr_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_tr_attack.items()}\n",
    "dict_split_te_clean = {key: split_dataframe(df) for key,df in preprocessed_dict_te_clean.items()}\n",
    "dict_split_te_attack = {key: split_dataframe(df) for key,df in preprocessed_dict_te_attack.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QkY6sGAvCCX"
   },
   "outputs": [],
   "source": [
    "# Check length of the splitted dataset\n",
    "for key, split_list in dict_split_tr_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_tr_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_clean.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')\n",
    "\n",
    "for key, split_list in dict_split_te_attack.items():\n",
    "    print(f'Key: {key}, Number of Items: {len(split_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0y2p6KKvFL4"
   },
   "outputs": [],
   "source": [
    "def count_nan(df_list):\n",
    "    nan_counts = sum(df.isna().values.sum() for df in df_list)\n",
    "    return f'({nan_counts})'\n",
    "\n",
    "len_dict_tr_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_clean.items()}\n",
    "len_dict_tr_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_tr_attack.items()}\n",
    "len_dict_te_clean = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_clean.items()}\n",
    "len_dict_te_attack = {key: count_nan(df_list) + str(sum(len(df) for df in df_list)) for key, df_list in dict_split_te_attack.items()}\n",
    "\n",
    "print(len_dict_tr_clean)\n",
    "print(len_dict_tr_attack)\n",
    "print(len_dict_te_clean)\n",
    "print(len_dict_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKR2i1E_IYke"
   },
   "source": [
    "## Trajectory Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kgNq3WpIpsQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D  # ë²”ë¡€ë¥¼ ìœ„í•œ Line2D í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
    "\n",
    "# ê³ ë„ë³„ë¡œ ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜\n",
    "def plot_altitude_attack_type_grids(altitude, attack_types, dict_tr_clean, dict_tr_attack, dict_te_clean, dict_te_attack):\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax_list = []\n",
    "\n",
    "    for i, attack_type in enumerate(attack_types):\n",
    "        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "        ax_list.append(ax)\n",
    "\n",
    "        key = f'{altitude}_{attack_type}'\n",
    "        dict_clean = dict_te_clean if altitude == '35m' else dict_tr_clean\n",
    "        dict_attack = dict_te_attack if altitude == '35m' else dict_tr_attack\n",
    "\n",
    "        clean_line, attack_line = None, None\n",
    "\n",
    "        if key in dict_clean:\n",
    "            for df in dict_clean[key]:\n",
    "                linewidth = 4 if i == 0 else 2  # ì²« ë²ˆì§¸ ê·¸ë˜í”„ì˜ ì„  êµµê¸°ë§Œ ëŠ˜ë¦¼\n",
    "                clean_line, = ax.plot(df['lat'], df['lon'], df['alt'], color='#00acee', linewidth=linewidth)\n",
    "                if attack_type != 'stat':\n",
    "                    ax.scatter(df['lat'].iloc[0], df['lon'].iloc[0], df['alt'].iloc[0], color='#00acee', marker='^', s=100, facecolors='none', edgecolors='#00acee')  # ì‹œì‘ì \n",
    "                    ax.scatter(df['lat'].iloc[-1], df['lon'].iloc[-1], df['alt'].iloc[-1], color='#00acee', marker='o', s=100)  # ëì \n",
    "\n",
    "        if key in dict_attack:\n",
    "            for df in dict_attack[key]:\n",
    "                linewidth = 4 if i == 0 else 2  # ì²« ë²ˆì§¸ ê·¸ë˜í”„ì˜ ì„  êµµê¸°ë§Œ ëŠ˜ë¦¼\n",
    "                attack_line, = ax.plot(df['lat'], df['lon'], df['alt'], color='#ff69b4', linewidth=linewidth)\n",
    "                if attack_type != 'stat':\n",
    "                    ax.scatter(df['lat'].iloc[0], df['lon'].iloc[0], df['alt'].iloc[0], color='#ff69b4', marker='^', s=100, facecolors='none', edgecolors='#ff69b4')  # ì‹œì‘ì \n",
    "                    ax.scatter(df['lat'].iloc[-1], df['lon'].iloc[-1], df['alt'].iloc[-1], color='#ff69b4', marker='o', s=100)  # ëì \n",
    "\n",
    "        ax.set_xlabel('Latitude', fontsize=20)\n",
    "        ax.set_ylabel('Longitude', fontsize=20)\n",
    "        ax.set_zlabel('Altitude', fontsize=20)\n",
    "        ax.set_title('', fontsize=20)  # íƒ€ì´í‹€ ì œê±°\n",
    "\n",
    "        # ì¶• ìˆ«ìë¥¼ í‘œì‹œí•˜ì§€ ì•ŠìŒ\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_zticklabels([])\n",
    "\n",
    "        # ê° ê·¸ë˜í”„ë§ˆë‹¤ ë²”ë¡€ ì¶”ê°€\n",
    "        legend_lines = [\n",
    "            Line2D([0], [0], color='#00acee', lw=2, label='Authentic'),\n",
    "            Line2D([0], [0], color='#ff69b4', lw=2, label='Spoofed')\n",
    "        ]\n",
    "        ax.legend(handles=legend_lines, loc='upper right', prop={'size': 14})\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ ì¡°ì •\n",
    "    fig.subplots_adjust(top=0.85, bottom=0.1, left=0.05, right=0.95, wspace=0.3)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì¶œë ¥\n",
    "    plt.show()\n",
    "\n",
    "# ê³ ë„ì™€ ê³µê²© ìœ í˜• ì„¤ì •\n",
    "altitude = '20m'\n",
    "attack_types = ['stat', 'dyn1', 'dyn2']\n",
    "\n",
    "# ê° ê³ ë„ ë° ê³µê²© ìœ í˜•ì— ëŒ€í•´ ê·¸ë˜í”„ ìƒì„± ë° ì¶œë ¥\n",
    "plot_altitude_attack_type_grids(altitude, attack_types, dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em6IEz5WXM9b"
   },
   "source": [
    "## Adding Sequenced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBfRCRJ8XQp4"
   },
   "outputs": [],
   "source": [
    "# Creating new columns = 'D_lon', 'D_lat', 'D_alt' : difference between two consecutive coordinates\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None) # <==== ê²½ê³ ë¥¼ ëˆë‹¤\n",
    "\n",
    "def add_diff(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' ì—´ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê¸°.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # ê° ì°¨ë¶„ì„ ê³„ì‚°í•˜ê³  'legitimate' ì—´ ë°”ë¡œ ì•ì— ì‚½ì…í•©ë‹ˆë‹¤.\n",
    "            df.insert(legitimate_index, 'D_lat', df['lat'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 1, 'D_lon', df['lon'].diff().fillna(0))\n",
    "            df.insert(legitimate_index + 2, 'D_alt', df['alt'].diff().fillna(0))\n",
    "\n",
    "add_diff(dict_split_tr_clean)\n",
    "add_diff(dict_split_tr_attack)\n",
    "add_diff(dict_split_te_clean)\n",
    "add_diff(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klRbmxdsXSIA"
   },
   "outputs": [],
   "source": [
    "# Creating new column = 'd_pos' : Ucladian distance between two consecutive coordinates\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def calculate_euc_distance(df):\n",
    "    coordinates = df[['lat', 'lon', 'alt']].to_numpy()\n",
    "    distances = [0.0]  # ì²« ë²ˆì§¸ í–‰ì—ëŠ” 0ì„ í• ë‹¹\n",
    "    for i in range(1, len(coordinates)):\n",
    "        distance = euclidean_distances([coordinates[i-1]], [coordinates[i]])[0][0]\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "def add_euc(dict_split):\n",
    "    # ê° ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚° ë° 'd_pos' ì—´ ì¶”ê°€\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°\n",
    "            distances = calculate_euc_distance(df)\n",
    "            # 'legitimate' ì—´ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê¸°.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'd_pos' ì—´ì„ 'legitimate' ì—´ ë°”ë¡œ ì „ì— ì¶”ê°€\n",
    "            df.insert(legitimate_index, 'd_pos', distances)\n",
    "\n",
    "add_euc(dict_split_tr_clean)\n",
    "add_euc(dict_split_tr_attack)\n",
    "add_euc(dict_split_te_clean)\n",
    "add_euc(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF0xsSdIXZJO"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'time_gap' : Time gap between two consecutive points\n",
    "\n",
    "def add_timeGap(dict_split):\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            # 'legitimate' ì—´ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê¸°.\n",
    "            legitimate_index = df.columns.get_loc('legitimate')\n",
    "            # 'timestamp' ì—´ì˜ ì°¨ë¶„ì„ ê³„ì‚°í•˜ê³  'legitimate' ì—´ ë°”ë¡œ ì•ì— ì‚½ì…\n",
    "            df.insert(legitimate_index, 'time_gap', df['timestamp'].diff().fillna(0))\n",
    "    return dict_split\n",
    "\n",
    "add_timeGap(dict_split_tr_clean)\n",
    "add_timeGap(dict_split_tr_attack)\n",
    "add_timeGap(dict_split_te_clean)\n",
    "add_timeGap(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifdeqKFFvKv7"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KVnN22va_V"
   },
   "source": [
    "## Creating Dataset for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lr3PAa9vNk8"
   },
   "outputs": [],
   "source": [
    "series_value = 0  # ì „ì—­ ë³€ìˆ˜ë¡œ series_value ì •ì˜\n",
    "\n",
    "def add_df_id(dict_split):\n",
    "    global series_value  # ì „ì—­ ë³€ìˆ˜ series_value ì‚¬ìš©\n",
    "    for key, split_list in dict_split.items():\n",
    "        for df in split_list:\n",
    "            df_length = len(df)\n",
    "            # ì„ì‹œ ì—´ ìƒì„±\n",
    "            series_id_col = [series_value] * df_length\n",
    "            measurement_id_col = list(range(df_length))\n",
    "            # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ìœ„ì¹˜ì— ì—´ ì‚½ì…\n",
    "            df.insert(0, 'series_ID', series_id_col)\n",
    "            df.insert(1, 'measurement_ID', measurement_id_col)\n",
    "            series_value += 1  # series_value ì—…ë°ì´íŠ¸\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "add_df_id(dict_split_tr_clean)\n",
    "add_df_id(dict_split_tr_attack)\n",
    "add_df_id(dict_split_te_clean)\n",
    "add_df_id(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-3nZMYJvPMB"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "500hgaZYvUod"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ ê¸¸ì´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "df_lengths = []\n",
    "\n",
    "# ëª¨ë“  ë”•ì…”ë„ˆë¦¬ì™€ ë°ì´í„°í”„ë ˆì„ì„ ìˆœíšŒí•˜ë©° ê¸¸ì´ ì €ì¥\n",
    "for dict_split in [dict_split_tr_clean, dict_split_tr_attack, dict_split_te_clean, dict_split_te_attack]:\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            df_lengths.append(len(df))\n",
    "\n",
    "# ìµœì†Œ ê¸¸ì´ ì°¾ê¸°\n",
    "min_length = min(df_lengths)\n",
    "\n",
    "# ê° ë°ì´í„°í”„ë ˆì„ì„ min_lengthë¡œ ì˜ë¼ë‚´ê¸°\n",
    "def dflen_min(dict_split):\n",
    "    for split_list in dict_split.values():\n",
    "        for i in range(len(split_list)):\n",
    "            split_list[i] = split_list[i].iloc[:min_length]\n",
    "\n",
    "# ê° ë”•ì…”ë„ˆë¦¬ì— ëŒ€í•´ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "dflen_min(dict_split_tr_clean)\n",
    "dflen_min(dict_split_tr_attack)\n",
    "dflen_min(dict_split_te_clean)\n",
    "dflen_min(dict_split_te_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MQHWAI0vgX-"
   },
   "outputs": [],
   "source": [
    "dict_split_te_attack['35m_dyn2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFnob-mwvg8k"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from multiprocessing import cpu_count       # í˜„ì¬ CPUì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ì–´ ìˆ˜ë¥¼ ë°˜í™˜\n",
    "from pathlib import Path        # ê²½ë¡œë¥¼ ì¡°ì‘\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler       # ë†’ì€ í•™ìŠµë¥ ë¡œ ì‹œì‘í•˜ì—¬ ì ì°¨ ë‚®ì¶”ë©´ì„œ í•™ìŠµí•˜ë„ë¡ ë™ì  ìŠ¤ì¼€ì¤„ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IVgltI0WBVd"
   },
   "source": [
    "### Feature Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq6E1pFGvinO"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(0)\n",
    "\n",
    "# ì œì™¸í•  ì—´ ì •í•˜ê¸°\n",
    "ID_COLS = ['series_ID', 'measurement_ID']  # ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì—´\n",
    "LOC_COLS = ['lat', 'lon', 'alt', 'alt_ellipsoid']   # location informations\n",
    "# LOC_COLS = ['alt_ellipsoid']\n",
    "AD_DROP_COLS = ['timestamp', 'noise_per_ms', 'jamming_indicator', 'c_variance_rad', 's_variance_m_s', 'epv', 'eph', 'hdop', 'vdop', 'satellites_used']\n",
    "# AD_DROP_COLS = ['timestamp', 'eph']\n",
    "\n",
    "# ì§€ì •ëœ ì—´ì„ ì œì™¸í•˜ê³  ë°ì´í„°í”„ë ˆì„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def df_to_np(dict_split, drop_cols):\n",
    "    nested_lists = []\n",
    "    for split_list in dict_split.values():\n",
    "        for df in split_list:\n",
    "            # ì§€ì •ëœ ì—´ ì œì™¸\n",
    "            df_dropped = df.drop(columns=drop_cols)\n",
    "            # ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            list_of_rows = df_dropped.values.tolist()\n",
    "            # ë³€í™˜ëœ ë¦¬ìŠ¤íŠ¸ë“¤ì„ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì¶”ê°€\n",
    "            nested_lists.append(list_of_rows)\n",
    "    return nested_lists\n",
    "\n",
    "drop_cols = ID_COLS + LOC_COLS + AD_DROP_COLS\n",
    "\n",
    "# ê° ë”•ì…”ë„ˆë¦¬ì— ëŒ€í•´ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "list_np_tr_clean = np.array(df_to_np(dict_split_tr_clean, drop_cols))\n",
    "list_np_tr_attack = np.array(df_to_np(dict_split_tr_attack, drop_cols))\n",
    "list_np_te_clean = np.array(df_to_np(dict_split_te_clean, drop_cols))\n",
    "list_np_te_attack = np.array(df_to_np(dict_split_te_attack, drop_cols))\n",
    "\n",
    "# Training ë°ì´í„°ì…‹ í•©ì¹˜ê¸°\n",
    "list_np_tr = np.concatenate([list_np_tr_clean, list_np_tr_attack])\n",
    "\n",
    "# Testing ë°ì´í„°ì…‹ í•©ì¹˜ê¸°\n",
    "list_np_te = np.concatenate([list_np_te_clean, list_np_te_attack])\n",
    "\n",
    "remaining_features = [item for item in dict_split_te_attack['35m_dyn2'][3].columns if item not in drop_cols]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (ì˜µì…˜)\n",
    "print(\"Remaining Features:\", remaining_features)\n",
    "print(\"Training Data Shape:\", list_np_tr.shape)\n",
    "print(\"Testing Data Shape:\", list_np_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqm2_1qvvolg"
   },
   "outputs": [],
   "source": [
    "def label_sep(X, time_dim_first=False):\n",
    "    # ë¼ë²¨ ë¶„ë¦¬ (ë§ˆì§€ë§‰ ì—´ì´ ë¼ë²¨)\n",
    "    y = X[:, -1, -1].reshape(-1, 1)\n",
    "    X = X[:, :, :-1]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_dataset(X, y, time_dim_first=False):\n",
    "    # í•„ìš”í•œ ê²½ìš°, ë°ì´í„°ì˜ ì°¨ì›ì„ ì¬ë°°ì—´\n",
    "    if time_dim_first:\n",
    "        X = X.transpose(0, 2, 1)\n",
    "\n",
    "    # ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    print(\"Shape of y after separating labels:\", y.shape)\n",
    "    print(\"Shape of X after separating labels:\", X.shape)\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "\n",
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    # í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„±\n",
    "    # bsëŠ” ë°°ì¹˜ í¬ê¸°, jobsëŠ” ë³‘ë ¬ ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ ì›Œì»¤ ìˆ˜\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # ëª¨ë¸ì˜ ì¶œë ¥ê³¼ íƒ€ê¹ƒ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
    "    # ê°€ì¥ ë†’ì€ í™•ë¥  ê°’ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuF5z3nR9uLP"
   },
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ë¼ë²¨ ë¶„ë¦¬\n",
    "np_tr_X, np_tr_y = label_sep(list_np_tr, time_dim_first=False)\n",
    "np_val_X, np_val_y = label_sep(list_np_te, time_dim_first=False)\n",
    "\n",
    "# PyTorch í…ì„œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "trn_ds = create_dataset(np_tr_X, np_tr_y, time_dim_first=False)\n",
    "val_ds = create_dataset(np_val_X, np_val_y, time_dim_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K52b0PmX6aOW"
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH3Nz7t91Ycf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„°ì…‹ í˜•íƒœì— ë”°ë¥¸ íŠ¹ì„±ë³„ í‰ê·  ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_feature_averages(X):\n",
    "    # ê° ë¬¶ìŒ ë³„ë¡œ í‰ê· ì„ ê³„ì‚°\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„±ë³„ í‰ê·  ê³„ì‚°\n",
    "X_train_avg = calculate_feature_averages(np_tr_X)\n",
    "X_test_avg = calculate_feature_averages(np_val_X)\n",
    "\n",
    "# ë¼ë²¨ì˜ ì°¨ì›ì„ ì¡°ì • (ì˜ˆ: [74, 1] -> [74])\n",
    "y_train = np_tr_y.flatten()\n",
    "y_test = np_val_y.flatten()\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_avg, y_train)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "test_accuracy = rf.score(X_test_avg, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# íŠ¹ì„± ì´ë¦„ ì¹˜í™˜ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "feature_name_mapping = {\n",
    "    'lat':r'$C_x$',\n",
    "    'lon':r'$C_y$',\n",
    "    'alt':r'$C_z$',\n",
    "    's_variance_m_s':r'$\\sigma_\\nu$',\n",
    "    'c_variance_rad':r'$\\sigma_g$',\n",
    "    'epv':r'$v_p$',\n",
    "    'hdop':r'$h_d$',\n",
    "    'vdop':r'$v_d$',\n",
    "    'noise_per_ms':r'$\\eta$',\n",
    "    'jamming_indicator':r'$\\vartheta$',\n",
    "    'vel_m_s':r'$\\nu$',\n",
    "    'vel_n_m_s':r'$\\nu_N$',\n",
    "    'vel_e_m_s':r'$\\nu_E$',\n",
    "    'vel_d_m_s':r'$\\nu_D$',\n",
    "    'cog_rad':r'$g$',\n",
    "    'satellites_used':r'$n_s$'\n",
    "}\n",
    "\n",
    "# ì¹˜í™˜ëœ íŠ¹ì„± ì´ë¦„ ì‚¬ìš©\n",
    "mapped_feature_names = [feature_name_mapping.get(feature, feature) for feature in remaining_features[:-1]]\n",
    "\n",
    "# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”\n",
    "plt.figure(figsize=(4, 4))\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=mapped_feature_names)\n",
    "\n",
    "# ìƒìœ„ ëª‡ê°œì˜ íŠ¹ì„±ë§Œ í‘œê¸°\n",
    "fts = 8                 # í‘œê¸°í•  íŠ¹ì„±ì˜ ê°œìˆ˜\n",
    "top_features = feat_importances.sort_values(ascending=True).tail(fts)\n",
    "top_features.plot(kind='barh')\n",
    "plt.xlabel('Importance',fontsize=14)\n",
    "plt.ylabel('Features',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR2EH9mNvv3Y"
   },
   "outputs": [],
   "source": [
    "# y ë ˆì´ë¸”ì˜ ë¶ˆìˆœê°’ ê²€ì‚¬\n",
    "\n",
    "# ë°ì´í„° ë¡œë” ìƒì„±\n",
    "train_dl, valid_dl = create_loaders(trn_ds, val_ds, bs=147, jobs=cpu_count())\n",
    "\n",
    "# DataLoaderì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜´\n",
    "x_batch, y_batch = next(iter(train_dl))\n",
    "\n",
    "# y_batchì˜ ìœ ì¼í•œ ê°’ë“¤ ì¶œë ¥\n",
    "unique_values = torch.unique(y_batch)\n",
    "print(\"Unique values in y_batch:\", unique_values)\n",
    "\n",
    "# 0ê³¼ 1 ì´ì™¸ì˜ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "invalid_values = y_batch[(y_batch != 0) & (y_batch != 1)]\n",
    "if len(invalid_values) > 0:\n",
    "    print(\"Invalid values found in y_batch:\", invalid_values)\n",
    "else:\n",
    "    print(\"No invalid values found in y_batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5hL6HO1V4AU"
   },
   "source": [
    "### Sequence Length Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0AFil--U51l"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def create_seq_data(X, seq_len=2):\n",
    "    num_samples, num_timesteps, num_features = X.shape\n",
    "    num_seq = num_timesteps - seq_len + 1\n",
    "\n",
    "    new_X = np.zeros((num_samples * num_seq, seq_len, num_features - 1))\n",
    "    new_y = np.zeros((num_samples * num_seq, 1))\n",
    "    count_legit_0 = 0\n",
    "    count_legit_1 = 0\n",
    "\n",
    "    for i in range(num_seq):\n",
    "        new_X[i * num_samples:(i + 1) * num_samples, :, :] = X[:, i:i+seq_len, :-1]\n",
    "        labels = X[:, i+seq_len-1, -1]\n",
    "        new_y[i * num_samples:(i + 1) * num_samples, 0] = labels\n",
    "        count_legit_0 += np.sum(labels == 0)\n",
    "\n",
    "    new_X_tensor = torch.tensor(new_X, dtype=torch.float32)\n",
    "    new_y_tensor = torch.tensor(new_y, dtype=torch.long)\n",
    "\n",
    "    print(f\"Instances with legitimate=0: {count_legit_0}\")\n",
    "    print(f\"Instances with legitimate=1: {count_legit_1}\")\n",
    "\n",
    "    return TensorDataset(new_X_tensor, new_y_tensor), (count_legit_0, count_legit_1)\n",
    "\n",
    "# sequence length ì§€ì •\n",
    "seq_len = 75\n",
    "\n",
    "# ìˆ˜í–‰\n",
    "ts_seq_tr, counts_tr = create_seq_data(list_np_tr, seq_len)\n",
    "ts_seq_te, counts_te = create_seq_data(list_np_te, seq_len)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ ì„œë¡œ ë°”ê¿”ì„œ ìˆ˜í–‰\n",
    "# ì›ë˜ëŠ” í›ˆë ¨ìš© ë°ì´í„°ì˜€ë˜ list_np_trì„ í…ŒìŠ¤íŠ¸ë¡œ, í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì˜€ë˜ list_np_teë¥¼ í›ˆë ¨ìœ¼ë¡œ ì‚¬ìš©\n",
    "ts_seq_te, counts_te = create_seq_data(list_np_tr, seq_len)  # ì›ë˜ í›ˆë ¨ ë°ì´í„° -> í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "ts_seq_tr, counts_tr = create_seq_data(list_np_te, seq_len)  # ì›ë˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° -> í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Training Set - Instances with legitimate=0:\", counts_tr[0], \"legitimate=1:\", counts_tr[1])\n",
    "print(\"Test Set - Instances with legitimate=0:\", counts_te[0], \"legitimate=1:\", counts_te[1])\n",
    "\n",
    "print(\"Training Data Tensor Size:\", ts_seq_tr.tensors[0].size())\n",
    "print(\"Training Labels Tensor Size:\", ts_seq_tr.tensors[1].size())\n",
    "print(\"Testing Data Tensor Size:\", ts_seq_te.tensors[0].size())\n",
    "print(\"Testing Labels Tensor Size:\", ts_seq_te.tensors[1].size())\n",
    "\n",
    "# trainê³¼ test TensorDatasetì„ ê²°í•©í•˜ì—¬ combined_dataset ìƒì„±\n",
    "combined_dataset = ConcatDataset([ts_seq_tr, ts_seq_te])\n",
    "\n",
    "# ê²°í•©ëœ ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸\n",
    "print(\"Combined Dataset - Total Instances:\", len(combined_dataset))\n",
    "print(\"First Entry - Data Shape:\", combined_dataset[0][0].shape)\n",
    "print(\"First Entry - Label Shape:\", combined_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBoM0E4wNZmG"
   },
   "source": [
    "### Masking (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ac5BJ_zNYp4"
   },
   "outputs": [],
   "source": [
    "# ë§ˆìŠ¤í‚¹ í•¨ìˆ˜ ì •ì˜ ë° ì ìš©\n",
    "def mask_sequence_data(X, mask_percentage=0.15):\n",
    "    num_samples, seq_len, num_features = X.shape\n",
    "    mask_size = int(np.ceil(mask_percentage * seq_len))\n",
    "    masked_indices = np.random.randint(0, seq_len, (num_samples, mask_size))\n",
    "    X_masked = X.clone()\n",
    "    mask_array = torch.zeros_like(X, dtype=torch.bool)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        X_masked[i, masked_indices[i], :] = 0\n",
    "        mask_array[i, masked_indices[i], :] = True\n",
    "\n",
    "    return X_masked, mask_array\n",
    "\n",
    "def visualize_masking(X_masked, num_samples=5):\n",
    "    for i in range(num_samples):\n",
    "        print(\"Sample\", i, \":\")\n",
    "        print(X_masked[i])\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "X_masked, _ = mask_sequence_data(torch.tensor(np.random.randn(10, 5, 3), dtype=torch.float32), mask_percentage=0.15) # ë§ˆìŠ¤í‚¹ ë¹„ìœ¨\n",
    "visualize_masking(X_masked)\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ ì ìš©\n",
    "masked_tr_X, original_tr_X = mask_sequence_data(ts_seq_tr.tensors[0])\n",
    "masked_te_X, original_te_X = mask_sequence_data(ts_seq_te.tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65odnf1vv0md"
   },
   "source": [
    "## Building RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xheczHc1vy_k"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        # ìƒì„±ì: ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì´ˆê¸°í™”\n",
    "        assert callable(schedule)  # scheduleì´ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜ì¸ì§€ í™•ì¸\n",
    "        self.schedule = schedule   # í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ ì €ì¥\n",
    "        super().__init__(optimizer, last_epoch)  # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ìƒì„±ì í˜¸ì¶œ\n",
    "\n",
    "    def get_lr(self):\n",
    "        # í˜„ì¬ ì—í¬í¬ì— ëŒ€í•œ í•™ìŠµë¥ ì„ ê³„ì‚°\n",
    "        # base_lrs (ê¸°ë³¸ í•™ìŠµë¥ )ì— ëŒ€í•´ schedule í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒˆë¡œìš´ í•™ìŠµë¥  ê³„ì‚°\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C1ofDvsv38p"
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    # ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    # t_max: í•™ìŠµë¥  ìˆœí™˜ ì£¼ê¸°\n",
    "    # eta_min: ìµœì†Œ í•™ìŠµë¥ \n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜: ê° ì—í¬í¬ì— ëŒ€í•œ í•™ìŠµë¥  ê³„ì‚°\n",
    "        # epoch: í˜„ì¬ ì—í¬í¬ ë²ˆí˜¸\n",
    "        # base_lr: ê¸°ë³¸ í•™ìŠµë¥ \n",
    "\n",
    "        # í˜„ì¬ ìˆœí™˜ ì£¼ê¸° ë‚´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚°\n",
    "        t = epoch % t_max\n",
    "\n",
    "        # ì½”ì‚¬ì¸ ê¸°ë°˜ì˜ í•™ìŠµë¥  ê³„ì‚°\n",
    "        # í•™ìŠµë¥ ì€ eta_minê³¼ base_lr ì‚¬ì´ì—ì„œ ì¡°ì •ë¨\n",
    "        return eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * t / t_max)) / 2\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIX-V-Gfv5oY"
   },
   "outputs": [],
   "source": [
    "# ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •\n",
    "n = 100  # ìˆœí™˜ ì£¼ê¸°\n",
    "sched = cosine(n)  # ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±\n",
    "\n",
    "# ì£¼ì–´ì§„ ë²”ìœ„(n * 4)ì— ëŒ€í•´ ê° ì—í¬í¬ë³„ë¡œ í•™ìŠµë¥  ê³„ì‚°\n",
    "lrs = [sched(t, 1) for t in range(n * 4)]  # t: ì—í¬í¬, 1: ê¸°ë³¸ í•™ìŠµë¥ \n",
    "\n",
    "plt.plot(lrs)  # í•™ìŠµë¥  ë³€í™” ê·¸ë˜í”„ í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNpSdhRXv7WS"
   },
   "outputs": [],
   "source": [
    "print('Preparing datasets')\n",
    "\n",
    "# Sequence Dataset ìƒì„±\n",
    "trn_ds = ts_seq_tr\n",
    "val_ds = ts_seq_te\n",
    "\n",
    "bs = 128  # ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "print(f'Creating data loaders with batch size: {bs}')\n",
    "# ë°ì´í„° ë¡œë” ìƒì„±\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs=bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf6psYmK8wxu"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZgrZaSHv-aE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "# Random Seed ì„¤ì •\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°, CUDA random seedë„ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        # ìƒì„±ì: ëª¨ë“ˆ ì´ˆê¸°í™”\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[64, 32, 16, output_dim], activation_layer=nn.ELU) # í™œì„±í™” í•¨ìˆ˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ìˆœë°©í–¥ íŒ¨ìŠ¤ ì •ì˜\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        # ì´ˆê¸° íˆë“  ìƒíƒœì™€ ì…€ ìƒíƒœ ìƒì„±\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            return [t.cuda() for t in (h0, c0)]\n",
    "        else:\n",
    "            return [h0, c0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnwiNEdP84Aq"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwphKfrXwmu8"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, 16, output_dim], activation_layer=nn.GELU)  # í™œì„±í™” í•¨ìˆ˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.init_hidden(x)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.mlp(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY0hWje66gQv"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFYLHXhG_MQU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import MLP\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Random Seed ì„¤ì •\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°, CUDA random seedë„ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# TransformerClassifier í´ë˜ìŠ¤ë¥¼ ì •ì˜\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nhead, num_layers, output_dim, max_len=5000):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.mlp = MLP(in_channels=hidden_dim, hidden_channels=[128, 64, 32, output_dim], activation_layer=nn.LeakyReLU)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)\n",
    "        # í‰ê·  í’€ë§ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‹œí€€ìŠ¤ ì •ë³´ í™œìš©\n",
    "        transformer_out = transformer_out.mean(dim=1)\n",
    "        output = self.mlp(transformer_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZtBC7tY9A3u"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAqrZN-T9AK0"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ êµ¬ì„±ì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "input_dim = 10  # ì…ë ¥ íŠ¹ì„±ì˜ ìˆ˜\n",
    "hidden_dim = 256  # Context vector size\n",
    "layer_dim = 3 # Stack number\n",
    "num_layers = layer_dim\n",
    "output_dim = 1  # ì¶œë ¥ ì°¨ì›: ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° 1\n",
    "nhead = 4  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ í—¤ë“œ ìˆ˜\n",
    "\n",
    "# í•™ìŠµë¥  ë° ì—í¬í¬ ì„¤ì •\n",
    "lr = 0.0005\n",
    "n_epochs = 1000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "# best_acc = 0\n",
    "# patience, trials = 100, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6re3Ab37Yeo9"
   },
   "source": [
    "# Train & Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TspvhfNM9Jyp"
   },
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxpxEprd9LYl"
   },
   "outputs": [],
   "source": [
    "ë¶„ë¥˜ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = GRUClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = TransformerClassifier(input_dim, hidden_dim, nhead, num_layers, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4HzZ2yUKxaI"
   },
   "source": [
    "## Process Complete Alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9Ja8v9UK5GE"
   },
   "outputs": [],
   "source": [
    "# í…”ë ˆê·¸ë¨ìœ¼ë¡œ í•™ìŠµì™„ë£Œ ì•Œë¦¼ ì˜¤ëŠ” ì„œë¹„ìŠ¤\n",
    "# https://federicoraimondi.github.io/myProjects/Data_Stuff/knockknock_tutorial/Knock_Knock_tutorial.html\n",
    "!pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qggw4DjlK7PY"
   },
   "outputs": [],
   "source": [
    "# Put your token as a string\n",
    "your_token = \"token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "873GQw9zLC60"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's get your chat id! Be sure to have sent a message to your bot.\n",
    "url = 'https://api.telegram.org/bot'+str(your_token)+'/getUpdates'\n",
    "response = requests.get(url)\n",
    "myinfo = response.json()\n",
    "if response.status_code == 401:\n",
    "  raise NameError('Check if your token is correct.')\n",
    "\n",
    "try:\n",
    "  CHAT_ID: int = myinfo['result'][1]['message']['chat']['id']\n",
    "\n",
    "  print('This is your Chat ID:', CHAT_ID)\n",
    "\n",
    "except:\n",
    "  print('Have you sent a message to your bot? Telegram bot are quite shy ğŸ¤£.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05cHPA_tYPau"
   },
   "source": [
    "## Traning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CCG3t0ZMh42"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from telegram import Bot\n",
    "\n",
    "# Colabì—ì„œ asyncio ì‚¬ìš©ì„ ìœ„í•œ ì¤€ë¹„\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ë¹„ë™ê¸°ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "async def async_send_telegram_message(text, chat_id, token):\n",
    "    bot = Bot(token=token)\n",
    "    await bot.send_message(chat_id=chat_id, text=text)\n",
    "\n",
    "# ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def send_telegram_message(text, chat_id, token):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = loop.create_task(async_send_telegram_message(text, chat_id, token))\n",
    "    loop.run_until_complete(task)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ì—ì„œ ì†ì‹¤ ê³„ì‚° ë¶€ë¶„ ìˆ˜ì •\n",
    "def train_model(model, trn_dl, val_dl, lr, n_epochs, chat_id, token, masked=False):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc = 0  # ìµœê³  ì •í™•ë„ ì´ˆê¸°í™”\n",
    "    patience, trials = 100, 0\n",
    "\n",
    "    print('Start model training')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "            if masked:\n",
    "                x_batch, mask_array = mask_sequence_data(x_batch)\n",
    "                mask_array = mask_array.cuda() if torch.cuda.is_available() else mask_array\n",
    "\n",
    "            x_batch = x_batch.cuda() if torch.cuda.is_available() else x_batch\n",
    "            y_batch = y_batch.cuda() if torch.cuda.is_available() else y_batch\n",
    "\n",
    "            opt.zero_grad()\n",
    "            out = model(x_batch)\n",
    "            loss = criterion(out, y_batch.float())\n",
    "            if masked:\n",
    "                loss = (loss * mask_array.float()).sum() / mask_array.float().sum()\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ í‰ê°€\n",
    "        model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "        correct, total = 0, 0\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = [t.cuda() for t in (x_val, y_val)]  # ë°ì´í„°ë¥¼ CUDAë¡œ ì´ë™\n",
    "            out = model(x_val)\n",
    "            probs = torch.sigmoid(out)  # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°\n",
    "            preds = probs > 0.5  # ì„ê³„ê°’(0.5)ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ê°’ ê²°ì •\n",
    "            # preds = F.log_softmax(out, dim=1).argmax(dim=1)  # ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "            total += y_val.size(0)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "\n",
    "        acc = correct / total  # ì •í™•ë„ ê³„ì‚°\n",
    "\n",
    "        # ì—í¬í¬ë³„ ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch: {epoch:3d}. Loss: {loss.item():.8f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "        # ìµœê³  ì •í™•ë„ ê°±ì‹  ë° ëª¨ë¸ ì €ì¥\n",
    "        if acc > best_acc:\n",
    "            trials = 0\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best.pth')\n",
    "            print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    # í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì•Œë¦¬ê³ , ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì›\n",
    "    print('The training is finished! Restoring the best model weights')\n",
    "\n",
    "    # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜´\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "\n",
    "    final_message = 'ë‹¤í•´ë– '\n",
    "    send_telegram_message(final_message, chat_id, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y9_kybywCor"
   },
   "outputs": [],
   "source": [
    "train_model(model, trn_dl, val_dl, lr, n_epochs, CHAT_ID, your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffm2UI8rOeUY"
   },
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FFuNfQ91EAA"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOR-KTV8wDz0"
   },
   "outputs": [],
   "source": [
    "import time  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "test_predictions = []  # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "print('Predicting on test dataset')\n",
    "\n",
    "start_time = time.time()  # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "\n",
    "with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "    for x_val, _ in val_dl:  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”(val_dl) ì‚¬ìš©\n",
    "        x_val = x_val.cuda() if torch.cuda.is_available() else x_val  # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ, ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™\n",
    "        out = model(x_val)  # ëª¨ë¸ì— ë°°ì¹˜ ë°ì´í„° ì „ë‹¬ ë° ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        probs = torch.sigmoid(out)  # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°\n",
    "        preds = probs > 0.5  # ì„ê³„ê°’(0.5)ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ê°’ ê²°ì •\n",
    "        preds = preds.long()  # Bool íƒ€ì…ì„ Long íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì˜ˆì¸¡ê°’ì„ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜)\n",
    "        test_predictions += preds.cpu().tolist()  # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•œ í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "end_time = time.time()  # í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡\n",
    "test_time = end_time - start_time  # í…ŒìŠ¤íŠ¸ì— ê±¸ë¦° ì‹œê°„ ê³„ì‚°\n",
    "print(f\"Test time: {test_time:.4f} seconds\")\n",
    "\n",
    "# ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ì¤€ë¹„\n",
    "y_true = []\n",
    "y_pred = test_predictions  # ì˜ˆì¸¡ ë ˆì´ë¸” (ìœ„ì˜ ì˜ˆì¸¡ ì½”ë“œ ê²°ê³¼)\n",
    "\n",
    "for _, y_batch in val_dl:  # ì‹¤ì œ ë ˆì´ë¸” ì¶”ì¶œ\n",
    "    y_true += y_batch.tolist()\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# F1 ì ìˆ˜ ê³„ì‚°\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# í˜¼ë™ í–‰ë ¬ ê³„ì‚°\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53625qr_qFi"
   },
   "outputs": [],
   "source": [
    "# ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„° í¬ì¸íŠ¸ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "\n",
    "# ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„° í¬ì¸íŠ¸ì˜ ìˆ˜ ì¶œë ¥\n",
    "print(f\"Number of misclassified points: {len(misclassified_indices)}\")\n",
    "\n",
    "# numpyëŠ” column ì´ë¦„ì´ ì—†ì—ˆê¸° ë•Œë¬¸ì— column ì´ë¦„ ë‹¤ì‹œ ë§Œë“¤ì–´ì£¼ê¸°\n",
    "column_names = dict_split_te_attack['35m_dyn2'][3].columns\n",
    "column_list = list(column_names)\n",
    "value_to_remove = drop_cols + ['legitimate']\n",
    "for values in value_to_remove:\n",
    "    if values in column_list:\n",
    "        column_list.remove(values)\n",
    "\n",
    "# ì˜ëª» ë¶„ë¥˜ëœ ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ì •ë³´ ì¶œë ¥\n",
    "for index in misclassified_indices:\n",
    "    # 'val_ds[index]'ë¡œë¶€í„° ë°ì´í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬\n",
    "    data_tensor, label_tensor = val_ds[index]\n",
    "\n",
    "    # ë°ì´í„° í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    numpy_data = data_tensor.numpy()\n",
    "\n",
    "    # NumPy ë°°ì—´ì„ Pandas ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.DataFrame(numpy_data, columns=column_list)\n",
    "\n",
    "    # ë ˆì´ë¸” í™•ì¸\n",
    "    label = label_tensor.item()\n",
    "\n",
    "    print(f\"Data Point Index: {index}\")\n",
    "    print(\"Data Frame:\\n\", df)\n",
    "    print(\"Label:\", label)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilDANzgxoak_"
   },
   "source": [
    "# K-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MB0jDbqoZ2y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ ì§€ì •\n",
    "ratio_tr = 0.8  # í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨\n",
    "ratio_te = 1 - ratio_tr  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨\n",
    "\n",
    "# k-fold êµì°¨ ê²€ì¦ íŒŒë¼ë¯¸í„°\n",
    "kf = 5\n",
    "k = kf  # ì§€ì •ëœ k-fold ìˆ˜\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# ì„±ëŠ¥ ì§€í‘œ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_fprs = []\n",
    "fold_fnrs = []\n",
    "\n",
    "# K-Fold êµì°¨ ê²€ì¦ ì‹œì‘\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(combined_dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k}\")\n",
    "\n",
    "    # ì§€ì •ëœ ë¹„ìœ¨ë¡œ í›ˆë ¨ê³¼ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
    "    num_train = int(len(train_idx) * ratio_tr)  # í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¼ ì¸ë±ìŠ¤ ë¶„í• \n",
    "    tr_idx, val_idx = train_idx[:num_train], train_idx[num_train:]  # tr_idx: í›ˆë ¨, val_idx: ê²€ì¦\n",
    "\n",
    "    # Subsetì„ í†µí•´ trainê³¼ validation ë°ì´í„° ìƒì„±\n",
    "    trn_ds = Subset(combined_dataset, tr_idx)\n",
    "    val_ds = Subset(combined_dataset, val_idx)\n",
    "\n",
    "    # ë°ì´í„° ë¡œë” ìƒì„± (ë°°ì¹˜ í¬ê¸° ë“± ê¸°ì¡´ ì½”ë“œ ì‚¬ìš©)\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=cpu_count())\n",
    "    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=cpu_count())\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (ì›í•˜ëŠ” ëª¨ë¸ë¡œ ì„¤ì •)\n",
    "    # model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)  # ì˜ˆ: LSTM\n",
    "    # model = GRUClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    model = TransformerClassifier(input_dim, hidden_dim, nhead, num_layers, output_dim)\n",
    "\n",
    "    # ëª¨ë¸ í›ˆë ¨\n",
    "    train_model(model, trn_dl, val_dl, lr, n_epochs, CHAT_ID, your_token)\n",
    "\n",
    "    # ëª¨ë¸ í‰ê°€\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "            out = model(x_val)\n",
    "            probs = torch.sigmoid(out)\n",
    "            preds = (probs > 0.5).long()\n",
    "            y_true += y_val.cpu().tolist()\n",
    "            y_pred += preds.cpu().tolist()\n",
    "\n",
    "    # fold ì„±ëŠ¥ ê³„ì‚° ë° ì €ì¥\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Confusion matrixì—ì„œ TN, FP, FN, TP ì¶”ì¶œí•˜ì—¬ FPRê³¼ FNR ê³„ì‚°\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0\n",
    "\n",
    "    # ì„±ëŠ¥ ì§€í‘œ ì €ì¥\n",
    "    fold_accuracies.append(acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_fprs.append(fpr)\n",
    "    fold_fnrs.append(fnr)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nK-Fold Cross Validation Results:\")\n",
    "print(f\"Mean Accuracy: {sum(fold_accuracies) / k:.4f}\")\n",
    "print(f\"Mean F1 Score: {sum(fold_f1_scores) / k:.4f}\")\n",
    "print(f\"Mean Precision: {sum(fold_precisions) / k:.4f}\")\n",
    "print(f\"Mean Recall: {sum(fold_recalls) / k:.4f}\")\n",
    "print(f\"Mean FPR: {sum(fold_fprs) / k:.4f}\")\n",
    "print(f\"Mean FNR: {sum(fold_fnrs) / k:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMf1ak8j7Fml8S0sL0NXEJu",
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
